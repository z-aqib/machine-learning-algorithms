{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9837068,"sourceType":"datasetVersion","datasetId":6034234},{"sourceId":9837128,"sourceType":"datasetVersion","datasetId":6034286}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Libraries\nin this part we will install all the necessary libraries on command prompt and then import the necessary functions from those libraries. ","metadata":{}},{"cell_type":"code","source":"# importing all the necessary libraries\nimport pandas as pd\n\nfrom numpy import mean\nimport numpy as np\nimport time\n\n# step 1: preprocessing\nfrom sklearn.impute import SimpleImputer # import some strategic imputer to fill in any missing values using mean\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler, Normalizer # scale all the values to one range to avoid any biasness (this bias is seen in mostly naive bayes and knn etc)\n\nfrom sklearn.impute import KNNImputer # import some strategic imputer to fill missing values using KNN (finds the nearest neighbour and fills it with that value)\n\nfrom sklearn.feature_selection import SequentialFeatureSelector, SelectKBest, f_classif, VarianceThreshold\n\n# step 2: data division\nfrom sklearn.model_selection import train_test_split, RepeatedKFold, cross_val_score, GridSearchCV, ParameterGrid # to divide the code into train/test using a specific percentage or with/without replacement\n\n# step 3: model\nfrom sklearn.tree import DecisionTreeClassifier                                                        \nfrom sklearn.naive_bayes import GaussianNB                                                              \nfrom sklearn.neighbors import KNeighborsClassifier                                                       \nfrom sklearn.ensemble import BaggingClassifier, VotingClassifier, ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nimport lightgbm as lgb \nimport xgboost as xgb\nfrom catboost import CatBoostClassifier \n\n# step 4: displaying accuracy\nfrom sklearn.metrics import roc_auc_score, accuracy_score # to display the accuracy of our tree\n\n# step 5: warning filter\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:05.841024Z","iopub.execute_input":"2024-11-07T18:51:05.841675Z","iopub.status.idle":"2024-11-07T18:51:05.850016Z","shell.execute_reply.started":"2024-11-07T18:51:05.841633Z","shell.execute_reply":"2024-11-07T18:51:05.848996Z"},"trusted":true},"execution_count":167,"outputs":[]},{"cell_type":"code","source":"# use this block to install any libraries not on the system\n# !pip install pandas\n# !pip install sklearn\n# python -m pip install scikit-learn lightgbm xgboost catboost","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:05.851792Z","iopub.execute_input":"2024-11-07T18:51:05.852084Z","iopub.status.idle":"2024-11-07T18:51:05.867148Z","shell.execute_reply.started":"2024-11-07T18:51:05.852052Z","shell.execute_reply":"2024-11-07T18:51:05.866210Z"},"_kg_hide-output":true,"trusted":true},"execution_count":168,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading\ndata shall be loaded into variables as data sets using pandas and csv readers. they will be checked to see if they are loaded properly and will be loaded as 2 sets: train and test as per given in the kaggle data","metadata":{}},{"cell_type":"code","source":"# lets load the training data set\ntrain_data = pd.read_csv(r\"/kaggle/input/imlchallenger1/train_set.csv\")\n\n# lets also check it by getting the first few rows of the data, there should be x1 - x78 and one target variable Y\ntrain_data.head() ","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:05.868629Z","iopub.execute_input":"2024-11-07T18:51:05.868937Z","iopub.status.idle":"2024-11-07T18:51:08.894787Z","shell.execute_reply.started":"2024-11-07T18:51:05.868892Z","shell.execute_reply":"2024-11-07T18:51:08.893783Z"},"trusted":true},"execution_count":169,"outputs":[{"execution_count":169,"output_type":"execute_result","data":{"text/plain":"   RecordId         X2         X3  X4  X5  X6          X7  X8   X9  X10  ...  \\\n0         1  87.000000  34.118411   0   2   0  165.100000   1  829    2  ...   \n1         2  82.372284  31.573280   0   0   1  162.983897   1  724    0  ...   \n2         3  50.000000  27.771653   0   0   1  165.100000   1  895    2  ...   \n3         4  66.236109  26.515922   0   0   1  167.009549   1  637    0  ...   \n4         5  81.303299  20.843691   0   0   1  158.165419   0  564    0  ...   \n\n        X70  X71  X72  X73  X74  X75  X76  X77  X78  Y  \n0  0.040000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0  \n1  0.033431  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0  \n2  0.010000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0  \n3  0.039363  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0  \n4  0.069242  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0  \n\n[5 rows x 79 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RecordId</th>\n      <th>X2</th>\n      <th>X3</th>\n      <th>X4</th>\n      <th>X5</th>\n      <th>X6</th>\n      <th>X7</th>\n      <th>X8</th>\n      <th>X9</th>\n      <th>X10</th>\n      <th>...</th>\n      <th>X70</th>\n      <th>X71</th>\n      <th>X72</th>\n      <th>X73</th>\n      <th>X74</th>\n      <th>X75</th>\n      <th>X76</th>\n      <th>X77</th>\n      <th>X78</th>\n      <th>Y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>87.000000</td>\n      <td>34.118411</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>165.100000</td>\n      <td>1</td>\n      <td>829</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0.040000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>82.372284</td>\n      <td>31.573280</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>162.983897</td>\n      <td>1</td>\n      <td>724</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.033431</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>50.000000</td>\n      <td>27.771653</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>165.100000</td>\n      <td>1</td>\n      <td>895</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0.010000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>66.236109</td>\n      <td>26.515922</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>167.009549</td>\n      <td>1</td>\n      <td>637</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.039363</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>81.303299</td>\n      <td>20.843691</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>158.165419</td>\n      <td>0</td>\n      <td>564</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.069242</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 79 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# lets load the test data\ntest_data = pd.read_csv(r\"/kaggle/input/imlchallenger1/test_set.csv\")\n\n# check if the data has been loaded by getting the first 5 rows - there should be x1 - x78 and no target variable Y as this is test data\ntest_data.head() ","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:08.896764Z","iopub.execute_input":"2024-11-07T18:51:08.897642Z","iopub.status.idle":"2024-11-07T18:51:10.189651Z","shell.execute_reply.started":"2024-11-07T18:51:08.897585Z","shell.execute_reply":"2024-11-07T18:51:10.188666Z"},"trusted":true},"execution_count":170,"outputs":[{"execution_count":170,"output_type":"execute_result","data":{"text/plain":"   RecordId         X2         X3  X4  X5  X6          X7  X8   X9  X10  ...  \\\n0    300001  79.000000  17.122318   0   0   1  170.200000   1  700    0  ...   \n1    300002  38.000000  43.693579   0   0   1  165.100000   1  814    0  ...   \n2    300003  36.064225  23.998944   0   0   1  167.086735   1  662    0  ...   \n3    300004  61.846764  31.693449   0   3   1  182.355708   2  862    0  ...   \n4    300005  71.591991  20.086147   1   0   1  166.704917   2  335    0  ...   \n\n        X69       X70  X71  X72  X73  X74  X75  X76  X77  X78  \n0  0.070000  0.030000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n1  0.050000  0.040000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n2  0.006948  0.006948  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n3  0.062613  0.033153  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n4  0.014854  0.004854  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n\n[5 rows x 78 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RecordId</th>\n      <th>X2</th>\n      <th>X3</th>\n      <th>X4</th>\n      <th>X5</th>\n      <th>X6</th>\n      <th>X7</th>\n      <th>X8</th>\n      <th>X9</th>\n      <th>X10</th>\n      <th>...</th>\n      <th>X69</th>\n      <th>X70</th>\n      <th>X71</th>\n      <th>X72</th>\n      <th>X73</th>\n      <th>X74</th>\n      <th>X75</th>\n      <th>X76</th>\n      <th>X77</th>\n      <th>X78</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>300001</td>\n      <td>79.000000</td>\n      <td>17.122318</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>170.200000</td>\n      <td>1</td>\n      <td>700</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.070000</td>\n      <td>0.030000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>300002</td>\n      <td>38.000000</td>\n      <td>43.693579</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>165.100000</td>\n      <td>1</td>\n      <td>814</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.050000</td>\n      <td>0.040000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>300003</td>\n      <td>36.064225</td>\n      <td>23.998944</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>167.086735</td>\n      <td>1</td>\n      <td>662</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.006948</td>\n      <td>0.006948</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>300004</td>\n      <td>61.846764</td>\n      <td>31.693449</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>182.355708</td>\n      <td>2</td>\n      <td>862</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.062613</td>\n      <td>0.033153</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>300005</td>\n      <td>71.591991</td>\n      <td>20.086147</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>166.704917</td>\n      <td>2</td>\n      <td>335</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.014854</td>\n      <td>0.004854</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 78 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Data Preprocessing\nbefore we start processing this data and using algorithms, we will fix this data first, this is called data preprocessing","metadata":{}},{"cell_type":"markdown","source":"## Conversion of Categorical to Numerical\nFirst we will convert categorical data to numerical data by doing one hot encoding, which turns it into binary variables","metadata":{}},{"cell_type":"code","source":"# one hot encoding - display it\npd.get_dummies(train_data) # this line will convert the train_data to one hot encoding but it will only display the result and not save it","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:10.190753Z","iopub.execute_input":"2024-11-07T18:51:10.191037Z","iopub.status.idle":"2024-11-07T18:51:10.353093Z","shell.execute_reply.started":"2024-11-07T18:51:10.191007Z","shell.execute_reply":"2024-11-07T18:51:10.352152Z"},"trusted":true},"execution_count":171,"outputs":[{"execution_count":171,"output_type":"execute_result","data":{"text/plain":"        RecordId         X2         X3  X4  X5  X6          X7  X8   X9  X10  \\\n0              1  87.000000  34.118411   0   2   0  165.100000   1  829    2   \n1              2  82.372284  31.573280   0   0   1  162.983897   1  724    0   \n2              3  50.000000  27.771653   0   0   1  165.100000   1  895    2   \n3              4  66.236109  26.515922   0   0   1  167.009549   1  637    0   \n4              5  81.303299  20.843691   0   0   1  158.165419   0  564    0   \n...          ...        ...        ...  ..  ..  ..         ...  ..  ...  ...   \n246117    246118  65.149110  33.357948   0   0   1  156.317941   1  711    0   \n246118    246119  48.000000  46.736176   0   0   1  157.000000   1  594    2   \n246119    246120  57.472080  41.854115   1   0   0  189.868698   2  455    0   \n246120    246121  66.000000  23.738662   1   0   1  168.000000   2  609    0   \n246121    246122  50.257640  32.753911   0   2   0  173.665068   1  637    0   \n\n        ...       X70  X71  X72       X73  X74       X75  X76  X77       X78  \\\n0       ...  0.040000  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  0.000000   \n1       ...  0.033431  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  0.000000   \n2       ...  0.010000  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  0.000000   \n3       ...  0.039363  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  0.000000   \n4       ...  0.069242  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  0.000000   \n...     ...       ...  ...  ...       ...  ...       ...  ...  ...       ...   \n246117  ...  0.027152  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  0.000000   \n246118  ...  0.560000  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  0.000000   \n246119  ...  0.020601  0.0  0.0  0.587987  0.0  0.412013  0.0  0.0  0.412013   \n246120  ... -1.000000  0.0  0.0  1.000000  0.0  0.000000  0.0  0.0  0.000000   \n246121  ...  0.000000  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  0.000000   \n\n        Y  \n0       0  \n1       0  \n2       0  \n3       0  \n4       0  \n...    ..  \n246117  0  \n246118  1  \n246119  0  \n246120  0  \n246121  0  \n\n[246122 rows x 79 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RecordId</th>\n      <th>X2</th>\n      <th>X3</th>\n      <th>X4</th>\n      <th>X5</th>\n      <th>X6</th>\n      <th>X7</th>\n      <th>X8</th>\n      <th>X9</th>\n      <th>X10</th>\n      <th>...</th>\n      <th>X70</th>\n      <th>X71</th>\n      <th>X72</th>\n      <th>X73</th>\n      <th>X74</th>\n      <th>X75</th>\n      <th>X76</th>\n      <th>X77</th>\n      <th>X78</th>\n      <th>Y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>87.000000</td>\n      <td>34.118411</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>165.100000</td>\n      <td>1</td>\n      <td>829</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0.040000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>82.372284</td>\n      <td>31.573280</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>162.983897</td>\n      <td>1</td>\n      <td>724</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.033431</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>50.000000</td>\n      <td>27.771653</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>165.100000</td>\n      <td>1</td>\n      <td>895</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0.010000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>66.236109</td>\n      <td>26.515922</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>167.009549</td>\n      <td>1</td>\n      <td>637</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.039363</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>81.303299</td>\n      <td>20.843691</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>158.165419</td>\n      <td>0</td>\n      <td>564</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.069242</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>246117</th>\n      <td>246118</td>\n      <td>65.149110</td>\n      <td>33.357948</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>156.317941</td>\n      <td>1</td>\n      <td>711</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.027152</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>246118</th>\n      <td>246119</td>\n      <td>48.000000</td>\n      <td>46.736176</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>157.000000</td>\n      <td>1</td>\n      <td>594</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0.560000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>246119</th>\n      <td>246120</td>\n      <td>57.472080</td>\n      <td>41.854115</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>189.868698</td>\n      <td>2</td>\n      <td>455</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.020601</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.587987</td>\n      <td>0.0</td>\n      <td>0.412013</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.412013</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>246120</th>\n      <td>246121</td>\n      <td>66.000000</td>\n      <td>23.738662</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>168.000000</td>\n      <td>2</td>\n      <td>609</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-1.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>246121</th>\n      <td>246122</td>\n      <td>50.257640</td>\n      <td>32.753911</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>173.665068</td>\n      <td>1</td>\n      <td>637</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>246122 rows × 79 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# we can see that there is no change in the number of columns meaning there is no categorical data. but for the sake of running the program. we must perform the preprocessing therefore we shall re-run the one hot encoding and save it somewhere\ntrain_data_processed = pd.get_dummies(train_data)\n\n# now we shall do the same on the test data so that we maintain the rules over all data\ntest_data_processed = pd.get_dummies(test_data)","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:10.355670Z","iopub.execute_input":"2024-11-07T18:51:10.355983Z","iopub.status.idle":"2024-11-07T18:51:10.504470Z","shell.execute_reply.started":"2024-11-07T18:51:10.355951Z","shell.execute_reply":"2024-11-07T18:51:10.503646Z"},"trusted":true},"execution_count":172,"outputs":[]},{"cell_type":"markdown","source":"## Data Splitting - festures and targets\nthe data in train_data set is of x1 - x78 columns (79 variables) and one target variable (Y). we must split that data so that we can perform data preprocessing on the features variables (will be referred to as X).","metadata":{}},{"cell_type":"code","source":"# so in X, it is ALL the columns EXCEPT the last column known as 'Y' (we can confirm this using the train_data.head() we did earlier) so we must get all columns and DROP only the 'y' column\nX = train_data_processed.drop(columns=['Y'])\nX # lets display X and see what it is now","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:10.506031Z","iopub.execute_input":"2024-11-07T18:51:10.506503Z","iopub.status.idle":"2024-11-07T18:51:10.609729Z","shell.execute_reply.started":"2024-11-07T18:51:10.506426Z","shell.execute_reply":"2024-11-07T18:51:10.608765Z"},"trusted":true},"execution_count":173,"outputs":[{"execution_count":173,"output_type":"execute_result","data":{"text/plain":"        RecordId         X2         X3  X4  X5  X6          X7  X8   X9  X10  \\\n0              1  87.000000  34.118411   0   2   0  165.100000   1  829    2   \n1              2  82.372284  31.573280   0   0   1  162.983897   1  724    0   \n2              3  50.000000  27.771653   0   0   1  165.100000   1  895    2   \n3              4  66.236109  26.515922   0   0   1  167.009549   1  637    0   \n4              5  81.303299  20.843691   0   0   1  158.165419   0  564    0   \n...          ...        ...        ...  ..  ..  ..         ...  ..  ...  ...   \n246117    246118  65.149110  33.357948   0   0   1  156.317941   1  711    0   \n246118    246119  48.000000  46.736176   0   0   1  157.000000   1  594    2   \n246119    246120  57.472080  41.854115   1   0   0  189.868698   2  455    0   \n246120    246121  66.000000  23.738662   1   0   1  168.000000   2  609    0   \n246121    246122  50.257640  32.753911   0   2   0  173.665068   1  637    0   \n\n        ...       X69       X70  X71  X72       X73  X74       X75  X76  X77  \\\n0       ...  0.110000  0.040000  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0   \n1       ...  0.100292  0.033431  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0   \n2       ...  0.020000  0.010000  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0   \n3       ...  0.108249  0.039363  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0   \n4       ...  0.164645  0.069242  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0   \n...     ...       ...       ...  ...  ...       ...  ...       ...  ...  ...   \n246117  ...  0.088610  0.027152  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0   \n246118  ... -1.000000  0.560000  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0   \n246119  ...  0.032961  0.020601  0.0  0.0  0.587987  0.0  0.412013  0.0  0.0   \n246120  ...  0.020000 -1.000000  0.0  0.0  1.000000  0.0  0.000000  0.0  0.0   \n246121  ...  0.013712  0.000000  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0   \n\n             X78  \n0       0.000000  \n1       0.000000  \n2       0.000000  \n3       0.000000  \n4       0.000000  \n...          ...  \n246117  0.000000  \n246118  0.000000  \n246119  0.412013  \n246120  0.000000  \n246121  0.000000  \n\n[246122 rows x 78 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RecordId</th>\n      <th>X2</th>\n      <th>X3</th>\n      <th>X4</th>\n      <th>X5</th>\n      <th>X6</th>\n      <th>X7</th>\n      <th>X8</th>\n      <th>X9</th>\n      <th>X10</th>\n      <th>...</th>\n      <th>X69</th>\n      <th>X70</th>\n      <th>X71</th>\n      <th>X72</th>\n      <th>X73</th>\n      <th>X74</th>\n      <th>X75</th>\n      <th>X76</th>\n      <th>X77</th>\n      <th>X78</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>87.000000</td>\n      <td>34.118411</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>165.100000</td>\n      <td>1</td>\n      <td>829</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0.110000</td>\n      <td>0.040000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>82.372284</td>\n      <td>31.573280</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>162.983897</td>\n      <td>1</td>\n      <td>724</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.100292</td>\n      <td>0.033431</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>50.000000</td>\n      <td>27.771653</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>165.100000</td>\n      <td>1</td>\n      <td>895</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0.020000</td>\n      <td>0.010000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>66.236109</td>\n      <td>26.515922</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>167.009549</td>\n      <td>1</td>\n      <td>637</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.108249</td>\n      <td>0.039363</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>81.303299</td>\n      <td>20.843691</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>158.165419</td>\n      <td>0</td>\n      <td>564</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.164645</td>\n      <td>0.069242</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>246117</th>\n      <td>246118</td>\n      <td>65.149110</td>\n      <td>33.357948</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>156.317941</td>\n      <td>1</td>\n      <td>711</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.088610</td>\n      <td>0.027152</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>246118</th>\n      <td>246119</td>\n      <td>48.000000</td>\n      <td>46.736176</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>157.000000</td>\n      <td>1</td>\n      <td>594</td>\n      <td>2</td>\n      <td>...</td>\n      <td>-1.000000</td>\n      <td>0.560000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>246119</th>\n      <td>246120</td>\n      <td>57.472080</td>\n      <td>41.854115</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>189.868698</td>\n      <td>2</td>\n      <td>455</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.032961</td>\n      <td>0.020601</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.587987</td>\n      <td>0.0</td>\n      <td>0.412013</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.412013</td>\n    </tr>\n    <tr>\n      <th>246120</th>\n      <td>246121</td>\n      <td>66.000000</td>\n      <td>23.738662</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>168.000000</td>\n      <td>2</td>\n      <td>609</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.020000</td>\n      <td>-1.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>246121</th>\n      <td>246122</td>\n      <td>50.257640</td>\n      <td>32.753911</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>173.665068</td>\n      <td>1</td>\n      <td>637</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.013712</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>246122 rows × 78 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# so as per our X output, we can see that number of columns in train_data is 79 and number of columns in X is 78 meaning we have successfully performed our removal of target variable\n# now to get the target variable alone, we can just get it alone,\nY = train_data_processed['Y']\nY # lets see what it is\n# as per our Y output, we can see it is of one column and 246k rows which means we have successfully extracted the target variable column","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:10.611049Z","iopub.execute_input":"2024-11-07T18:51:10.611378Z","iopub.status.idle":"2024-11-07T18:51:10.619287Z","shell.execute_reply.started":"2024-11-07T18:51:10.611341Z","shell.execute_reply":"2024-11-07T18:51:10.618387Z"},"trusted":true},"execution_count":174,"outputs":[{"execution_count":174,"output_type":"execute_result","data":{"text/plain":"0         0\n1         0\n2         0\n3         0\n4         0\n         ..\n246117    0\n246118    1\n246119    0\n246120    0\n246121    0\nName: Y, Length: 246122, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"## Data Imputation \nmany cells in our data may be empty - we must fill these cells with data. we have multiple options to deal with them:\n- we remove the entire rows (Case 1)\n- we fill the cells with the average of the column (Case 2)\n- we fill the cells based on KNN imputation (nearest neighbour) (Case 3)","metadata":{}},{"cell_type":"code","source":"# Average Mean Imputation\n# ----------------------------- case -----------------------------\n# this will fill all the empty spaces using the average of all the spaces\nimputer = SimpleImputer(strategy='mean')","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:10.620570Z","iopub.execute_input":"2024-11-07T18:51:10.620878Z","iopub.status.idle":"2024-11-07T18:51:10.628653Z","shell.execute_reply.started":"2024-11-07T18:51:10.620845Z","shell.execute_reply":"2024-11-07T18:51:10.627924Z"},"trusted":true},"execution_count":175,"outputs":[]},{"cell_type":"code","source":"# KNN Imputation\n# ----------------------------- case -----------------------------\n# this fills them in using k-nearest neighbours of all the spaces\n# imputer = KNNImputer(n_neighbors=7)","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:10.629580Z","iopub.execute_input":"2024-11-07T18:51:10.629860Z","iopub.status.idle":"2024-11-07T18:51:10.643210Z","shell.execute_reply.started":"2024-11-07T18:51:10.629829Z","shell.execute_reply":"2024-11-07T18:51:10.642499Z"},"trusted":true},"execution_count":176,"outputs":[]},{"cell_type":"code","source":"X = imputer.fit_transform(X)                                        # fill them in X\ntest_data_processed = imputer.transform(test_data_processed)    # fill them in test data","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:10.644374Z","iopub.execute_input":"2024-11-07T18:51:10.644678Z","iopub.status.idle":"2024-11-07T18:51:11.170371Z","shell.execute_reply.started":"2024-11-07T18:51:10.644648Z","shell.execute_reply":"2024-11-07T18:51:11.169267Z"},"trusted":true},"execution_count":177,"outputs":[]},{"cell_type":"markdown","source":"## Data Scaling\nsome columns may be very large then other columns when compared. it would not affect at the moment as we are using decision trees, but to maintain a fair enviroment, we shall perform scaling on every run.\nthere are two types of scaling: \n- min max scaling (also known as normalization)\n- standardisation (z-score normalization)\n- max abs scaler\n- robust scaler\n- normalizer","metadata":{}},{"cell_type":"code","source":"# ----------------------------- case  -----------------------------\n# in this case we shall perform min max scaling. to do that, we must use our MinMaxScaler that we have imported above\nscaler = MinMaxScaler()\n# now we must use this scaler to scale X\nscaler.fit_transform(X)","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:11.175119Z","iopub.execute_input":"2024-11-07T18:51:11.175436Z","iopub.status.idle":"2024-11-07T18:51:11.321061Z","shell.execute_reply.started":"2024-11-07T18:51:11.175402Z","shell.execute_reply":"2024-11-07T18:51:11.319978Z"},"trusted":true},"execution_count":178,"outputs":[{"execution_count":178,"output_type":"execute_result","data":{"text/plain":"array([[0.00000000e+00, 9.72602740e-01, 3.63856188e-01, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [4.06304216e-06, 9.09209364e-01, 3.15807703e-01, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [8.12608432e-06, 4.65753425e-01, 2.44038356e-01, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       ...,\n       [9.99991874e-01, 5.68110690e-01, 5.09895343e-01, ...,\n        0.00000000e+00, 0.00000000e+00, 4.12013395e-01],\n       [9.99995937e-01, 6.84931507e-01, 1.67901180e-01, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 4.69282739e-01, 3.38096342e-01, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"},"metadata":{}}]},{"cell_type":"code","source":"# ----------------------------- case -----------------------------\n# scaler = MaxAbsScaler()\n# # now we must use this scaler to scale X\n# scaler.fit_transform(X)","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:11.322672Z","iopub.execute_input":"2024-11-07T18:51:11.323066Z","iopub.status.idle":"2024-11-07T18:51:11.328299Z","shell.execute_reply.started":"2024-11-07T18:51:11.323023Z","shell.execute_reply":"2024-11-07T18:51:11.327136Z"},"trusted":true},"execution_count":179,"outputs":[]},{"cell_type":"code","source":"# our output shows us that every value in the array is between 0 and 1. thus lets save this value on X\nX = scaler.fit_transform(X)\n\n# now we must do the same on our test_data set\ntest_data_processed = scaler.transform(test_data_processed)","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:11.329624Z","iopub.execute_input":"2024-11-07T18:51:11.330010Z","iopub.status.idle":"2024-11-07T18:51:11.531155Z","shell.execute_reply.started":"2024-11-07T18:51:11.329968Z","shell.execute_reply":"2024-11-07T18:51:11.530129Z"},"trusted":true},"execution_count":180,"outputs":[]},{"cell_type":"markdown","source":"# Filters\nthere are two types of filters to filter out columns/features:\n- variance filter (a column which has same values throughout the column like all are sunny)\n- correlation filter (two columns which are same like weight in kg and weight in pounds)","metadata":{}},{"cell_type":"code","source":"print(\"X : \", X.shape)\nprint(\"test data : \", test_data_processed.shape)","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:11.532497Z","iopub.execute_input":"2024-11-07T18:51:11.532828Z","iopub.status.idle":"2024-11-07T18:51:11.538095Z","shell.execute_reply.started":"2024-11-07T18:51:11.532795Z","shell.execute_reply":"2024-11-07T18:51:11.536948Z"},"trusted":true},"execution_count":181,"outputs":[{"name":"stdout","text":"X :  (246122, 78)\ntest data :  (105482, 78)\n","output_type":"stream"}]},{"cell_type":"code","source":"# variance filter\n# ----------------------------- case  -----------------------------\n# variance_filter = VarianceThreshold(threshold=0.001)  # Adjust the threshold if needed\n# X = variance_filter.fit_transform(X)\n# test_data_processed = variance_filter.fit_transform(test_data_processed)\nX.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:11.539417Z","iopub.execute_input":"2024-11-07T18:51:11.539777Z","iopub.status.idle":"2024-11-07T18:51:11.549089Z","shell.execute_reply.started":"2024-11-07T18:51:11.539746Z","shell.execute_reply":"2024-11-07T18:51:11.548166Z"},"trusted":true},"execution_count":182,"outputs":[{"execution_count":182,"output_type":"execute_result","data":{"text/plain":"(246122, 78)"},"metadata":{}}]},{"cell_type":"code","source":"test_data_processed.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:11.550203Z","iopub.execute_input":"2024-11-07T18:51:11.550596Z","iopub.status.idle":"2024-11-07T18:51:11.559915Z","shell.execute_reply.started":"2024-11-07T18:51:11.550551Z","shell.execute_reply":"2024-11-07T18:51:11.559027Z"},"trusted":true},"execution_count":183,"outputs":[{"execution_count":183,"output_type":"execute_result","data":{"text/plain":"(105482, 78)"},"metadata":{}}]},{"cell_type":"code","source":"# # correlation filter\n# # ----------------------------- case  -----------------------------\n# corr_matrix = pd.DataFrame(X).corr().abs()\n# upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n# to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\n# X = pd.DataFrame(X).drop(columns=to_drop)\n# test_data_processed = pd.DataFrame(test_data_processed).drop(columns=to_drop)\nX.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:11.561252Z","iopub.execute_input":"2024-11-07T18:51:11.561574Z","iopub.status.idle":"2024-11-07T18:51:11.570220Z","shell.execute_reply.started":"2024-11-07T18:51:11.561543Z","shell.execute_reply":"2024-11-07T18:51:11.569308Z"},"trusted":true},"execution_count":184,"outputs":[{"execution_count":184,"output_type":"execute_result","data":{"text/plain":"(246122, 78)"},"metadata":{}}]},{"cell_type":"code","source":"test_data_processed.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:11.571107Z","iopub.execute_input":"2024-11-07T18:51:11.571490Z","iopub.status.idle":"2024-11-07T18:51:11.585256Z","shell.execute_reply.started":"2024-11-07T18:51:11.571410Z","shell.execute_reply":"2024-11-07T18:51:11.584282Z"},"trusted":true},"execution_count":185,"outputs":[{"execution_count":185,"output_type":"execute_result","data":{"text/plain":"(105482, 78)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Data Splitting - train and validate\nnow our test_data set is of rows with NO target variable whereas the train_data set is WITH target variable.\nour rules in machine learning is that we must train half or 70% of the data and then we must check its accuracy using the remaining half or 30% of the data - we can only check accuracy IF we have the answers i.e. the target variable. \nSo, what we need to do is, is split the train_data set into 2, by a 70% and 30% ratio. we train the model using the 70% and then test the model using the 30% and then use that model to predict the test_data set.","metadata":{}},{"cell_type":"code","source":"# holdout method\ntrainX, testX, trainY, testY = train_test_split(X, Y, test_size=0.3, random_state=2)","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:11.586421Z","iopub.execute_input":"2024-11-07T18:51:11.586807Z","iopub.status.idle":"2024-11-07T18:51:11.827917Z","shell.execute_reply.started":"2024-11-07T18:51:11.586767Z","shell.execute_reply":"2024-11-07T18:51:11.826896Z"},"trusted":true},"execution_count":186,"outputs":[]},{"cell_type":"markdown","source":"# functions","metadata":{}},{"cell_type":"code","source":"def fbselection(direction, sample_model, features):\n    selection = SequentialFeatureSelector(sample_model, direction=direction, n_features_to_select=features, scoring='roc_auc')\n    return modelSelector(sample_model, selection)\n\ndef modelSelector(sample_model, selection):\n    trainX = selection.fit_transform(trainX, trainY)\n    testX = selection.transform(testX)                                  # Ensure the test set is transformed similarly\n    test_data_processed = selection.transform(test_data_processed)      # test data is also transformed\n    X = selection.transform(X)                                          # full data transforming\n    return sample_model\n\ndef kbest(sample_model, features):\n    selection = SelectKBest(score_func=f_classif, k=features)\n    return modelSelector(sample_model, selection)","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:11.829119Z","iopub.execute_input":"2024-11-07T18:51:11.829436Z","iopub.status.idle":"2024-11-07T18:51:11.839421Z","shell.execute_reply.started":"2024-11-07T18:51:11.829404Z","shell.execute_reply":"2024-11-07T18:51:11.838290Z"},"trusted":true},"execution_count":187,"outputs":[]},{"cell_type":"code","source":"def featureImportance(sample_model, features, X, trainX, trainY, testX, test_data_processed):\n    print(\"fitting\")\n    \n    # fit the model\n    sample_model.fit(trainX, trainY)\n\n    print(\"extracting features\")\n\n    # extract all the feature names from data\n    importances = sample_model.feature_importances_\n    feature_names = train_data_processed.drop(columns=['Y']).columns\n    print(feature_names)\n\n    # sort with respect to importance\n    feature_importance_df = pd.DataFrame({\n        'Feature': feature_names,\n        'Importance': importances\n    }).sort_values(by='Importance', ascending=False)\n\n    # extract the top ones\n    top_features = feature_importance_df['Feature'].head(features).values\n    print(top_features)\n\n    # change all data according to the top ones we have selected\n    trainX = pd.DataFrame(trainX, columns=feature_names)[top_features]\n    testX = pd.DataFrame(testX, columns=feature_names)[top_features]\n    X = pd.DataFrame(X, columns=feature_names)[top_features]\n    test_data_processed = pd.DataFrame(test_data_processed, columns=feature_names)[top_features]\n\n    print(\"features extracted\")\n    \n    # retrain the model\n    sample_model.fit(trainX, trainY)\n\n    print(\"features trained\")\n    \n    return sample_model, X, trainX, trainY, testX, test_data_processed","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:11.841104Z","iopub.execute_input":"2024-11-07T18:51:11.841540Z","iopub.status.idle":"2024-11-07T18:51:11.855207Z","shell.execute_reply.started":"2024-11-07T18:51:11.841496Z","shell.execute_reply":"2024-11-07T18:51:11.854130Z"},"trusted":true},"execution_count":188,"outputs":[]},{"cell_type":"markdown","source":"## model intialization\nhere model is intialized","metadata":{}},{"cell_type":"code","source":"### SAMPLE ###\n# -------------------- case X (add the case number here) --------------------\n# # intialize models here as model_1, model_2, perform feature selection and feature importance BEFORE they are inserted in voting classifier\n# \n# # intialize voting classifier\n# model = VotingClassifier(estimators=[['name_of_model', model_1], ['name_of_model', model_2]], voting:'soft', verbose=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:11.856852Z","iopub.execute_input":"2024-11-07T18:51:11.857214Z","iopub.status.idle":"2024-11-07T18:51:11.868033Z","shell.execute_reply.started":"2024-11-07T18:51:11.857174Z","shell.execute_reply":"2024-11-07T18:51:11.866955Z"},"trusted":true},"execution_count":189,"outputs":[]},{"cell_type":"code","source":"# # -------------------- case 161 --------------------\n# # intialize models here as model_1, model_2, perform feature selection and feature importance BEFORE they are inserted in voting classifier\n# model_1 = xgb.XGBClassifier()\n# model_1 = featureImportance(model_1, 35, X, trainX, trainY, testX, test_data_processed)\n# model_1 = BaggingClassifier(estimator=model_1, n_estimators=100, verbose=2)\n# model_2 = lgb.LGBMClassifier(learning_rate=0.01, max_depth=3, n_estimators=1000)\n# model_2 = BaggingClassifier(estimator=model_2, n_estimators=50, verbose=2)\n# # intialize voting classifier\n# model = VotingClassifier(estimators=[['xgb_best_fimp', model_1], ['lgb_best', model_2]], voting='soft', verbose=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:11.869736Z","iopub.execute_input":"2024-11-07T18:51:11.870117Z","iopub.status.idle":"2024-11-07T18:51:11.877537Z","shell.execute_reply.started":"2024-11-07T18:51:11.870076Z","shell.execute_reply":"2024-11-07T18:51:11.876402Z"},"trusted":true},"execution_count":190,"outputs":[]},{"cell_type":"code","source":"# # -------------------- case 162 --------------------\n# # intialize models here as model_1, model_2, perform feature selection and feature importance BEFORE they are inserted in voting classifier\n# xgb = xgb.XGBClassifier()\n# xgb = featureImportance(xgb, 35, X, trainX, trainY, testX, test_data_processed)\n# model_2 = lgb.LGBMClassifier(learning_rate=0.02, max_depth=2, n_estimators=4000)\n# model_1 = BaggingClassifier(estimator=model_2, n_estimators=50, verbose=2)\n# # intialize voting classifier\n# model = VotingClassifier(estimators=[['lgb_bagged', model_1], ['lgb', model_2]], voting='soft', verbose=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:11.879052Z","iopub.execute_input":"2024-11-07T18:51:11.879554Z","iopub.status.idle":"2024-11-07T18:51:11.890691Z","shell.execute_reply.started":"2024-11-07T18:51:11.879511Z","shell.execute_reply":"2024-11-07T18:51:11.889738Z"},"trusted":true},"execution_count":191,"outputs":[]},{"cell_type":"code","source":"# -------------------- case X --------------------\nxgb_m = xgb.XGBClassifier(n_estimators=2000 ,learning_rate= 0.03, max_depth = 4, random_state  = 42, device = \"cuda\")\nxgb_m, X, trainX, trainY, testX, test_data_processed = featureImportance(xgb_m, 40, X, trainX, trainY, testX, test_data_processed)\nmodel_2 = lgb.LGBMClassifier(learning_rate=0.02, max_depth=2, n_estimators=4000 , device='gpu')\nmodel_1 = BaggingClassifier(estimator= model_2, n_estimators=100, verbose=2, n_jobs=-1)\nmodel = VotingClassifier(estimators=[('bg_c', model_1), ('lgb2', model_2)], voting='soft', verbose=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:11.891829Z","iopub.execute_input":"2024-11-07T18:51:11.892183Z","iopub.status.idle":"2024-11-07T18:51:24.297544Z","shell.execute_reply.started":"2024-11-07T18:51:11.892145Z","shell.execute_reply":"2024-11-07T18:51:24.296564Z"},"trusted":true},"execution_count":192,"outputs":[{"name":"stdout","text":"fitting\nextracting features\nIndex(['RecordId', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10',\n       'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20',\n       'X21', 'X22', 'X23', 'X24', 'X25', 'X26', 'X27', 'X28', 'X29', 'X30',\n       'X31', 'X32', 'X33', 'X34', 'X35', 'X36', 'X37', 'X38', 'X39', 'X40',\n       'X41', 'X42', 'X43', 'X44', 'X45', 'X46', 'X47', 'X48', 'X49', 'X50',\n       'X51', 'X52', 'X53', 'X54', 'X55', 'X56', 'X57', 'X58', 'X59', 'X60',\n       'X61', 'X62', 'X63', 'X64', 'X65', 'X66', 'X67', 'X68', 'X69', 'X70',\n       'X71', 'X72', 'X73', 'X74', 'X75', 'X76', 'X77', 'X78'],\n      dtype='object')\n['X69' 'X26' 'X17' 'X78' 'X74' 'X73' 'X18' 'X72' 'X6' 'X75' 'X20' 'X70'\n 'X22' 'X15' 'X59' 'X44' 'X14' 'X39' 'X40' 'X38' 'X16' 'X28' 'X68' 'X32'\n 'X42' 'X43' 'X36' 'X58' 'X45' 'X34' 'X41' 'X46' 'X5' 'X24' 'X37' 'X23'\n 'X35' 'X4' 'X2' 'X12']\nfeatures extracted\nfeatures trained\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"X shape -> \", X.shape)\nprint(\"trainX shape -> \", trainX.shape)\nprint(\"testX shape -> \", testX.shape)\nprint(\"test_data_processed shape -> \", test_data_processed.shape)","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:24.299024Z","iopub.execute_input":"2024-11-07T18:51:24.299399Z","iopub.status.idle":"2024-11-07T18:51:24.309307Z","shell.execute_reply.started":"2024-11-07T18:51:24.299354Z","shell.execute_reply":"2024-11-07T18:51:24.308347Z"},"trusted":true},"execution_count":193,"outputs":[{"name":"stdout","text":"X shape ->  (246122, 40)\ntrainX shape ->  (172285, 40)\ntestX shape ->  (73837, 40)\ntest_data_processed shape ->  (105482, 40)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Bagging intialization\nhere we will introduce and intialize bagging","metadata":{}},{"cell_type":"code","source":"# model = BaggingClassifier(estimator=model, n_estimators=50, verbose=2)\n# -- ","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:24.311148Z","iopub.execute_input":"2024-11-07T18:51:24.312380Z","iopub.status.idle":"2024-11-07T18:51:24.321523Z","shell.execute_reply.started":"2024-11-07T18:51:24.312331Z","shell.execute_reply":"2024-11-07T18:51:24.320760Z"},"trusted":true},"execution_count":194,"outputs":[]},{"cell_type":"markdown","source":"## model running\nhere we run the model","metadata":{}},{"cell_type":"code","source":"# fit the model\nmodel.fit(trainX, trainY)","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:51:24.322598Z","iopub.execute_input":"2024-11-07T18:51:24.322885Z","iopub.status.idle":"2024-11-07T19:18:59.395039Z","shell.execute_reply.started":"2024-11-07T18:51:24.322854Z","shell.execute_reply":"2024-11-07T19:18:59.393987Z"},"trusted":true},"execution_count":195,"outputs":[{"name":"stderr","text":"[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n","output_type":"stream"},{"name":"stdout","text":"Building estimator 1 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.017581 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002589 -> initscore=-5.953994\n[LightGBM] [Info] Start training from score -5.953994\nBuilding estimator 2 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.017183 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002641 -> initscore=-5.933963\n[LightGBM] [Info] Start training from score -5.933963\nBuilding estimator 3 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.017629 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002635 -> initscore=-5.936170\n[LightGBM] [Info] Start training from score -5.936170\nBuilding estimator 4 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.012647 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002513 -> initscore=-5.983651\n[LightGBM] [Info] Start training from score -5.983651\nBuilding estimator 5 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.015725 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002699 -> initscore=-5.912165\n[LightGBM] [Info] Start training from score -5.912165\nBuilding estimator 6 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.016732 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002560 -> initscore=-5.965297\n[LightGBM] [Info] Start training from score -5.965297\nBuilding estimator 7 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.016629 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002409 -> initscore=-6.026215\n[LightGBM] [Info] Start training from score -6.026215\nBuilding estimator 8 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.020547 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002589 -> initscore=-5.953994\n[LightGBM] [Info] Start training from score -5.953994\nBuilding estimator 9 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.014755 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002595 -> initscore=-5.951749\n[LightGBM] [Info] Start training from score -5.951749\nBuilding estimator 10 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.013835 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002566 -> initscore=-5.963027\n[LightGBM] [Info] Start training from score -5.963027\nBuilding estimator 11 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\nBuilding estimator 1 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.022001 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002507 -> initscore=-5.985969\n[LightGBM] [Info] Start training from score -5.985969\nBuilding estimator 2 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.027740 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002595 -> initscore=-5.951749\n[LightGBM] [Info] Start training from score -5.951749\nBuilding estimator 3 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.018015 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002618 -> initscore=-5.942817\n[LightGBM] [Info] Start training from score -5.942817\nBuilding estimator 4 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.019560 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002606 -> initscore=-5.947273\n[LightGBM] [Info] Start training from score -5.947273\nBuilding estimator 5 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.016729 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002751 -> initscore=-5.892943\n[LightGBM] [Info] Start training from score -5.892943\nBuilding estimator 6 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.013758 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002554 -> initscore=-5.967573\n[LightGBM] [Info] Start training from score -5.967573\nBuilding estimator 7 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.020433 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002693 -> initscore=-5.914324\n[LightGBM] [Info] Start training from score -5.914324\nBuilding estimator 8 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.016377 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002600 -> initscore=-5.949508\n[LightGBM] [Info] Start training from score -5.949508\nBuilding estimator 9 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.012244 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002542 -> initscore=-5.972141\n[LightGBM] [Info] Start training from score -5.972141\nBuilding estimator 10 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.017615 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002426 -> initscore=-6.018995\n[LightGBM] [Info] Start training from score -6.018995\nBuilding estimator 11 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\nBuilding estimator 1 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.013525 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002612 -> initscore=-5.945042\n[LightGBM] [Info] Start training from score -5.945042\nBuilding estimator 2 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.011847 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002386 -> initscore=-6.035924\n[LightGBM] [Info] Start training from score -6.035924\nBuilding estimator 3 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.016996 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002362 -> initscore=-6.045727\n[LightGBM] [Info] Start training from score -6.045727\nBuilding estimator 4 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.019232 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002566 -> initscore=-5.963027\n[LightGBM] [Info] Start training from score -5.963027\nBuilding estimator 5 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.023286 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002554 -> initscore=-5.967573\n[LightGBM] [Info] Start training from score -5.967573\nBuilding estimator 6 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.019619 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002629 -> initscore=-5.938380\n[LightGBM] [Info] Start training from score -5.938380\nBuilding estimator 7 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.017270 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002612 -> initscore=-5.945042\n[LightGBM] [Info] Start training from score -5.945042\nBuilding estimator 8 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.013595 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002386 -> initscore=-6.035924\n[LightGBM] [Info] Start training from score -6.035924\nBuilding estimator 9 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.025833 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002391 -> initscore=-6.033488\n[LightGBM] [Info] Start training from score -6.033488\nBuilding estimator 10 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.014745 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002589 -> initscore=-5.953994\n[LightGBM] [Info] Start training from score -5.953994\nBuilding estimator 11 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\nBuilding estimator 1 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.024237 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002722 -> initscore=-5.903577\n[LightGBM] [Info] Start training from score -5.903577\nBuilding estimator 2 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.019041 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002711 -> initscore=-5.907862\n[LightGBM] [Info] Start training from score -5.907862\nBuilding estimator 3 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.027933 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002548 -> initscore=-5.969855\n[LightGBM] [Info] Start training from score -5.969855\nBuilding estimator 4 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.014498 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002641 -> initscore=-5.933963\n[LightGBM] [Info] Start training from score -5.933963\nBuilding estimator 5 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.017831 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002658 -> initscore=-5.927374\n[LightGBM] [Info] Start training from score -5.927374\nBuilding estimator 6 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.026432 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002432 -> initscore=-6.016599\n[LightGBM] [Info] Start training from score -6.016599\nBuilding estimator 7 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.014818 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002740 -> initscore=-5.897183\n[LightGBM] [Info] Start training from score -5.897183\nBuilding estimator 8 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.016700 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002426 -> initscore=-6.018995\n[LightGBM] [Info] Start training from score -6.018995\nBuilding estimator 9 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.018395 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002676 -> initscore=-5.920828\n[LightGBM] [Info] Start training from score -5.920828\nBuilding estimator 10 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.019063 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002641 -> initscore=-5.933963\n[LightGBM] [Info] Start training from score -5.933963\nBuilding estimator 11 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.016655 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002548 -> initscore=-5.969855\n[LightGBM] [Info] Start training from score -5.969855\nBuilding estimator 12 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.010419 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002455 -> initscore=-6.007075\n[LightGBM] [Info] Start training from score -6.007075\nBuilding estimator 13 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.017533 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002484 -> initscore=-5.995295\n[LightGBM] [Info] Start training from score -5.995295\nBuilding estimator 14 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.020781 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002391 -> initscore=-6.033488\n[LightGBM] [Info] Start training from score -6.033488\nBuilding estimator 15 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.014846 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002490 -> initscore=-5.992955\n[LightGBM] [Info] Start training from score -5.992955\nBuilding estimator 16 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.018337 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002519 -> initscore=-5.981339\n[LightGBM] [Info] Start training from score -5.981339\nBuilding estimator 17 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.018195 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002595 -> initscore=-5.951749\n[LightGBM] [Info] Start training from score -5.951749\nBuilding estimator 18 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.022922 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002803 -> initscore=-5.874081\n[LightGBM] [Info] Start training from score -5.874081\nBuilding estimator 19 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.019178 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002438 -> initscore=-6.014210\n[LightGBM] [Info] Start training from score -6.014210\nBuilding estimator 20 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.015642 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002542 -> initscore=-5.972141\n[LightGBM] [Info] Start training from score -5.972141\nBuilding estimator 21 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.015811 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002821 -> initscore=-5.867872\n[LightGBM] [Info] Start training from score -5.867872\nBuilding estimator 12 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.020553 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002536 -> initscore=-5.974432\n[LightGBM] [Info] Start training from score -5.974432\nBuilding estimator 13 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.022626 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002519 -> initscore=-5.981339\n[LightGBM] [Info] Start training from score -5.981339\nBuilding estimator 14 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.014361 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002751 -> initscore=-5.892943\n[LightGBM] [Info] Start training from score -5.892943\nBuilding estimator 15 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.022150 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002531 -> initscore=-5.976729\n[LightGBM] [Info] Start training from score -5.976729\nBuilding estimator 16 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.019711 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002478 -> initscore=-5.997640\n[LightGBM] [Info] Start training from score -5.997640\nBuilding estimator 17 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.019436 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002664 -> initscore=-5.925187\n[LightGBM] [Info] Start training from score -5.925187\nBuilding estimator 18 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.018456 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002560 -> initscore=-5.965297\n[LightGBM] [Info] Start training from score -5.965297\nBuilding estimator 19 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.014235 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002513 -> initscore=-5.983651\n[LightGBM] [Info] Start training from score -5.983651\nBuilding estimator 20 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.015429 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002542 -> initscore=-5.972141\n[LightGBM] [Info] Start training from score -5.972141\nBuilding estimator 21 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.015632 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002676 -> initscore=-5.920828\n[LightGBM] [Info] Start training from score -5.920828\nBuilding estimator 12 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.013474 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002635 -> initscore=-5.936170\n[LightGBM] [Info] Start training from score -5.936170\nBuilding estimator 13 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.017155 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002519 -> initscore=-5.981339\n[LightGBM] [Info] Start training from score -5.981339\nBuilding estimator 14 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.011813 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002635 -> initscore=-5.936170\n[LightGBM] [Info] Start training from score -5.936170\nBuilding estimator 15 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.023595 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002606 -> initscore=-5.947273\n[LightGBM] [Info] Start training from score -5.947273\nBuilding estimator 16 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.013490 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002536 -> initscore=-5.974432\n[LightGBM] [Info] Start training from score -5.974432\nBuilding estimator 17 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.015916 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002577 -> initscore=-5.958500\n[LightGBM] [Info] Start training from score -5.958500\nBuilding estimator 18 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.017927 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002821 -> initscore=-5.867872\n[LightGBM] [Info] Start training from score -5.867872\nBuilding estimator 19 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.014558 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002664 -> initscore=-5.925187\n[LightGBM] [Info] Start training from score -5.925187\nBuilding estimator 20 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.014818 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002705 -> initscore=-5.910011\n[LightGBM] [Info] Start training from score -5.910011\nBuilding estimator 21 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.017452 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002838 -> initscore=-5.861701\n[LightGBM] [Info] Start training from score -5.861701\nBuilding estimator 12 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.020677 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002409 -> initscore=-6.026215\n[LightGBM] [Info] Start training from score -6.026215\nBuilding estimator 13 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.012065 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002763 -> initscore=-5.888721\n[LightGBM] [Info] Start training from score -5.888721\nBuilding estimator 14 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.011945 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002467 -> initscore=-6.002346\n[LightGBM] [Info] Start training from score -6.002346\nBuilding estimator 15 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.019553 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002513 -> initscore=-5.983651\n[LightGBM] [Info] Start training from score -5.983651\nBuilding estimator 16 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.019938 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002612 -> initscore=-5.945042\n[LightGBM] [Info] Start training from score -5.945042\nBuilding estimator 17 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.014728 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002734 -> initscore=-5.899310\n[LightGBM] [Info] Start training from score -5.899310\nBuilding estimator 18 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.013301 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002589 -> initscore=-5.953994\n[LightGBM] [Info] Start training from score -5.953994\nBuilding estimator 19 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.020201 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002595 -> initscore=-5.951749\n[LightGBM] [Info] Start training from score -5.951749\nBuilding estimator 20 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.014086 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002641 -> initscore=-5.933963\n[LightGBM] [Info] Start training from score -5.933963\nBuilding estimator 21 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed: 27.2min finished\n","output_type":"stream"},{"name":"stdout","text":"[Voting] ..................... (1 of 2) Processing bg_c, total=27.2min\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.005736 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002595 -> initscore=-5.951749\n[LightGBM] [Info] Start training from score -5.951749\n[Voting] ..................... (2 of 2) Processing lgb2, total=  21.6s\n","output_type":"stream"},{"execution_count":195,"output_type":"execute_result","data":{"text/plain":"VotingClassifier(estimators=[('bg_c',\n                              BaggingClassifier(estimator=LGBMClassifier(device='gpu',\n                                                                         learning_rate=0.02,\n                                                                         max_depth=2,\n                                                                         n_estimators=4000),\n                                                n_estimators=100, n_jobs=-1,\n                                                verbose=2)),\n                             ('lgb2',\n                              LGBMClassifier(device='gpu', learning_rate=0.02,\n                                             max_depth=2, n_estimators=4000))],\n                 verbose=True, voting='soft')","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;bg_c&#x27;,\n                              BaggingClassifier(estimator=LGBMClassifier(device=&#x27;gpu&#x27;,\n                                                                         learning_rate=0.02,\n                                                                         max_depth=2,\n                                                                         n_estimators=4000),\n                                                n_estimators=100, n_jobs=-1,\n                                                verbose=2)),\n                             (&#x27;lgb2&#x27;,\n                              LGBMClassifier(device=&#x27;gpu&#x27;, learning_rate=0.02,\n                                             max_depth=2, n_estimators=4000))],\n                 verbose=True, voting=&#x27;soft&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;bg_c&#x27;,\n                              BaggingClassifier(estimator=LGBMClassifier(device=&#x27;gpu&#x27;,\n                                                                         learning_rate=0.02,\n                                                                         max_depth=2,\n                                                                         n_estimators=4000),\n                                                n_estimators=100, n_jobs=-1,\n                                                verbose=2)),\n                             (&#x27;lgb2&#x27;,\n                              LGBMClassifier(device=&#x27;gpu&#x27;, learning_rate=0.02,\n                                             max_depth=2, n_estimators=4000))],\n                 verbose=True, voting=&#x27;soft&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>bg_c</label></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(device=&#x27;gpu&#x27;, learning_rate=0.02, max_depth=2, n_estimators=4000)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(device=&#x27;gpu&#x27;, learning_rate=0.02, max_depth=2, n_estimators=4000)</pre></div></div></div></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lgb2</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(device=&#x27;gpu&#x27;, learning_rate=0.02, max_depth=2, n_estimators=4000)</pre></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"# predict using this model\ny_pred = model.predict(testX)","metadata":{"execution":{"iopub.status.busy":"2024-11-07T19:18:59.400636Z","iopub.execute_input":"2024-11-07T19:18:59.400964Z","iopub.status.idle":"2024-11-07T19:28:02.764613Z","shell.execute_reply.started":"2024-11-07T19:18:59.400926Z","shell.execute_reply":"2024-11-07T19:28:02.763503Z"},"trusted":true},"execution_count":196,"outputs":[{"name":"stderr","text":"[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  9.0min finished\n","output_type":"stream"}]},{"cell_type":"code","source":"# display the accuracy of this prediction\naccuracy = accuracy_score(testY, y_pred)\nprint(\"model accuracy = \", accuracy, \"   \")\n\n# now lets calculate the ROC AUC score according to this prediction\nroc_score = roc_auc_score(testY, y_pred)\nprint(\"roc score = \", roc_score, \"   \")","metadata":{"execution":{"iopub.status.busy":"2024-11-07T19:28:02.765980Z","iopub.execute_input":"2024-11-07T19:28:02.766421Z","iopub.status.idle":"2024-11-07T19:28:02.824956Z","shell.execute_reply.started":"2024-11-07T19:28:02.766361Z","shell.execute_reply":"2024-11-07T19:28:02.823924Z"},"trusted":true},"execution_count":197,"outputs":[{"name":"stdout","text":"model accuracy =  0.9972236141771741    \nroc score =  0.5271326592834472    \n","output_type":"stream"}]},{"cell_type":"markdown","source":"## predict for test dataset\nfit the model and predict for test dataset","metadata":{}},{"cell_type":"code","source":"model.fit(X, Y)","metadata":{"execution":{"iopub.status.busy":"2024-11-07T19:28:02.828193Z","iopub.execute_input":"2024-11-07T19:28:02.828536Z","iopub.status.idle":"2024-11-07T20:05:49.678759Z","shell.execute_reply.started":"2024-11-07T19:28:02.828501Z","shell.execute_reply":"2024-11-07T20:05:49.677594Z"},"trusted":true},"execution_count":198,"outputs":[{"name":"stderr","text":"[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.022716 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002745 -> initscore=-5.895061\n[LightGBM] [Info] Start training from score -5.895061\nBuilding estimator 22 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.015837 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002612 -> initscore=-5.945042\n[LightGBM] [Info] Start training from score -5.945042\nBuilding estimator 23 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.021364 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002618 -> initscore=-5.942817\n[LightGBM] [Info] Start training from score -5.942817\nBuilding estimator 24 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.018696 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002612 -> initscore=-5.945042\n[LightGBM] [Info] Start training from score -5.945042\nBuilding estimator 25 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.018714 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002745 -> initscore=-5.895061\n[LightGBM] [Info] Start training from score -5.895061\nBuilding estimator 1 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.042438 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002608 -> initscore=-5.946382\n[LightGBM] [Info] Start training from score -5.946382\nBuilding estimator 2 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.028034 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002621 -> initscore=-5.941708\n[LightGBM] [Info] Start training from score -5.941708\nBuilding estimator 3 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.045857 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002751 -> initscore=-5.893157\n[LightGBM] [Info] Start training from score -5.893157\nBuilding estimator 4 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.023241 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002608 -> initscore=-5.946382\n[LightGBM] [Info] Start training from score -5.946382\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.015796 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002560 -> initscore=-5.965297\n[LightGBM] [Info] Start training from score -5.965297\nBuilding estimator 22 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.019325 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002763 -> initscore=-5.888721\n[LightGBM] [Info] Start training from score -5.888721\nBuilding estimator 23 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.016301 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002415 -> initscore=-6.023803\n[LightGBM] [Info] Start training from score -6.023803\nBuilding estimator 24 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.021735 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002525 -> initscore=-5.979031\n[LightGBM] [Info] Start training from score -5.979031\nBuilding estimator 25 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.016463 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002519 -> initscore=-5.981339\n[LightGBM] [Info] Start training from score -5.981339\nBuilding estimator 1 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.036552 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002661 -> initscore=-5.926283\n[LightGBM] [Info] Start training from score -5.926283\nBuilding estimator 2 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.021895 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002759 -> initscore=-5.890199\n[LightGBM] [Info] Start training from score -5.890199\nBuilding estimator 3 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.021178 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002596 -> initscore=-5.951079\n[LightGBM] [Info] Start training from score -5.951079\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nBuilding estimator 4 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.022408 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002584 -> initscore=-5.955797\n[LightGBM] [Info] Start training from score -5.955797\nBuilding estimator 5 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.030528 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002564 -> initscore=-5.963710\n[LightGBM] [Info] Start training from score -5.963710\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.014494 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002693 -> initscore=-5.914324\n[LightGBM] [Info] Start training from score -5.914324\nBuilding estimator 22 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.015533 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002391 -> initscore=-6.033488\n[LightGBM] [Info] Start training from score -6.033488\nBuilding estimator 23 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.009399 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002420 -> initscore=-6.021396\n[LightGBM] [Info] Start training from score -6.021396\nBuilding estimator 24 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.014156 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002444 -> initscore=-6.011826\n[LightGBM] [Info] Start training from score -6.011826\nBuilding estimator 25 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.019145 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002566 -> initscore=-5.963027\n[LightGBM] [Info] Start training from score -5.963027\nBuilding estimator 1 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.059908 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002556 -> initscore=-5.966892\n[LightGBM] [Info] Start training from score -5.966892\nBuilding estimator 2 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9191\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.026709 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002653 -> initscore=-5.929349\n[LightGBM] [Info] Start training from score -5.929349\nBuilding estimator 3 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.019209 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002755 -> initscore=-5.891677\n[LightGBM] [Info] Start training from score -5.891677\nBuilding estimator 4 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.028045 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002686 -> initscore=-5.917140\n[LightGBM] [Info] Start training from score -5.917140\nBuilding estimator 5 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.024088 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002694 -> initscore=-5.914110\n[LightGBM] [Info] Start training from score -5.914110\nBuilding estimator 6 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.022113 secs. 1 sparse feature groups\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.018271 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002600 -> initscore=-5.949508\n[LightGBM] [Info] Start training from score -5.949508\nBuilding estimator 22 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.020060 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002653 -> initscore=-5.929566\n[LightGBM] [Info] Start training from score -5.929566\nBuilding estimator 23 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.018193 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002641 -> initscore=-5.933963\n[LightGBM] [Info] Start training from score -5.933963\nBuilding estimator 24 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.016603 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002624 -> initscore=-5.940596\n[LightGBM] [Info] Start training from score -5.940596\nBuilding estimator 25 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 447, number of negative: 171838\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 172285, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (5.26 MB) transferred to GPU in 0.017248 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002473 -> initscore=-5.999990\n[LightGBM] [Info] Start training from score -5.999990\nBuilding estimator 1 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.031406 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002588 -> initscore=-5.954221\n[LightGBM] [Info] Start training from score -5.954221\nBuilding estimator 2 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.031280 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002633 -> initscore=-5.937056\n[LightGBM] [Info] Start training from score -5.937056\nBuilding estimator 3 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.021139 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002726 -> initscore=-5.902083\n[LightGBM] [Info] Start training from score -5.902083\nBuilding estimator 4 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.018567 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002568 -> initscore=-5.962122\n[LightGBM] [Info] Start training from score -5.962122\nBuilding estimator 5 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.037219 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002608 -> initscore=-5.946382\n[LightGBM] [Info] Start training from score -5.946382\nBuilding estimator 6 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.025399 secs. 1 sparse feature groups\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nBuilding estimator 5 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.025437 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002442 -> initscore=-6.012543\n[LightGBM] [Info] Start training from score -6.012543\nBuilding estimator 6 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.024631 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002747 -> initscore=-5.894639\n[LightGBM] [Info] Start training from score -5.894639\nBuilding estimator 7 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.033683 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002844 -> initscore=-5.859654\n[LightGBM] [Info] Start training from score -5.859654\nBuilding estimator 8 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.023042 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002633 -> initscore=-5.937056\n[LightGBM] [Info] Start training from score -5.937056\nBuilding estimator 9 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.019527 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002527 -> initscore=-5.978112\n[LightGBM] [Info] Start training from score -5.978112\nBuilding estimator 10 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.021212 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002698 -> initscore=-5.912599\n[LightGBM] [Info] Start training from score -5.912599\nBuilding estimator 11 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.032799 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002629 -> initscore=-5.938604\n[LightGBM] [Info] Start training from score -5.938604\nBuilding estimator 12 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.025277 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002665 -> initscore=-5.924753\n[LightGBM] [Info] Start training from score -5.924753\nBuilding estimator 13 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.039962 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002637 -> initscore=-5.935510\n[LightGBM] [Info] Start training from score -5.935510\nBuilding estimator 14 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.021105 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002787 -> initscore=-5.879914\n[LightGBM] [Info] Start training from score -5.879914\nBuilding estimator 15 of 25 for this parallel run (total 100)...\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nBuilding estimator 6 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.034345 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002738 -> initscore=-5.897610\n[LightGBM] [Info] Start training from score -5.897610\nBuilding estimator 7 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.025267 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002483 -> initscore=-5.996000\n[LightGBM] [Info] Start training from score -5.996000\nBuilding estimator 8 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.022372 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002556 -> initscore=-5.966892\n[LightGBM] [Info] Start training from score -5.966892\nBuilding estimator 9 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.024175 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002580 -> initscore=-5.957374\n[LightGBM] [Info] Start training from score -5.957374\nBuilding estimator 10 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.023865 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002678 -> initscore=-5.920178\n[LightGBM] [Info] Start training from score -5.920178\nBuilding estimator 11 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.022499 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002568 -> initscore=-5.962122\n[LightGBM] [Info] Start training from score -5.962122\nBuilding estimator 12 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.042417 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002600 -> initscore=-5.949511\n[LightGBM] [Info] Start training from score -5.949511\nBuilding estimator 13 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.035772 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002734 -> initscore=-5.899099\n[LightGBM] [Info] Start training from score -5.899099\nBuilding estimator 14 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.019751 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002710 -> initscore=-5.908079\n[LightGBM] [Info] Start training from score -5.908079\nBuilding estimator 15 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.024971 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002665 -> initscore=-5.924753\n[LightGBM] [Info] Start training from score -5.924753\nBuilding estimator 16 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002568 -> initscore=-5.962122\n[LightGBM] [Info] Start training from score -5.962122\nBuilding estimator 7 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.019252 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002608 -> initscore=-5.946382\n[LightGBM] [Info] Start training from score -5.946382\nBuilding estimator 8 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.022592 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002852 -> initscore=-5.856793\n[LightGBM] [Info] Start training from score -5.856793\nBuilding estimator 9 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.024521 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002678 -> initscore=-5.920178\n[LightGBM] [Info] Start training from score -5.920178\nBuilding estimator 10 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9191\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.019469 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002621 -> initscore=-5.941708\n[LightGBM] [Info] Start training from score -5.941708\nBuilding estimator 11 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.023945 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002588 -> initscore=-5.954221\n[LightGBM] [Info] Start training from score -5.954221\nBuilding estimator 12 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.028143 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002596 -> initscore=-5.951079\n[LightGBM] [Info] Start training from score -5.951079\nBuilding estimator 13 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.024407 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002613 -> initscore=-5.944822\n[LightGBM] [Info] Start training from score -5.944822\nBuilding estimator 14 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.019428 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002625 -> initscore=-5.940155\n[LightGBM] [Info] Start training from score -5.940155\nBuilding estimator 15 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.018896 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002633 -> initscore=-5.937056\n[LightGBM] [Info] Start training from score -5.937056\nBuilding estimator 16 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.021476 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002584 -> initscore=-5.955797\n[LightGBM] [Info] Start training from score -5.955797\nBuilding estimator 17 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002434 -> initscore=-6.015884\n[LightGBM] [Info] Start training from score -6.015884\nBuilding estimator 7 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.025504 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002515 -> initscore=-5.982959\n[LightGBM] [Info] Start training from score -5.982959\nBuilding estimator 8 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.024401 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002649 -> initscore=-5.930885\n[LightGBM] [Info] Start training from score -5.930885\nBuilding estimator 9 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.024319 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002495 -> initscore=-5.991090\n[LightGBM] [Info] Start training from score -5.991090\nBuilding estimator 10 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.022431 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002649 -> initscore=-5.930885\n[LightGBM] [Info] Start training from score -5.930885\nBuilding estimator 11 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.031421 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002247 -> initscore=-6.095975\n[LightGBM] [Info] Start training from score -6.095975\nBuilding estimator 12 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.022319 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002401 -> initscore=-6.029362\n[LightGBM] [Info] Start training from score -6.029362\nBuilding estimator 13 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.024080 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002682 -> initscore=-5.918658\n[LightGBM] [Info] Start training from score -5.918658\nBuilding estimator 14 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.027732 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002714 -> initscore=-5.906577\n[LightGBM] [Info] Start training from score -5.906577\nBuilding estimator 15 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.021897 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002426 -> initscore=-6.019237\n[LightGBM] [Info] Start training from score -6.019237\nBuilding estimator 16 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.020349 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002767 -> initscore=-5.887250\n[LightGBM] [Info] Start training from score -5.887250\nBuilding estimator 17 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.024403 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002755 -> initscore=-5.891677\n[LightGBM] [Info] Start training from score -5.891677\nBuilding estimator 17 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.024857 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002795 -> initscore=-5.876995\n[LightGBM] [Info] Start training from score -5.876995\nBuilding estimator 18 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.023992 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002556 -> initscore=-5.966892\n[LightGBM] [Info] Start training from score -5.966892\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nBuilding estimator 19 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.033758 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002653 -> initscore=-5.929349\n[LightGBM] [Info] Start training from score -5.929349\nBuilding estimator 20 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.020698 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002686 -> initscore=-5.917140\n[LightGBM] [Info] Start training from score -5.917140\nBuilding estimator 21 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.022261 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002787 -> initscore=-5.879914\n[LightGBM] [Info] Start training from score -5.879914\nBuilding estimator 22 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9191\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.028328 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002629 -> initscore=-5.938604\n[LightGBM] [Info] Start training from score -5.938604\nBuilding estimator 23 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.022622 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002637 -> initscore=-5.935510\n[LightGBM] [Info] Start training from score -5.935510\nBuilding estimator 24 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.026909 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002519 -> initscore=-5.981341\n[LightGBM] [Info] Start training from score -5.981341\nBuilding estimator 16 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.019333 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002751 -> initscore=-5.893157\n[LightGBM] [Info] Start training from score -5.893157\nBuilding estimator 17 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.022248 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002682 -> initscore=-5.918658\n[LightGBM] [Info] Start training from score -5.918658\nBuilding estimator 18 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.021344 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002633 -> initscore=-5.937056\n[LightGBM] [Info] Start training from score -5.937056\nBuilding estimator 19 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.037233 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002641 -> initscore=-5.933966\n[LightGBM] [Info] Start training from score -5.933966\nBuilding estimator 20 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.019119 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002556 -> initscore=-5.966892\n[LightGBM] [Info] Start training from score -5.966892\nBuilding estimator 21 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.026498 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002747 -> initscore=-5.894639\n[LightGBM] [Info] Start training from score -5.894639\nBuilding estimator 22 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.024907 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002718 -> initscore=-5.905077\n[LightGBM] [Info] Start training from score -5.905077\nBuilding estimator 23 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.019705 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002840 -> initscore=-5.861088\n[LightGBM] [Info] Start training from score -5.861088\nBuilding estimator 24 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.023675 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002515 -> initscore=-5.982959\n[LightGBM] [Info] Start training from score -5.982959\nBuilding estimator 25 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed: 37.3min finished\n","output_type":"stream"},{"name":"stdout","text":"[Voting] ..................... (1 of 2) Processing bg_c, total=37.3min\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.007521 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002637 -> initscore=-5.935510\n[LightGBM] [Info] Start training from score -5.935510\n[Voting] ..................... (2 of 2) Processing lgb2, total=  29.7s\n","output_type":"stream"},{"execution_count":198,"output_type":"execute_result","data":{"text/plain":"VotingClassifier(estimators=[('bg_c',\n                              BaggingClassifier(estimator=LGBMClassifier(device='gpu',\n                                                                         learning_rate=0.02,\n                                                                         max_depth=2,\n                                                                         n_estimators=4000),\n                                                n_estimators=100, n_jobs=-1,\n                                                verbose=2)),\n                             ('lgb2',\n                              LGBMClassifier(device='gpu', learning_rate=0.02,\n                                             max_depth=2, n_estimators=4000))],\n                 verbose=True, voting='soft')","text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;bg_c&#x27;,\n                              BaggingClassifier(estimator=LGBMClassifier(device=&#x27;gpu&#x27;,\n                                                                         learning_rate=0.02,\n                                                                         max_depth=2,\n                                                                         n_estimators=4000),\n                                                n_estimators=100, n_jobs=-1,\n                                                verbose=2)),\n                             (&#x27;lgb2&#x27;,\n                              LGBMClassifier(device=&#x27;gpu&#x27;, learning_rate=0.02,\n                                             max_depth=2, n_estimators=4000))],\n                 verbose=True, voting=&#x27;soft&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;bg_c&#x27;,\n                              BaggingClassifier(estimator=LGBMClassifier(device=&#x27;gpu&#x27;,\n                                                                         learning_rate=0.02,\n                                                                         max_depth=2,\n                                                                         n_estimators=4000),\n                                                n_estimators=100, n_jobs=-1,\n                                                verbose=2)),\n                             (&#x27;lgb2&#x27;,\n                              LGBMClassifier(device=&#x27;gpu&#x27;, learning_rate=0.02,\n                                             max_depth=2, n_estimators=4000))],\n                 verbose=True, voting=&#x27;soft&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>bg_c</label></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(device=&#x27;gpu&#x27;, learning_rate=0.02, max_depth=2, n_estimators=4000)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(device=&#x27;gpu&#x27;, learning_rate=0.02, max_depth=2, n_estimators=4000)</pre></div></div></div></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lgb2</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(device=&#x27;gpu&#x27;, learning_rate=0.02, max_depth=2, n_estimators=4000)</pre></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"test_prediction = model.predict_proba(test_data_processed)\n\ntest_prediction=test_prediction[:, 1]\n\nprint(test_prediction)","metadata":{"execution":{"iopub.status.busy":"2024-11-07T20:05:49.680047Z","iopub.execute_input":"2024-11-07T20:05:49.680365Z","iopub.status.idle":"2024-11-07T20:17:44.559737Z","shell.execute_reply.started":"2024-11-07T20:05:49.680332Z","shell.execute_reply":"2024-11-07T20:17:44.558586Z"},"trusted":true},"execution_count":199,"outputs":[{"name":"stderr","text":"[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed: 11.8min finished\n","output_type":"stream"},{"name":"stdout","text":"[5.44377748e-04 1.03651346e-03 8.04367056e-06 ... 1.61800283e-04\n 2.15797627e-05 8.77525322e-05]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## write into csv\nnow we write the predictions into the csv file","metadata":{}},{"cell_type":"code","source":"sample_data = pd.read_csv(r\"/kaggle/input/sample-sub/sample_submission.csv\")\n\nsample_data['Y'] = test_prediction\nsample_data\n\nsample_data.to_csv(r\"/kaggle/working/voting1.csv\", index=False)\nsample_data","metadata":{"execution":{"iopub.status.busy":"2024-11-07T20:17:44.561095Z","iopub.execute_input":"2024-11-07T20:17:44.561425Z","iopub.status.idle":"2024-11-07T20:17:44.997165Z","shell.execute_reply.started":"2024-11-07T20:17:44.561390Z","shell.execute_reply":"2024-11-07T20:17:44.996081Z"},"trusted":true},"execution_count":200,"outputs":[{"execution_count":200,"output_type":"execute_result","data":{"text/plain":"        RecordId         Y\n0         300001  0.000544\n1         300002  0.001037\n2         300003  0.000008\n3         300004  0.000826\n4         300005  0.000060\n...          ...       ...\n105477    405478  0.000010\n105478    405479  0.359138\n105479    405480  0.000162\n105480    405481  0.000022\n105481    405482  0.000088\n\n[105482 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RecordId</th>\n      <th>Y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>300001</td>\n      <td>0.000544</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>300002</td>\n      <td>0.001037</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>300003</td>\n      <td>0.000008</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>300004</td>\n      <td>0.000826</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>300005</td>\n      <td>0.000060</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>105477</th>\n      <td>405478</td>\n      <td>0.000010</td>\n    </tr>\n    <tr>\n      <th>105478</th>\n      <td>405479</td>\n      <td>0.359138</td>\n    </tr>\n    <tr>\n      <th>105479</th>\n      <td>405480</td>\n      <td>0.000162</td>\n    </tr>\n    <tr>\n      <th>105480</th>\n      <td>405481</td>\n      <td>0.000022</td>\n    </tr>\n    <tr>\n      <th>105481</th>\n      <td>405482</td>\n      <td>0.000088</td>\n    </tr>\n  </tbody>\n</table>\n<p>105482 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-11-07T20:17:44.998473Z","iopub.execute_input":"2024-11-07T20:17:44.998846Z","iopub.status.idle":"2024-11-07T20:17:45.027975Z","shell.execute_reply.started":"2024-11-07T20:17:44.998813Z","shell.execute_reply":"2024-11-07T20:17:45.026898Z"},"trusted":true},"execution_count":201,"outputs":[{"execution_count":201,"output_type":"execute_result","data":{"text/plain":"VotingClassifier(estimators=[('bg_c',\n                              BaggingClassifier(estimator=LGBMClassifier(device='gpu',\n                                                                         learning_rate=0.02,\n                                                                         max_depth=2,\n                                                                         n_estimators=4000),\n                                                n_estimators=100, n_jobs=-1,\n                                                verbose=2)),\n                             ('lgb2',\n                              LGBMClassifier(device='gpu', learning_rate=0.02,\n                                             max_depth=2, n_estimators=4000))],\n                 verbose=True, voting='soft')","text/html":"<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;bg_c&#x27;,\n                              BaggingClassifier(estimator=LGBMClassifier(device=&#x27;gpu&#x27;,\n                                                                         learning_rate=0.02,\n                                                                         max_depth=2,\n                                                                         n_estimators=4000),\n                                                n_estimators=100, n_jobs=-1,\n                                                verbose=2)),\n                             (&#x27;lgb2&#x27;,\n                              LGBMClassifier(device=&#x27;gpu&#x27;, learning_rate=0.02,\n                                             max_depth=2, n_estimators=4000))],\n                 verbose=True, voting=&#x27;soft&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;bg_c&#x27;,\n                              BaggingClassifier(estimator=LGBMClassifier(device=&#x27;gpu&#x27;,\n                                                                         learning_rate=0.02,\n                                                                         max_depth=2,\n                                                                         n_estimators=4000),\n                                                n_estimators=100, n_jobs=-1,\n                                                verbose=2)),\n                             (&#x27;lgb2&#x27;,\n                              LGBMClassifier(device=&#x27;gpu&#x27;, learning_rate=0.02,\n                                             max_depth=2, n_estimators=4000))],\n                 verbose=True, voting=&#x27;soft&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>bg_c</label></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(device=&#x27;gpu&#x27;, learning_rate=0.02, max_depth=2, n_estimators=4000)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(device=&#x27;gpu&#x27;, learning_rate=0.02, max_depth=2, n_estimators=4000)</pre></div></div></div></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lgb2</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(device=&#x27;gpu&#x27;, learning_rate=0.02, max_depth=2, n_estimators=4000)</pre></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}},{"name":"stdout","text":"[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.026837 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002771 -> initscore=-5.885778\n[LightGBM] [Info] Start training from score -5.885778\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.022181 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002608 -> initscore=-5.946382\n[LightGBM] [Info] Start training from score -5.946382\nBuilding estimator 18 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.022282 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002580 -> initscore=-5.957374\n[LightGBM] [Info] Start training from score -5.957374\nBuilding estimator 19 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.025246 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002519 -> initscore=-5.981341\n[LightGBM] [Info] Start training from score -5.981341\nBuilding estimator 20 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.024923 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002824 -> initscore=-5.866843\n[LightGBM] [Info] Start training from score -5.866843\nBuilding estimator 21 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.031205 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002694 -> initscore=-5.914110\n[LightGBM] [Info] Start training from score -5.914110\nBuilding estimator 22 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.031212 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002608 -> initscore=-5.946382\n[LightGBM] [Info] Start training from score -5.946382\nBuilding estimator 23 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.022697 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002564 -> initscore=-5.963710\n[LightGBM] [Info] Start training from score -5.963710\nBuilding estimator 24 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9191\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.018470 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002576 -> initscore=-5.958954\n[LightGBM] [Info] Start training from score -5.958954\nBuilding estimator 25 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.021852 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002499 -> initscore=-5.989458\n[LightGBM] [Info] Start training from score -5.989458\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.018550 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002816 -> initscore=-5.869733\n[LightGBM] [Info] Start training from score -5.869733\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nBuilding estimator 18 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.021892 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002751 -> initscore=-5.893157\n[LightGBM] [Info] Start training from score -5.893157\nBuilding estimator 19 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.015652 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002686 -> initscore=-5.917140\n[LightGBM] [Info] Start training from score -5.917140\nBuilding estimator 20 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.014708 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002535 -> initscore=-5.974894\n[LightGBM] [Info] Start training from score -5.974894\nBuilding estimator 21 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.019055 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002783 -> initscore=-5.881377\n[LightGBM] [Info] Start training from score -5.881377\nBuilding estimator 22 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.018760 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002584 -> initscore=-5.955797\n[LightGBM] [Info] Start training from score -5.955797\nBuilding estimator 23 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.028899 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002714 -> initscore=-5.906577\n[LightGBM] [Info] Start training from score -5.906577\nBuilding estimator 24 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.025121 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002816 -> initscore=-5.869733\n[LightGBM] [Info] Start training from score -5.869733\nBuilding estimator 25 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.019803 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002808 -> initscore=-5.872631\n[LightGBM] [Info] Start training from score -5.872631\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.017257 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002633 -> initscore=-5.937056\n[LightGBM] [Info] Start training from score -5.937056\nBuilding estimator 25 of 25 for this parallel run (total 100)...\n[LightGBM] [Info] Number of positive: 649, number of negative: 245473\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9190\n[LightGBM] [Info] Number of data points in the train set: 246122, number of used features: 40\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 30 dense feature groups (7.51 MB) transferred to GPU in 0.022123 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002710 -> initscore=-5.908079\n[LightGBM] [Info] Start training from score -5.908079\n","output_type":"stream"}]}]}