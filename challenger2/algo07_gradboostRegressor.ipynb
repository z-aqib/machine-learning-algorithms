{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd67a471-fbf2-4cee-9b17-65cc82d7886f",
   "metadata": {
    "papermill": {
     "duration": 0.013014,
     "end_time": "2024-11-07T20:19:46.538794",
     "exception": false,
     "start_time": "2024-11-07T20:19:46.525780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importing Libraries\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "in this part we will install all the necessary libraries on command prompt and then import the necessary functions from those libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd54b756",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:16:32.093336Z",
     "iopub.status.busy": "2024-11-30T22:16:32.092822Z",
     "iopub.status.idle": "2024-11-30T22:16:33.443912Z",
     "shell.execute_reply": "2024-11-30T22:16:33.443064Z",
     "shell.execute_reply.started": "2024-11-30T22:16:32.093291Z"
    },
    "papermill": {
     "duration": 5.853121,
     "end_time": "2024-11-07T20:19:52.405091",
     "exception": false,
     "start_time": "2024-11-07T20:19:46.551970",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -------------------- Data Handling --------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# -------------------- Preprocessing --------------------\n",
    "from sklearn.impute import SimpleImputer, KNNImputer  # Handle missing values\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler, Normalizer  # Feature scaling\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, SelectKBest, f_classif, VarianceThreshold, f_regression  # Feature selection\n",
    "from sklearn.decomposition import PCA  # Dimensionality reduction\n",
    "from sklearn.compose import ColumnTransformer  # Preprocessing for different feature types\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder  # Feature transformation\n",
    "from sklearn.pipeline import Pipeline  # Create machine learning pipelines\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "from category_encoders import TargetEncoder, BinaryEncoder\n",
    "\n",
    "# -------------------- Model --------------------\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# -------------------- Model Evaluation --------------------\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, cross_val_score, GridSearchCV, ParameterGrid  # Train-test split, cross-validation, grid search\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, make_scorer  # Regression metrics\n",
    "\n",
    "# -------------------- Warning Handling --------------------\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c8984b",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1ca4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_datetime():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92c27a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Filter\n",
    "def correlationFilter(X, test_data_processed, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Removes highly correlated features from the dataset based on a given threshold.\n",
    "\n",
    "    Parameters:\n",
    "        X: Full dataset as a DataFrame.\n",
    "        test_data_processed: Processed test data as a DataFrame.\n",
    "        threshold: Correlation threshold above which features are considered highly correlated.\n",
    "\n",
    "    Returns:\n",
    "        Filtered datasets (X and test_data_processed) with reduced multicollinearity.\n",
    "    \"\"\"\n",
    "    print(\"Calculating correlation matrix...\")\n",
    "\n",
    "    # Compute the correlation matrix\n",
    "    corr_matrix = X.corr().abs()\n",
    "\n",
    "    # Extract the upper triangle of the correlation matrix\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    # Identify columns to drop based on the threshold\n",
    "    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > threshold)]\n",
    "\n",
    "    print(f\"Features to drop due to high correlation (> {threshold}):\", to_drop)\n",
    "\n",
    "    # Drop the highly correlated features\n",
    "    X = X.drop(columns=to_drop)\n",
    "    test_data_processed = test_data_processed.drop(columns=to_drop)\n",
    "\n",
    "    print(\"Highly correlated features removed.\")\n",
    "    print(\"New dataset shape:\", X.shape)\n",
    "\n",
    "    return X, test_data_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9601648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance Filter\n",
    "def varianceFilter(X, test_data_processed, threshold=0.001):\n",
    "    \"\"\"\n",
    "    Removes features with low variance from the dataset based on a given threshold.\n",
    "\n",
    "    Parameters:\n",
    "        X: Full dataset as a DataFrame.\n",
    "        test_data_processed: Processed test data as a DataFrame.\n",
    "        threshold: Variance threshold below which features are considered low variance.\n",
    "\n",
    "    Returns:\n",
    "        Filtered datasets (X and test_data_processed) with low variance features removed.\n",
    "    \"\"\"\n",
    "    print(\"Calculating feature variances...\")\n",
    "\n",
    "    # Compute the variance of each feature\n",
    "    feature_variances = X.var(axis=0)\n",
    "\n",
    "    # Display variance statistics\n",
    "    print(feature_variances.describe())\n",
    "\n",
    "    # Identify features with low variance\n",
    "    low_variance_columns = feature_variances[feature_variances < threshold].index.tolist()\n",
    "\n",
    "    print(f\"Features with variance below {threshold}: {low_variance_columns}\")\n",
    "\n",
    "    # Remove the low variance features\n",
    "    X = X.drop(columns=low_variance_columns)\n",
    "    test_data_processed = test_data_processed.drop(columns=low_variance_columns)\n",
    "\n",
    "    print(\"Low variance features removed.\")\n",
    "    print(\"New dataset shape:\", X.shape)\n",
    "\n",
    "    return X, test_data_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402de817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(train_data, test_data, target_column='price_doc', variance_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Apply PCA to reduce dimensionality of the dataset while preserving the specified variance threshold.\n",
    "\n",
    "    Parameters:\n",
    "        train_data (DataFrame): The training data, including features and target.\n",
    "        test_data (DataFrame): The test data, including features.\n",
    "        target_column (str): The name of the target column in the data (default is 'price_doc').\n",
    "        variance_threshold (float): The threshold for the cumulative variance to retain (default is 0.95).\n",
    "\n",
    "    Returns:\n",
    "        train_data (DataFrame): The training data with PCA-reduced features and target.\n",
    "        test_data (DataFrame): The test data with PCA-reduced features.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Starting PCA process...\")\n",
    "\n",
    "    # Separate features and target variable\n",
    "    print(\"Separating features and target variable...\")\n",
    "    train_features = train_data.drop(columns=[target_column])\n",
    "    train_target = train_data[target_column]\n",
    "\n",
    "    test_features = test_data.drop(columns=[target_column])\n",
    "\n",
    "    print(f\"Train features shape: {train_features.shape}\")\n",
    "    print(f\"Test features shape: {test_features.shape}\")\n",
    "\n",
    "    # Perform PCA to determine the optimal number of components\n",
    "    print(\"Fitting PCA to training data...\")\n",
    "    pca = PCA()\n",
    "    pca.fit(train_features)  # Fit PCA on training data\n",
    "\n",
    "    # Plot explained variance ratio to decide on optimal components\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('PCA: Explained Variance vs Number of Components')\n",
    "    plt.show()\n",
    "\n",
    "    # Select the number of components that explain the desired variance\n",
    "    print(f\"Applying PCA with {variance_threshold*100}% explained variance...\")\n",
    "    pca = PCA(n_components=variance_threshold)\n",
    "    train_features_pca = pca.fit_transform(train_features)\n",
    "    test_features_pca = pca.transform(test_features)\n",
    "\n",
    "    print(f\"Train features shape after PCA: {train_features_pca.shape}\")\n",
    "    print(f\"Test features shape after PCA: {test_features_pca.shape}\")\n",
    "\n",
    "    # Reconstruct the train_data and test_data with PCA-reduced features and target\n",
    "    print(\"Reconstructing train and test datasets with PCA-transformed features...\")\n",
    "    train_data_pca = pd.DataFrame(train_features_pca)\n",
    "    train_data_pca[target_column] = train_target.reset_index(drop=True)\n",
    "\n",
    "    test_data_pca = pd.DataFrame(test_features_pca)\n",
    "    test_data_pca[target_column] = test_data[target_column].reset_index(drop=True)\n",
    "\n",
    "    print(f\"Train data shape after PCA: {train_data_pca.shape}\")\n",
    "    print(f\"Test data shape after PCA: {test_data_pca.shape}\")\n",
    "\n",
    "    return train_data_pca, test_data_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba08960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward and Backward Selection\n",
    "def fbselection(direction, sample_model, features, X, trainX, trainY, testX, test_data_processed):\n",
    "    \"\"\"\n",
    "    Performs forward or backward feature selection.\n",
    "\n",
    "    Parameters:\n",
    "        direction: 'forward' or 'backward' for the selection method.\n",
    "        sample_model: The model to use for feature selection.\n",
    "        features: Number of features to select.\n",
    "        X: Full dataset.\n",
    "        trainX: Training feature dataset.\n",
    "        trainY: Training target dataset.\n",
    "        testX: Testing feature dataset.\n",
    "        test_data_processed: Processed test data.\n",
    "\n",
    "    Returns:\n",
    "        Updated model and adjusted datasets.\n",
    "    \"\"\"\n",
    "    print(\"Starting forward/backward selection...\")\n",
    "\n",
    "    # Define Sequential Feature Selector\n",
    "    selection = SequentialFeatureSelector(\n",
    "        sample_model,\n",
    "        direction=direction,\n",
    "        n_features_to_select=features,\n",
    "        scoring='roc_auc'\n",
    "    )\n",
    "\n",
    "    return modelSelector(sample_model, selection, X, trainX, trainY, testX, test_data_processed)\n",
    "\n",
    "\n",
    "# Model Selector for applying the transformation\n",
    "def modelSelector(sample_model, selection, X, trainX, trainY, testX, test_data_processed):\n",
    "    \"\"\"\n",
    "    Applies the given selection method to transform datasets.\n",
    "\n",
    "    Parameters:\n",
    "        sample_model: The model to use for feature selection.\n",
    "        selection: Feature selection object (e.g., SequentialFeatureSelector, SelectKBest).\n",
    "        X: Full dataset.\n",
    "        trainX: Training feature dataset.\n",
    "        trainY: Training target dataset.\n",
    "        testX: Testing feature dataset.\n",
    "        test_data_processed: Processed test data.\n",
    "\n",
    "    Returns:\n",
    "        Updated model and adjusted datasets.\n",
    "    \"\"\"\n",
    "    print(\"Extracting features using the selection method...\")\n",
    "\n",
    "    # Fit and transform training data\n",
    "    trainX = selection.fit_transform(trainX, trainY)\n",
    "\n",
    "    print(\"Features extracted, transforming other datasets...\")\n",
    "\n",
    "    # Transform other datasets using the fitted selection object\n",
    "    testX = selection.transform(testX)\n",
    "    test_data_processed = selection.transform(test_data_processed)\n",
    "    X = selection.transform(X)\n",
    "\n",
    "    print(\"All datasets transformed.\")\n",
    "    print(\"X shape -> \", X.shape)\n",
    "    print(\"trainX shape -> \", trainX.shape)\n",
    "    print(\"testX shape -> \", testX.shape)\n",
    "    print(\"test_data_processed shape -> \", test_data.shape)\n",
    "\n",
    "    return sample_model, X, trainX, trainY, testX, test_data_processed\n",
    "\n",
    "\n",
    "# K-Best Selection\n",
    "def kbest(sample_model, features, X, trainX, trainY, testX, test_data_processed):\n",
    "    \"\"\"\n",
    "    Selects the top K features based on statistical tests.\n",
    "\n",
    "    Parameters:\n",
    "        sample_model: The model to use for feature selection.\n",
    "        features: Number of top features to select.\n",
    "        X: Full dataset.\n",
    "        trainX: Training feature dataset.\n",
    "        trainY: Training target dataset.\n",
    "        testX: Testing feature dataset.\n",
    "        test_data_processed: Processed test data.\n",
    "\n",
    "    Returns:\n",
    "        Updated model and adjusted datasets.\n",
    "    \"\"\"\n",
    "    print(\"Starting K-Best feature selection...\")\n",
    "\n",
    "    # Define SelectKBest object\n",
    "    selection = SelectKBest(score_func=f_regression, k=features)\n",
    "\n",
    "    return modelSelector(sample_model, selection, X, trainX, trainY, testX, test_data_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f3b1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Function\n",
    "def featureImportance(sample_model, features, X, trainX, trainY, testX, test_data_processed):\n",
    "    \"\"\"\n",
    "    Adjusts the dataset and model based on feature importance.\n",
    "    \n",
    "    Parameters:\n",
    "        sample_model: The model used for feature importance extraction.\n",
    "        features: Number of top features to select.\n",
    "        X: The full dataset.\n",
    "        trainX: Training feature dataset.\n",
    "        trainY: Training target dataset.\n",
    "        testX: Testing feature dataset.\n",
    "        test_data_processed: Processed test data.\n",
    "\n",
    "    Returns:\n",
    "        Updated model and adjusted datasets.\n",
    "    \"\"\"\n",
    "    print(\"Fitting the model...\")\n",
    "\n",
    "    # Fit the model\n",
    "    sample_model.fit(trainX, trainY)\n",
    "\n",
    "    print(\"Extracting feature importances...\")\n",
    "\n",
    "    # Extract feature importances\n",
    "    importances = sample_model.feature_importances_\n",
    "\n",
    "    # Extract feature names\n",
    "    feature_names = trainX.columns\n",
    "\n",
    "    print(\"Feature names:\", feature_names)\n",
    "\n",
    "    # Create a DataFrame for feature importance\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Extract top features based on importance\n",
    "    top_features = feature_importance_df['Feature'].head(features).values\n",
    "\n",
    "    print(\"Top features:\", top_features)\n",
    "\n",
    "    # Filter datasets to include only top features\n",
    "    trainX = trainX[top_features]\n",
    "    testX = testX[top_features]\n",
    "    X = X[top_features]\n",
    "    test_data_processed = test_data_processed[top_features]\n",
    "\n",
    "    print(\"Top features extracted and datasets updated.\")\n",
    "\n",
    "    # Retrain the model with top features\n",
    "    print(\"Retraining the model with selected features...\")\n",
    "    sample_model.fit(trainX, trainY)\n",
    "\n",
    "    print(\"Model retrained with top features.\")\n",
    "\n",
    "    return sample_model, X, trainX, trainY, testX, test_data_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdeb617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch(param_grid, model, trainX, trainY):\n",
    "    \"\"\"\n",
    "    Performs a grid search to optimize hyperparameters for a given model.\n",
    "\n",
    "    Parameters:\n",
    "        param_grid: Dictionary containing parameter grid for optimization.\n",
    "        model: The machine learning model to optimize.\n",
    "        trainX: Training feature dataset.\n",
    "        trainY: Training target dataset.\n",
    "\n",
    "    Returns:\n",
    "        Optimized model with the best parameters found during grid search.\n",
    "    \"\"\"\n",
    "    print(\"Starting grid search...\")\n",
    "\n",
    "    # Intialize scorer\n",
    "    scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=3,\n",
    "        scoring=scorer,\n",
    "        verbose=3,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    print(\"Grid search initialized.\")\n",
    "\n",
    "    # Fit the grid search on the training data\n",
    "    grid_search.fit(trainX, trainY)\n",
    "\n",
    "    print(\"Grid search fitting completed.\")\n",
    "\n",
    "    # Retrieve the best model found during grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Best model found:\", best_model)\n",
    "\n",
    "    # Retrieve and display the best parameters\n",
    "    best_parameters = grid_search.best_params_\n",
    "    print(\"Best parameters:\", best_parameters)\n",
    "\n",
    "    # Retrieve and display the best score\n",
    "    print(\"Best cross-validated score:\", grid_search.best_score_)\n",
    "\n",
    "    # Assign the best model\n",
    "    model = best_model\n",
    "    print(\"Model assigned. Grid search completed.\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590b2444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_pred, testY):\n",
    "    \"\"\"\n",
    "    Computes and displays various regression metrics for model evaluation.\n",
    "\n",
    "    Parameters:\n",
    "        y_pred: Predicted values.\n",
    "        testY: Actual target values.\n",
    "\n",
    "    Returns:\n",
    "        Root Mean Squared Error (RMSE) of the predictions.\n",
    "    \"\"\"\n",
    "    print(\"Starting to compute metrics...\")\n",
    "\n",
    "    # Mean Squared Error (MSE)\n",
    "    mse = mean_squared_error(testY, y_pred)\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "\n",
    "    # Root Mean Squared Error (RMSE)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "\n",
    "    # Mean Absolute Error (MAE)\n",
    "    mae = mean_absolute_error(testY, y_pred)\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "\n",
    "    # Coefficient of Determination (R² Score)\n",
    "    r2_Score = r2_score(testY, y_pred)\n",
    "    print(f\"Coefficient of Determination (R² Score): {r2_Score:.2f}\")\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d81a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, trainX, trainY, testX, testY):\n",
    "    \"\"\"\n",
    "    Trains the model, evaluates it on the test set, and computes evaluation metrics.\n",
    "\n",
    "    Parameters:\n",
    "        model: The machine learning model to train and evaluate.\n",
    "        trainX: Training feature dataset.\n",
    "        trainY: Training target dataset.\n",
    "        testX: Testing feature dataset.\n",
    "        testY: Testing target dataset.\n",
    "\n",
    "    Returns:\n",
    "        Trained model and the Root Mean Squared Error (RMSE) on the test set.\n",
    "    \"\"\"\n",
    "    print(\"Training model\", get_current_datetime())\n",
    "    model.fit(trainX, trainY)\n",
    "\n",
    "    print(\"Computing score\", get_current_datetime())\n",
    "    print(\"Model score (training set):\", model.score(trainX, trainY))\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(testX)\n",
    "\n",
    "    # Compute metrics\n",
    "    rmse = metrics(y_pred, testY)\n",
    "\n",
    "    return model, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605e7cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFile(model, X, Y, test_data, file_name):\n",
    "    \"\"\"\n",
    "    Fits the model on the provided dataset, predicts on test data, and saves the predictions to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        model: The machine learning model to use for training and prediction.\n",
    "        X: Full feature dataset.\n",
    "        Y: Target dataset.\n",
    "        test_data: Test dataset (should include all features except the target column).\n",
    "        file_name: Name of the output CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(\"Fitting model on X and Y\", get_current_datetime())\n",
    "    model.fit(X, Y)\n",
    "\n",
    "    print(\"Scoring model on X and Y\", get_current_datetime())\n",
    "    score = model.score(X, Y)\n",
    "    print(\"Model training score:\", score)\n",
    "\n",
    "    print(\"Predicting on test data\", get_current_datetime())\n",
    "    test_prediction = model.predict(test_data)#.drop(columns=['price_doc']))\n",
    "    print(\"Predictions:\", test_prediction)\n",
    "\n",
    "    print(\"Preparing sample submission file\", get_current_datetime())\n",
    "    # sample_data = pd.read_csv(r\"/kaggle/input/challenge2/sample_submission.csv\")\n",
    "    sample_data = pd.read_csv(r\"D:\\Users\\DELL\\OneDrive - Institute of Business Administration\\IBA\\sem5\\machine learning\\ipynb notebooks\\challenger2\\iml-fall-2024-challenge-2\\sample_submission.csv\")\n",
    "    sample_data['price_doc'] = test_prediction\n",
    "\n",
    "    print(\"Saving submission file\", get_current_datetime())\n",
    "    # base_path = r\"/kaggle/working/\"\n",
    "    base_path = r\"D:\\Users\\DELL\\OneDrive - Institute of Business Administration\\IBA\\sem5\\machine learning\\ipynb notebooks\\challenger2\\iml-fall-2024-challenge-2\"\n",
    "    full_path = base_path + file_name\n",
    "\n",
    "    sample_data.to_csv(full_path, index=False)\n",
    "    print(f\"File saved at: {full_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091f4d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(model, trainX, trainY, testX, testY, X, Y, test_data, file_name):\n",
    "    model, rmse = run_model(model, trainX, trainY, testX, testY)\n",
    "    createFile(model, X, Y, test_data, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3912ca3f",
   "metadata": {
    "papermill": {
     "duration": 0.011237,
     "end_time": "2024-11-07T20:19:52.458899",
     "exception": false,
     "start_time": "2024-11-07T20:19:52.447662",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Loading\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data shall be loaded into variables as data sets using pandas and csv readers. they will be checked to see if they are loaded properly and will be loaded as 2 sets: train and test as per given in the kaggle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9125e82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:16:33.464089Z",
     "iopub.status.busy": "2024-11-30T22:16:33.463749Z",
     "iopub.status.idle": "2024-11-30T22:16:47.958190Z",
     "shell.execute_reply": "2024-11-30T22:16:47.957140Z",
     "shell.execute_reply.started": "2024-11-30T22:16:33.464053Z"
    },
    "papermill": {
     "duration": 4.591761,
     "end_time": "2024-11-07T20:19:57.062353",
     "exception": false,
     "start_time": "2024-11-07T20:19:52.470592",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_sq</th>\n",
       "      <th>life_sq</th>\n",
       "      <th>floor</th>\n",
       "      <th>product_type</th>\n",
       "      <th>sub_area</th>\n",
       "      <th>area_m</th>\n",
       "      <th>raion_popul</th>\n",
       "      <th>green_zone_part</th>\n",
       "      <th>indust_part</th>\n",
       "      <th>children_preschool</th>\n",
       "      <th>...</th>\n",
       "      <th>cafe_count_5000_price_2500</th>\n",
       "      <th>cafe_count_5000_price_4000</th>\n",
       "      <th>cafe_count_5000_price_high</th>\n",
       "      <th>big_church_count_5000</th>\n",
       "      <th>church_count_5000</th>\n",
       "      <th>mosque_count_5000</th>\n",
       "      <th>leisure_count_5000</th>\n",
       "      <th>sport_count_5000</th>\n",
       "      <th>market_count_5000</th>\n",
       "      <th>price_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Investment</td>\n",
       "      <td>Bibirevo</td>\n",
       "      <td>6407578.100</td>\n",
       "      <td>155572.0</td>\n",
       "      <td>0.189727</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>9576.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5850000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Investment</td>\n",
       "      <td>Nagatinskij Zaton</td>\n",
       "      <td>9589336.912</td>\n",
       "      <td>115352.0</td>\n",
       "      <td>0.372602</td>\n",
       "      <td>0.049637</td>\n",
       "      <td>6880.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Investment</td>\n",
       "      <td>Tekstil'shhiki</td>\n",
       "      <td>4808269.831</td>\n",
       "      <td>101708.0</td>\n",
       "      <td>0.112560</td>\n",
       "      <td>0.118537</td>\n",
       "      <td>5879.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5700000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Investment</td>\n",
       "      <td>Basmannoe</td>\n",
       "      <td>8398460.622</td>\n",
       "      <td>108171.0</td>\n",
       "      <td>0.015234</td>\n",
       "      <td>0.037316</td>\n",
       "      <td>5706.0</td>\n",
       "      <td>...</td>\n",
       "      <td>319.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16331452.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Investment</td>\n",
       "      <td>Nizhegorodskoe</td>\n",
       "      <td>7506452.020</td>\n",
       "      <td>43795.0</td>\n",
       "      <td>0.007670</td>\n",
       "      <td>0.486246</td>\n",
       "      <td>2418.0</td>\n",
       "      <td>...</td>\n",
       "      <td>62.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9100000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 272 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   full_sq  life_sq  floor product_type           sub_area       area_m  \\\n",
       "0     43.0     27.0    4.0   Investment           Bibirevo  6407578.100   \n",
       "1     34.0     19.0    3.0   Investment  Nagatinskij Zaton  9589336.912   \n",
       "2     43.0     29.0    2.0   Investment     Tekstil'shhiki  4808269.831   \n",
       "3     77.0     77.0    4.0   Investment          Basmannoe  8398460.622   \n",
       "4     67.0     46.0   14.0   Investment     Nizhegorodskoe  7506452.020   \n",
       "\n",
       "   raion_popul  green_zone_part  indust_part  children_preschool  ...  \\\n",
       "0     155572.0         0.189727     0.000070              9576.0  ...   \n",
       "1     115352.0         0.372602     0.049637              6880.0  ...   \n",
       "2     101708.0         0.112560     0.118537              5879.0  ...   \n",
       "3     108171.0         0.015234     0.037316              5706.0  ...   \n",
       "4      43795.0         0.007670     0.486246              2418.0  ...   \n",
       "\n",
       "   cafe_count_5000_price_2500  cafe_count_5000_price_4000  \\\n",
       "0                         9.0                         4.0   \n",
       "1                        15.0                         3.0   \n",
       "2                        10.0                         3.0   \n",
       "3                       319.0                       108.0   \n",
       "4                        62.0                        14.0   \n",
       "\n",
       "   cafe_count_5000_price_high  big_church_count_5000  church_count_5000  \\\n",
       "0                         0.0                   13.0               22.0   \n",
       "1                         0.0                   15.0               29.0   \n",
       "2                         0.0                   11.0               27.0   \n",
       "3                        17.0                  135.0              236.0   \n",
       "4                         1.0                   53.0               78.0   \n",
       "\n",
       "   mosque_count_5000  leisure_count_5000  sport_count_5000 market_count_5000  \\\n",
       "0                1.0                 0.0              52.0               4.0   \n",
       "1                1.0                10.0              66.0              14.0   \n",
       "2                0.0                 4.0              67.0              10.0   \n",
       "3                2.0                91.0             195.0              14.0   \n",
       "4                1.0                20.0             113.0              17.0   \n",
       "\n",
       "    price_doc  \n",
       "0   5850000.0  \n",
       "1   6000000.0  \n",
       "2   5700000.0  \n",
       "3  16331452.0  \n",
       "4   9100000.0  \n",
       "\n",
       "[5 rows x 272 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets load the training data set\n",
    "# train_data = pd.read_csv(r\"/kaggle/input/challenge2/train.csv\")\n",
    "train_data = pd.read_csv(r\"D:\\Users\\DELL\\OneDrive - Institute of Business Administration\\IBA\\sem5\\machine learning\\ipynb notebooks\\challenger2\\iml-fall-2024-challenge-2\\train\\train.csv\")\n",
    "\n",
    "# lets also check it by getting the first few rows of the data, there should  be one target variable Y\n",
    "train_data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f55297",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:16:47.960533Z",
     "iopub.status.busy": "2024-11-30T22:16:47.960107Z",
     "iopub.status.idle": "2024-11-30T22:16:53.824969Z",
     "shell.execute_reply": "2024-11-30T22:16:53.823917Z",
     "shell.execute_reply.started": "2024-11-30T22:16:47.960488Z"
    },
    "papermill": {
     "duration": 1.948205,
     "end_time": "2024-11-07T20:19:59.023969",
     "exception": false,
     "start_time": "2024-11-07T20:19:57.075764",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row ID</th>\n",
       "      <th>full_sq</th>\n",
       "      <th>life_sq</th>\n",
       "      <th>floor</th>\n",
       "      <th>product_type</th>\n",
       "      <th>sub_area</th>\n",
       "      <th>area_m</th>\n",
       "      <th>raion_popul</th>\n",
       "      <th>green_zone_part</th>\n",
       "      <th>indust_part</th>\n",
       "      <th>...</th>\n",
       "      <th>cafe_count_5000_price_1500</th>\n",
       "      <th>cafe_count_5000_price_2500</th>\n",
       "      <th>cafe_count_5000_price_4000</th>\n",
       "      <th>cafe_count_5000_price_high</th>\n",
       "      <th>big_church_count_5000</th>\n",
       "      <th>church_count_5000</th>\n",
       "      <th>mosque_count_5000</th>\n",
       "      <th>leisure_count_5000</th>\n",
       "      <th>sport_count_5000</th>\n",
       "      <th>market_count_5000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Row3</td>\n",
       "      <td>89.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Investment</td>\n",
       "      <td>Mitino</td>\n",
       "      <td>1.258354e+07</td>\n",
       "      <td>178473.0</td>\n",
       "      <td>0.194703</td>\n",
       "      <td>0.069753</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Row6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Investment</td>\n",
       "      <td>Sokol'niki</td>\n",
       "      <td>1.032047e+07</td>\n",
       "      <td>57405.0</td>\n",
       "      <td>0.523439</td>\n",
       "      <td>0.042307</td>\n",
       "      <td>...</td>\n",
       "      <td>144.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Row11</td>\n",
       "      <td>38.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Investment</td>\n",
       "      <td>Zapadnoe Degunino</td>\n",
       "      <td>7.632940e+06</td>\n",
       "      <td>78810.0</td>\n",
       "      <td>0.051844</td>\n",
       "      <td>0.437885</td>\n",
       "      <td>...</td>\n",
       "      <td>39.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Row12</td>\n",
       "      <td>43.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Investment</td>\n",
       "      <td>Kuncevo</td>\n",
       "      <td>5.235177e+07</td>\n",
       "      <td>142462.0</td>\n",
       "      <td>0.070662</td>\n",
       "      <td>0.035145</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Row14</td>\n",
       "      <td>31.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Investment</td>\n",
       "      <td>Lefortovo</td>\n",
       "      <td>8.993640e+06</td>\n",
       "      <td>89971.0</td>\n",
       "      <td>0.066941</td>\n",
       "      <td>0.306977</td>\n",
       "      <td>...</td>\n",
       "      <td>205.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 272 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  row ID  full_sq  life_sq  floor product_type           sub_area  \\\n",
       "0   Row3     89.0     50.0    9.0   Investment             Mitino   \n",
       "1   Row6     25.0     14.0   10.0   Investment         Sokol'niki   \n",
       "2  Row11     38.0     19.0   11.0   Investment  Zapadnoe Degunino   \n",
       "3  Row12     43.0     28.0    4.0   Investment            Kuncevo   \n",
       "4  Row14     31.0     21.0    3.0   Investment          Lefortovo   \n",
       "\n",
       "         area_m  raion_popul  green_zone_part  indust_part  ...  \\\n",
       "0  1.258354e+07     178473.0         0.194703     0.069753  ...   \n",
       "1  1.032047e+07      57405.0         0.523439     0.042307  ...   \n",
       "2  7.632940e+06      78810.0         0.051844     0.437885  ...   \n",
       "3  5.235177e+07     142462.0         0.070662     0.035145  ...   \n",
       "4  8.993640e+06      89971.0         0.066941     0.306977  ...   \n",
       "\n",
       "   cafe_count_5000_price_1500  cafe_count_5000_price_2500  \\\n",
       "0                        15.0                        11.0   \n",
       "1                       144.0                        81.0   \n",
       "2                        39.0                         8.0   \n",
       "3                        21.0                        13.0   \n",
       "4                       205.0                        88.0   \n",
       "\n",
       "   cafe_count_5000_price_4000  cafe_count_5000_price_high  \\\n",
       "0                         2.0                         1.0   \n",
       "1                        16.0                         3.0   \n",
       "2                         3.0                         0.0   \n",
       "3                         9.0                         1.0   \n",
       "4                        19.0                         2.0   \n",
       "\n",
       "   big_church_count_5000  church_count_5000  mosque_count_5000  \\\n",
       "0                    4.0                4.0                0.0   \n",
       "1                   38.0               80.0                1.0   \n",
       "2                   10.0                9.0                0.0   \n",
       "3                    7.0               15.0                0.0   \n",
       "4                   63.0              100.0                0.0   \n",
       "\n",
       "   leisure_count_5000  sport_count_5000 market_count_5000  \n",
       "0                 0.0              26.0               3.0  \n",
       "1                27.0             127.0               8.0  \n",
       "2                 0.0              35.0               4.0  \n",
       "3                 2.0              47.0               0.0  \n",
       "4                28.0             132.0              14.0  \n",
       "\n",
       "[5 rows x 272 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets load the test data\n",
    "# test_data = pd.read_csv(r\"/kaggle/input/challenge2/test.csv\")\n",
    "test_data = pd.read_csv(r\"D:\\Users\\DELL\\OneDrive - Institute of Business Administration\\IBA\\sem5\\machine learning\\ipynb notebooks\\challenger2\\iml-fall-2024-challenge-2\\test\\test.csv\")\n",
    "\n",
    "# check if the data has been loaded by getting the first 5 rows - there should be no target variable Y as this is test data\n",
    "test_data.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db89cc8",
   "metadata": {
    "papermill": {
     "duration": 0.012664,
     "end_time": "2024-11-07T20:19:59.050712",
     "exception": false,
     "start_time": "2024-11-07T20:19:59.038048",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "before we start processing this data and using algorithms, we will fix this data first, this is called data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a83e6e",
   "metadata": {},
   "source": [
    "## split data into categorical and numerical\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "categorical will have one-hot and simple imputer of most frequent while numerical will have simple mean imputer and minmax scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "306cc606",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:16:53.826867Z",
     "iopub.status.busy": "2024-11-30T22:16:53.826391Z",
     "iopub.status.idle": "2024-11-30T22:16:54.125483Z",
     "shell.execute_reply": "2024-11-30T22:16:54.124575Z",
     "shell.execute_reply.started": "2024-11-30T22:16:53.826819Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "categorical_cols = train_data.select_dtypes(include=[\"object\"]).columns\n",
    "numerical_cols = train_data.select_dtypes(exclude=[\"object\"]).drop(columns=['price_doc']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90228b33",
   "metadata": {},
   "source": [
    "## Pipelines and Column Transformers\n",
    "\n",
    "Pipelines in machine learning allow for chaining multiple preprocessing steps and modeling into a single object, ensuring that all transformations are applied consistently. ColumnTransformers enable column-specific transformations, allowing different preprocessing techniques for different features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c766ac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_transformer = Pipeline(steps=[\n",
    "#     (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "#     (\"scaler\", MinMaxScaler())\n",
    "# ])\n",
    "\n",
    "# cat_transformer = Pipeline(steps=[\n",
    "#     (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "#     (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b6dfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Column transformer for preprocessing\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         (\"num\", num_transformer, numerical_cols),\n",
    "#         (\"cat\", cat_transformer, categorical_cols)\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37955afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = preprocessor.fit_transform(train_data)\n",
    "# print(\"train completed\")\n",
    "# test_data = preprocessor.transform(test_data)\n",
    "# print(\"test data completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d3b9f4-49f7-4c84-931e-dff9c756dd75",
   "metadata": {},
   "source": [
    "## Imputers\n",
    "Imputers are used to handle missing data in a dataset by filling in missing values with estimated ones. Common strategies include using the mean, median, or most frequent value for numerical data, and the most frequent value for categorical data. Imputation helps ensure that models can be trained without the issue of missing values disrupting the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8f1a00-39b3-4207-9f2b-5ad588eded07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:16:54.126949Z",
     "iopub.status.busy": "2024-11-30T22:16:54.126615Z",
     "iopub.status.idle": "2024-11-30T22:16:55.857529Z",
     "shell.execute_reply": "2024-11-30T22:16:55.856664Z",
     "shell.execute_reply.started": "2024-11-30T22:16:54.126919Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------ Numerical Imputers ------------------\n",
    "\n",
    "# Mean Imputer (Fills missing values with the mean of each column)\n",
    "num_imputer = SimpleImputer(strategy=\"mean\")\n",
    "\n",
    "# Median Imputer (Fills missing values with the median of each column)\n",
    "# num_imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# Most Frequent Imputer (Fills missing values with the most frequent value of each column)\n",
    "# num_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "# Constant Imputer (Fills missing values with a constant value, e.g., 0)\n",
    "# num_imputer = SimpleImputer(strategy=\"constant\", fill_value=0)\n",
    "\n",
    "# KNN Imputer (Fills missing values based on nearest neighbors)\n",
    "# num_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# ------------------ Categorical Imputers ------------------\n",
    "\n",
    "# Most Frequent Imputer (Fills missing values with the most frequent value in each column)\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "# Constant Imputer (Fills missing values with a constant value, e.g., 'Unknown')\n",
    "# cat_imputer = SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")\n",
    "\n",
    "# ------------------ Apply Imputers ------------------\n",
    "\n",
    "# Impute numerical columns\n",
    "train_data[numerical_cols] = num_imputer.fit_transform(train_data[numerical_cols])\n",
    "test_data[numerical_cols] = num_imputer.transform(test_data[numerical_cols])\n",
    "\n",
    "# Impute categorical columns\n",
    "train_data[categorical_cols] = cat_imputer.fit_transform(train_data[categorical_cols])\n",
    "test_data[categorical_cols] = cat_imputer.transform(test_data[categorical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1be982f-df0e-481d-a8ce-9c1cee449a56",
   "metadata": {},
   "source": [
    "## Scalers\n",
    "Scalers are used to normalize or standardize numerical features in a dataset to ensure they are on a similar scale. This is crucial for algorithms that are sensitive to the magnitude of features, such as KNN or gradient-based models. Common scalers include MinMaxScaler, StandardScaler, and RobustScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce83d8bf-4f13-4063-be48-43b1e3fce721",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:16:55.858863Z",
     "iopub.status.busy": "2024-11-30T22:16:55.858545Z",
     "iopub.status.idle": "2024-11-30T22:16:57.439167Z",
     "shell.execute_reply": "2024-11-30T22:16:57.438069Z",
     "shell.execute_reply.started": "2024-11-30T22:16:55.858833Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------ Scalers ------------------\n",
    "\n",
    "# MinMaxScaler (Scales the features to a range [0, 1])\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# StandardScaler (Standardizes the features by removing the mean and scaling to unit variance)\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# MaxAbsScaler (Scales the features by their maximum absolute value, for data that is already centered at zero)\n",
    "# scaler = MaxAbsScaler()\n",
    "\n",
    "# RobustScaler (Scales the features using the median and interquartile range, less sensitive to outliers)\n",
    "# scaler = RobustScaler()\n",
    "\n",
    "# Normalizer (Scales the features to have unit norm, i.e., each sample is scaled to have unit norm)\n",
    "# scaler = Normalizer()\n",
    "\n",
    "# ------------------ Apply Scaler ------------------\n",
    "\n",
    "# Scale numerical columns in training and test data\n",
    "train_data[numerical_cols] = scaler.fit_transform(train_data[numerical_cols])\n",
    "test_data[numerical_cols] = scaler.transform(test_data[numerical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47600083-8cb3-455a-98fd-c65c7d1841d7",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "Encoding is the process of converting categorical variables into numerical representations so that machine learning models can process them. Techniques like One-Hot Encoding, Label Encoding, and Target Encoding are commonly used to convert categorical data into a format suitable for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e90a6a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:16:57.440887Z",
     "iopub.status.busy": "2024-11-30T22:16:57.440534Z",
     "iopub.status.idle": "2024-11-30T22:16:59.588545Z",
     "shell.execute_reply": "2024-11-30T22:16:59.587705Z",
     "shell.execute_reply.started": "2024-11-30T22:16:57.440856Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------ Encoding Methods ------------------\n",
    "\n",
    "# One-Hot Encoding (Creates binary columns for each category, default drop_first=False)\n",
    "train_data = pd.get_dummies(train_data, columns=categorical_cols, drop_first=False)\n",
    "test_data = pd.get_dummies(test_data, columns=categorical_cols, drop_first=False)\n",
    "\n",
    "# Label Encoding (Converts each category into a unique integer value)\n",
    "# label_encoder = LabelEncoder()\n",
    "# for col in categorical_cols:\n",
    "#     train_data[col] = label_encoder.fit_transform(train_data[col])\n",
    "#     test_data[col] = label_encoder.transform(test_data[col])\n",
    "\n",
    "# Ordinal Encoding (Maps categories to ordered integers, requires predefined order)\n",
    "# ordinal_encoder = OrdinalEncoder()\n",
    "# train_data[categorical_cols] = ordinal_encoder.fit_transform(train_data[categorical_cols])\n",
    "# test_data[categorical_cols] = ordinal_encoder.transform(test_data[categorical_cols])\n",
    "\n",
    "# Target Encoding (Encodes categories based on the mean of the target variable)\n",
    "# target_encoder = TargetEncoder()\n",
    "# train_data[categorical_cols] = target_encoder.fit_transform(train_data[categorical_cols], train_data['target_column'])\n",
    "# test_data[categorical_cols] = target_encoder.transform(test_data[categorical_cols])\n",
    "\n",
    "# Binary Encoding (Encodes categories as binary digits)\n",
    "# binary_encoder = BinaryEncoder()\n",
    "# train_data[categorical_cols] = binary_encoder.fit_transform(train_data[categorical_cols])\n",
    "# test_data[categorical_cols] = binary_encoder.transform(test_data[categorical_cols])\n",
    "\n",
    "# ------------------ Align Test Data with Training Data ------------------\n",
    "\n",
    "# Align test data columns with train data columns (fill missing columns with 0)\n",
    "test_data = test_data.reindex(columns=train_data.columns, fill_value=0)\n",
    "\n",
    "# ------------------ Optional: Drop 'price_doc' Column ------------------\n",
    "\n",
    "# Remove the target column if present in test_data\n",
    "# test_data = test_data.drop(columns=['price_doc'], errors=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5e67cf",
   "metadata": {},
   "source": [
    "## correlation matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "i tried getting the correlation matrix but apparently a 2000 columns matrix is very computationally expensive as it performs pairs for all. so dont run it. it takes too long and then fails. i ran for 5 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae908d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:16:59.591699Z",
     "iopub.status.busy": "2024-11-30T22:16:59.591388Z",
     "iopub.status.idle": "2024-11-30T22:16:59.596033Z",
     "shell.execute_reply": "2024-11-30T22:16:59.595027Z",
     "shell.execute_reply.started": "2024-11-30T22:16:59.591671Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # DONT RUN\n",
    "# corr_matrix = train_data.corr()\n",
    "# print(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a46168",
   "metadata": {},
   "source": [
    "## Variance Filter\n",
    "The variance filter is used to remove features with low variance, which provide little information for predictive modeling. Features with very similar values across all observations are considered redundant and can be safely excluded from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb6c7ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:17:02.234826Z",
     "iopub.status.busy": "2024-11-30T22:17:02.234420Z",
     "iopub.status.idle": "2024-11-30T22:17:10.827879Z",
     "shell.execute_reply": "2024-11-30T22:17:10.826918Z",
     "shell.execute_reply.started": "2024-11-30T22:17:02.234792Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Save the target column separately\n",
    "# target_column = train_data['price_doc']\n",
    "\n",
    "# # Drop the target column from the features\n",
    "# train_data_copy = train_data.drop(columns=['price_doc'])\n",
    "\n",
    "# # Call the correlation filter function to filter out highly correlated features\n",
    "# train_data, test_data = varianceFiter(train_data_copy, test_data, 0.01)\n",
    "\n",
    "# # Append the target column back to the filtered data\n",
    "# train_data['price_doc'] = target_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d547f285",
   "metadata": {},
   "source": [
    "## Correlation Filter\n",
    "The correlation filter helps to remove highly correlated features. When two features are highly correlated, they convey similar information, and removing one can help reduce redundancy and improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b824fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the target column separately\n",
    "# target_column = train_data['price_doc']\n",
    "\n",
    "# # Drop the target column from the features\n",
    "# train_data_copy = train_data.drop(columns=['price_doc'])\n",
    "\n",
    "# # Call the correlation filter function to filter out highly correlated features\n",
    "# train_data, test_data = correlationFilter(train_data_copy, test_data, 0.9)\n",
    "\n",
    "# # Append the target column back to the filtered data\n",
    "# train_data['price_doc'] = target_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463d5ca5",
   "metadata": {},
   "source": [
    "## PCA (Principal Component Analysis)\n",
    "PCA is a dimensionality reduction technique that transforms the data into a new coordinate system, where the greatest variances lie along the first axes (principal components). It helps reduce the number of features while retaining most of the data's variance, improving model performance and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d7e6e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:19:58.783304Z",
     "iopub.status.busy": "2024-11-30T22:19:58.782735Z",
     "iopub.status.idle": "2024-11-30T22:19:58.818148Z",
     "shell.execute_reply": "2024-11-30T22:19:58.815910Z",
     "shell.execute_reply.started": "2024-11-30T22:19:58.783243Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = apply_pca(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd65ed6",
   "metadata": {
    "papermill": {
     "duration": 0.012565,
     "end_time": "2024-11-07T20:19:59.480848",
     "exception": false,
     "start_time": "2024-11-07T20:19:59.468283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Splitting - features and targets\n",
    "the data in train_data set is of x1 - x271 columns (271 variables) and one target variable (Y). we must split that data so that we can perform data preprocessing on the features variables (will be referred to as X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565fb08f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:19:58.821774Z",
     "iopub.status.busy": "2024-11-30T22:19:58.821179Z",
     "iopub.status.idle": "2024-11-30T22:19:58.996618Z",
     "shell.execute_reply": "2024-11-30T22:19:58.995737Z",
     "shell.execute_reply.started": "2024-11-30T22:19:58.821712Z"
    },
    "papermill": {
     "duration": 0.123675,
     "end_time": "2024-11-07T20:19:59.617410",
     "exception": false,
     "start_time": "2024-11-07T20:19:59.493735",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_sq</th>\n",
       "      <th>life_sq</th>\n",
       "      <th>floor</th>\n",
       "      <th>area_m</th>\n",
       "      <th>raion_popul</th>\n",
       "      <th>green_zone_part</th>\n",
       "      <th>indust_part</th>\n",
       "      <th>children_preschool</th>\n",
       "      <th>preschool_education_centers_raion</th>\n",
       "      <th>children_school</th>\n",
       "      <th>...</th>\n",
       "      <th>water_1line_yes</th>\n",
       "      <th>big_road1_1line_no</th>\n",
       "      <th>big_road1_1line_yes</th>\n",
       "      <th>railroad_1line_no</th>\n",
       "      <th>railroad_1line_yes</th>\n",
       "      <th>ecology_excellent</th>\n",
       "      <th>ecology_good</th>\n",
       "      <th>ecology_no data</th>\n",
       "      <th>ecology_poor</th>\n",
       "      <th>ecology_satisfactory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.008074</td>\n",
       "      <td>0.003611</td>\n",
       "      <td>0.051949</td>\n",
       "      <td>0.021207</td>\n",
       "      <td>0.624792</td>\n",
       "      <td>0.220726</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.493543</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.536135</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006384</td>\n",
       "      <td>0.002541</td>\n",
       "      <td>0.038962</td>\n",
       "      <td>0.036804</td>\n",
       "      <td>0.460577</td>\n",
       "      <td>0.435610</td>\n",
       "      <td>0.095115</td>\n",
       "      <td>0.352005</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.401322</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008074</td>\n",
       "      <td>0.003878</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.013367</td>\n",
       "      <td>0.404870</td>\n",
       "      <td>0.130052</td>\n",
       "      <td>0.227141</td>\n",
       "      <td>0.299454</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.319270</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.014457</td>\n",
       "      <td>0.010297</td>\n",
       "      <td>0.051949</td>\n",
       "      <td>0.030966</td>\n",
       "      <td>0.431258</td>\n",
       "      <td>0.015692</td>\n",
       "      <td>0.071506</td>\n",
       "      <td>0.290372</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.347872</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.012580</td>\n",
       "      <td>0.006151</td>\n",
       "      <td>0.181821</td>\n",
       "      <td>0.026594</td>\n",
       "      <td>0.168416</td>\n",
       "      <td>0.006804</td>\n",
       "      <td>0.931742</td>\n",
       "      <td>0.117755</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.124029</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181502</th>\n",
       "      <td>0.009012</td>\n",
       "      <td>0.004413</td>\n",
       "      <td>0.038962</td>\n",
       "      <td>0.306263</td>\n",
       "      <td>0.009811</td>\n",
       "      <td>0.686564</td>\n",
       "      <td>0.011151</td>\n",
       "      <td>0.008977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009199</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181503</th>\n",
       "      <td>0.009012</td>\n",
       "      <td>0.004413</td>\n",
       "      <td>0.038962</td>\n",
       "      <td>0.306263</td>\n",
       "      <td>0.009811</td>\n",
       "      <td>0.686564</td>\n",
       "      <td>0.011151</td>\n",
       "      <td>0.008977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009199</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181504</th>\n",
       "      <td>0.009012</td>\n",
       "      <td>0.004413</td>\n",
       "      <td>0.038962</td>\n",
       "      <td>0.306263</td>\n",
       "      <td>0.009811</td>\n",
       "      <td>0.686564</td>\n",
       "      <td>0.011151</td>\n",
       "      <td>0.008977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009199</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181505</th>\n",
       "      <td>0.009012</td>\n",
       "      <td>0.004413</td>\n",
       "      <td>0.038962</td>\n",
       "      <td>0.306263</td>\n",
       "      <td>0.009811</td>\n",
       "      <td>0.686564</td>\n",
       "      <td>0.011151</td>\n",
       "      <td>0.008977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009199</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181506</th>\n",
       "      <td>0.009012</td>\n",
       "      <td>0.004413</td>\n",
       "      <td>0.038962</td>\n",
       "      <td>0.306263</td>\n",
       "      <td>0.009811</td>\n",
       "      <td>0.686564</td>\n",
       "      <td>0.011151</td>\n",
       "      <td>0.008977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009199</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>181507 rows × 2214 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         full_sq   life_sq     floor    area_m  raion_popul  green_zone_part  \\\n",
       "0       0.008074  0.003611  0.051949  0.021207     0.624792         0.220726   \n",
       "1       0.006384  0.002541  0.038962  0.036804     0.460577         0.435610   \n",
       "2       0.008074  0.003878  0.025974  0.013367     0.404870         0.130052   \n",
       "3       0.014457  0.010297  0.051949  0.030966     0.431258         0.015692   \n",
       "4       0.012580  0.006151  0.181821  0.026594     0.168416         0.006804   \n",
       "...          ...       ...       ...       ...          ...              ...   \n",
       "181502  0.009012  0.004413  0.038962  0.306263     0.009811         0.686564   \n",
       "181503  0.009012  0.004413  0.038962  0.306263     0.009811         0.686564   \n",
       "181504  0.009012  0.004413  0.038962  0.306263     0.009811         0.686564   \n",
       "181505  0.009012  0.004413  0.038962  0.306263     0.009811         0.686564   \n",
       "181506  0.009012  0.004413  0.038962  0.306263     0.009811         0.686564   \n",
       "\n",
       "        indust_part  children_preschool  preschool_education_centers_raion  \\\n",
       "0          0.000134            0.493543                           0.384615   \n",
       "1          0.095115            0.352005                           0.384615   \n",
       "2          0.227141            0.299454                           0.307692   \n",
       "3          0.071506            0.290372                           0.538462   \n",
       "4          0.931742            0.117755                           0.153846   \n",
       "...             ...                 ...                                ...   \n",
       "181502     0.011151            0.008977                           0.000000   \n",
       "181503     0.011151            0.008977                           0.000000   \n",
       "181504     0.011151            0.008977                           0.000000   \n",
       "181505     0.011151            0.008977                           0.000000   \n",
       "181506     0.011151            0.008977                           0.000000   \n",
       "\n",
       "        children_school  ...  water_1line_yes  big_road1_1line_no  \\\n",
       "0              0.536135  ...            False                True   \n",
       "1              0.401322  ...            False                True   \n",
       "2              0.319270  ...            False                True   \n",
       "3              0.347872  ...            False                True   \n",
       "4              0.124029  ...            False                True   \n",
       "...                 ...  ...              ...                 ...   \n",
       "181502         0.009199  ...            False                True   \n",
       "181503         0.009199  ...            False                True   \n",
       "181504         0.009199  ...            False                True   \n",
       "181505         0.009199  ...            False                True   \n",
       "181506         0.009199  ...            False                True   \n",
       "\n",
       "        big_road1_1line_yes  railroad_1line_no  railroad_1line_yes  \\\n",
       "0                     False               True               False   \n",
       "1                     False               True               False   \n",
       "2                     False               True               False   \n",
       "3                     False              False                True   \n",
       "4                     False               True               False   \n",
       "...                     ...                ...                 ...   \n",
       "181502                False               True               False   \n",
       "181503                False               True               False   \n",
       "181504                False               True               False   \n",
       "181505                False               True               False   \n",
       "181506                False               True               False   \n",
       "\n",
       "        ecology_excellent  ecology_good  ecology_no data  ecology_poor  \\\n",
       "0                   False          True            False         False   \n",
       "1                    True         False            False         False   \n",
       "2                   False         False            False          True   \n",
       "3                    True         False            False         False   \n",
       "4                   False         False            False          True   \n",
       "...                   ...           ...              ...           ...   \n",
       "181502              False         False             True         False   \n",
       "181503              False         False             True         False   \n",
       "181504              False         False             True         False   \n",
       "181505              False         False             True         False   \n",
       "181506              False         False             True         False   \n",
       "\n",
       "        ecology_satisfactory  \n",
       "0                      False  \n",
       "1                      False  \n",
       "2                      False  \n",
       "3                      False  \n",
       "4                      False  \n",
       "...                      ...  \n",
       "181502                 False  \n",
       "181503                 False  \n",
       "181504                 False  \n",
       "181505                 False  \n",
       "181506                 False  \n",
       "\n",
       "[181507 rows x 2214 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the 'price_doc' column from train_data to get the features (X)\n",
    "X = train_data.drop(columns=['price_doc'])\n",
    "\n",
    "# Display X to confirm the result\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fd0e61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:19:58.998104Z",
     "iopub.status.busy": "2024-11-30T22:19:58.997788Z",
     "iopub.status.idle": "2024-11-30T22:19:59.006929Z",
     "shell.execute_reply": "2024-11-30T22:19:59.005793Z",
     "shell.execute_reply.started": "2024-11-30T22:19:58.998075Z"
    },
    "papermill": {
     "duration": 0.023643,
     "end_time": "2024-11-07T20:19:59.654979",
     "exception": false,
     "start_time": "2024-11-07T20:19:59.631336",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          5850000.0\n",
       "1          6000000.0\n",
       "2          5700000.0\n",
       "3         16331452.0\n",
       "4          9100000.0\n",
       "             ...    \n",
       "181502     3480000.0\n",
       "181503     3480000.0\n",
       "181504     3480000.0\n",
       "181505     3480000.0\n",
       "181506     3480000.0\n",
       "Name: price_doc, Length: 181507, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the target variable 'price_doc' from train_data into Y\n",
    "Y = train_data['price_doc']\n",
    "\n",
    "# Display Y to confirm it contains only the target variable\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edcb49b",
   "metadata": {
    "papermill": {
     "duration": 0.016785,
     "end_time": "2024-11-07T20:20:00.974594",
     "exception": false,
     "start_time": "2024-11-07T20:20:00.957809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Splitting - train and validate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "now our test_data set is of rows with NO target variable whereas the train_data set is WITH target variable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "our rules in machine learning is that we must train half or 70% of the data and then we must check its accuracy using the remaining half or 30% of the data - we can only check accuracy IF we have the answers i.e. the target variable. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "So, what we need to do is, is split the train_data set into 2, by a 70% and 30% ratio. we train the model using the 70% and then test the model using the 30% and then use that model to predict the test_data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "237c64fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:19:59.076075Z",
     "iopub.status.busy": "2024-11-30T22:19:59.075720Z",
     "iopub.status.idle": "2024-11-30T22:19:59.464683Z",
     "shell.execute_reply": "2024-11-30T22:19:59.463579Z",
     "shell.execute_reply.started": "2024-11-30T22:19:59.076044Z"
    },
    "papermill": {
     "duration": 0.273955,
     "end_time": "2024-11-07T20:20:01.264394",
     "exception": false,
     "start_time": "2024-11-07T20:20:00.990439",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# holdout method\n",
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6047d2",
   "metadata": {
    "papermill": {
     "duration": 0.013797,
     "end_time": "2024-11-07T20:20:01.398362",
     "exception": false,
     "start_time": "2024-11-07T20:20:01.384565",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## model intialization\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "here model is intialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "511dc0e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:19:59.492075Z",
     "iopub.status.busy": "2024-11-30T22:19:59.491297Z",
     "iopub.status.idle": "2024-11-30T22:19:59.506105Z",
     "shell.execute_reply": "2024-11-30T22:19:59.505098Z",
     "shell.execute_reply.started": "2024-11-30T22:19:59.492029Z"
    },
    "papermill": {
     "duration": 0.021268,
     "end_time": "2024-11-07T20:20:01.433590",
     "exception": false,
     "start_time": "2024-11-07T20:20:01.412322",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(n_estimators=600, learning_rate=0.01,min_samples_leaf=5, min_samples_split=3, random_state=2,verbose=2,max_features='log2',max_depth= 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "585f7723",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:19:59.507970Z",
     "iopub.status.busy": "2024-11-30T22:19:59.507470Z",
     "iopub.status.idle": "2024-11-30T22:19:59.518916Z",
     "shell.execute_reply": "2024-11-30T22:19:59.517945Z",
     "shell.execute_reply.started": "2024-11-30T22:19:59.507927Z"
    },
    "papermill": {
     "duration": 0.02329,
     "end_time": "2024-11-07T20:20:14.141486",
     "exception": false,
     "start_time": "2024-11-07T20:20:14.118196",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape ->  (181507, 2214)\n",
      "trainX shape ->  (127054, 2214)\n",
      "testX shape ->  (54453, 2214)\n",
      "test_data_processed shape ->  (77789, 2215)\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape -> \", X.shape)\n",
    "print(\"trainX shape -> \", trainX.shape)\n",
    "print(\"testX shape -> \", testX.shape)\n",
    "print(\"test_data_processed shape -> \", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c9426d",
   "metadata": {},
   "source": [
    "# feature selection\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "here we will apply feature selection and feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e26330a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:19:59.533102Z",
     "iopub.status.busy": "2024-11-30T22:19:59.532791Z",
     "iopub.status.idle": "2024-11-30T22:19:59.544955Z",
     "shell.execute_reply": "2024-11-30T22:19:59.543814Z",
     "shell.execute_reply.started": "2024-11-30T22:19:59.533074Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(learning_rate=0.01, max_depth=12, max_features=&#x27;log2&#x27;,\n",
       "                          min_samples_leaf=5, min_samples_split=3,\n",
       "                          n_estimators=600, random_state=2, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;&nbsp;GradientBoostingRegressor<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\">?<span>Documentation for GradientBoostingRegressor</span></a><span class=\"sk-estimator-doc-link \">i<span>Not fitted</span></span></label><div class=\"sk-toggleable__content \"><pre>GradientBoostingRegressor(learning_rate=0.01, max_depth=12, max_features=&#x27;log2&#x27;,\n",
       "                          min_samples_leaf=5, min_samples_split=3,\n",
       "                          n_estimators=600, random_state=2, verbose=2)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.01, max_depth=12, max_features='log2',\n",
       "                          min_samples_leaf=5, min_samples_split=3,\n",
       "                          n_estimators=600, random_state=2, verbose=2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07f3406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'price_doc' column exists in the test_data, and drop it if present\n",
    "# Check if 'price_doc' column exists in the test_data, and drop it if present\n",
    "if 'price_doc' in test_data.columns:\n",
    "    test_data = test_data.drop(columns=['price_doc'])\n",
    "    print(\"'price_doc' column has been dropped.\")\n",
    "else:\n",
    "    print(\"'price_doc' column was not found, nothing to drop.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b543f060",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:19:59.546596Z",
     "iopub.status.busy": "2024-11-30T22:19:59.546197Z",
     "iopub.status.idle": "2024-11-30T22:19:59.553297Z",
     "shell.execute_reply": "2024-11-30T22:19:59.552316Z",
     "shell.execute_reply.started": "2024-11-30T22:19:59.546564Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting K-Best feature selection...\n",
      "Extracting features using the selection method...\n",
      "Features extracted, transforming other datasets...\n",
      "All datasets transformed.\n"
     ]
    }
   ],
   "source": [
    "# apply feature selection here\n",
    "model, X, trainX, trainY, testX, test_data = kbest(model, 200, X, trainX, trainY, testX, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5fc618ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:19:59.555046Z",
     "iopub.status.busy": "2024-11-30T22:19:59.554676Z",
     "iopub.status.idle": "2024-11-30T22:19:59.566870Z",
     "shell.execute_reply": "2024-11-30T22:19:59.565832Z",
     "shell.execute_reply.started": "2024-11-30T22:19:59.555016Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(learning_rate=0.01, max_depth=12, max_features=&#x27;log2&#x27;,\n",
       "                          min_samples_leaf=5, min_samples_split=3,\n",
       "                          n_estimators=600, random_state=2, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;&nbsp;GradientBoostingRegressor<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\">?<span>Documentation for GradientBoostingRegressor</span></a><span class=\"sk-estimator-doc-link \">i<span>Not fitted</span></span></label><div class=\"sk-toggleable__content \"><pre>GradientBoostingRegressor(learning_rate=0.01, max_depth=12, max_features=&#x27;log2&#x27;,\n",
       "                          min_samples_leaf=5, min_samples_split=3,\n",
       "                          n_estimators=600, random_state=2, verbose=2)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.01, max_depth=12, max_features='log2',\n",
       "                          min_samples_leaf=5, min_samples_split=3,\n",
       "                          n_estimators=600, random_state=2, verbose=2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6405d8",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "Grid Search is a technique used for hyperparameter tuning in machine learning models. It systematically tests different combinations of hyperparameters to find the best-performing set, based on a specified performance metric (such as accuracy or mean squared error). GridSearchCV from scikit-learn automates this process by performing cross-validation on each combination to identify the optimal model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58ba6d7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:19:59.591801Z",
     "iopub.status.busy": "2024-11-30T22:19:59.591463Z",
     "iopub.status.idle": "2024-11-30T22:19:59.600628Z",
     "shell.execute_reply": "2024-11-30T22:19:59.599834Z",
     "shell.execute_reply.started": "2024-11-30T22:19:59.591772Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# define hyper parameters of grid\n",
    "# param_grid = {\n",
    "#     'max_depth': [ 1, 2, 3, 4, 5 ]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b969d8af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:19:59.602695Z",
     "iopub.status.busy": "2024-11-30T22:19:59.601951Z",
     "iopub.status.idle": "2024-11-30T22:19:59.611313Z",
     "shell.execute_reply": "2024-11-30T22:19:59.610403Z",
     "shell.execute_reply.started": "2024-11-30T22:19:59.602630Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model = gridsearch(param_grid, model, scorer, trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1178f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c4143",
   "metadata": {
    "papermill": {
     "duration": 0.014144,
     "end_time": "2024-11-07T20:20:14.233534",
     "exception": false,
     "start_time": "2024-11-07T20:20:14.219390",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## model running\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "here we run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6222c904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 2024-12-01 18:38:17\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 469341275895858.5625           20.82m\n",
      "         2 462961142471899.8125           20.15m\n",
      "         3 456739367489211.2500           20.51m\n",
      "         4 450613685305417.9375           19.17m\n",
      "         5 444656510086319.1250           19.19m\n",
      "         6 438812775316429.2500           18.89m\n",
      "         7 433035461955445.0625           18.80m\n",
      "         8 427288698837566.8125           18.41m\n",
      "         9 421783568985588.8125           18.57m\n",
      "        10 416427589000325.0625           18.21m\n",
      "        11 411151514430763.1250           17.90m\n",
      "        12 405944341324282.0625           17.67m\n",
      "        13 400821733245209.3125           17.51m\n",
      "        14 395652407549520.0625           17.37m\n",
      "        15 390655430984203.8750           17.18m\n",
      "        16 385818391466834.6250           16.99m\n",
      "        17 381128526310775.3750           16.82m\n",
      "        18 376514260588036.0000           16.72m\n",
      "        19 371889361011765.8750           16.75m\n",
      "        20 367401865883477.3750           16.62m\n",
      "        21 363128251233006.5000           16.55m\n",
      "        22 358900328378071.2500           16.42m\n",
      "        23 354714601860360.1875           16.34m\n",
      "        24 350617448801597.6250           16.19m\n",
      "        25 346507707564122.3750           16.11m\n",
      "        26 342409031982621.7500           16.08m\n",
      "        27 338443767976689.2500           16.02m\n",
      "        28 334534174542959.7500           15.95m\n",
      "        29 330701152937494.5625           15.84m\n",
      "        30 326984695876424.8750           15.81m\n",
      "        31 323414121586572.2500           15.71m\n",
      "        32 319833804287663.6250           15.66m\n",
      "        33 316282479258117.5625           15.59m\n",
      "        34 312776867856237.4375           15.60m\n",
      "        35 309250299453966.3750           15.53m\n",
      "        36 305923428274082.5625           15.47m\n",
      "        37 302661050167359.8750           15.45m\n",
      "        38 299502418022652.7500           15.42m\n",
      "        39 296141008097689.0625           15.35m\n",
      "        40 292987286979358.1250           15.33m\n",
      "        41 290024915189866.9375           15.30m\n",
      "        42 287049667389096.9375           15.29m\n",
      "        43 284179311451231.2500           15.25m\n",
      "        44 281249194499481.0312           15.21m\n",
      "        45 278334109727812.8125           15.17m\n",
      "        46 275590316185728.0000           15.15m\n",
      "        47 272865621573577.0938           15.10m\n",
      "        48 270050087590863.0312           15.09m\n",
      "        49 267451972363203.3125           15.03m\n",
      "        50 264678723540426.8438           14.98m\n",
      "        51 262166491540351.7188           14.96m\n",
      "        52 259673170371776.7188           14.92m\n",
      "        53 257296243707913.4375           14.88m\n",
      "        54 254917452622995.0625           14.84m\n",
      "        55 252547272969682.3750           14.81m\n",
      "        56 250144268172980.8125           14.77m\n",
      "        57 247817247778129.8750           14.76m\n",
      "        58 245567379966642.3750           14.72m\n",
      "        59 243307849771826.3750           14.66m\n",
      "        60 241119468356109.7188           14.62m\n",
      "        61 238877073220602.5312           14.61m\n",
      "        62 236860491498665.4062           14.58m\n",
      "        63 234770341289539.3125           14.58m\n",
      "        64 232752492820138.1562           14.61m\n",
      "        65 230698267860558.3125           14.56m\n",
      "        66 228748410504402.7812           14.52m\n",
      "        67 226886050718797.0000           14.49m\n",
      "        68 224994400814553.5000           14.45m\n",
      "        69 223171625979739.7812           14.45m\n",
      "        70 221279391955837.1250           14.39m\n",
      "        71 219426665022856.6875           14.35m\n",
      "        72 217689655086217.4375           14.30m\n",
      "        73 215907307349268.4375           14.26m\n",
      "        74 214156847924292.8438           14.22m\n",
      "        75 212429303656593.8125           14.19m\n",
      "        76 210740627279751.5938           14.15m\n",
      "        77 209173305664680.9688           14.11m\n",
      "        78 207620344941437.1250           14.12m\n",
      "        79 205988286618210.3438           14.08m\n",
      "        80 204509940443324.6875           14.07m\n",
      "        81 203044318087055.6562           14.00m\n",
      "        82 201593963765074.7188           13.97m\n",
      "        83 200113986545054.3125           14.13m\n",
      "        84 198629878510023.5625           14.19m\n",
      "        85 197223188759627.8438           17.36m\n",
      "        86 195814594471232.4688           17.49m\n",
      "        87 194366137542487.4375           17.49m\n",
      "        88 193041966959885.4375           17.40m\n",
      "        89 191657739381116.9062           17.29m\n",
      "        90 190342058012556.9062           17.21m\n",
      "        91 189092484826818.6562           17.10m\n",
      "        92 187832536881821.3438           17.00m\n",
      "        93 186524332087084.9062           16.87m\n",
      "        94 185247632913708.8750           16.75m\n",
      "        95 184091283121996.2188           16.63m\n",
      "        96 182903837403647.3750           16.53m\n",
      "        97 181641608139545.0625           16.40m\n",
      "        98 180452834401785.2812           16.28m\n",
      "        99 179326140236726.0625           16.18m\n",
      "       100 178182946934295.0000           16.07m\n",
      "       101 177025491333494.1250           15.96m\n",
      "       102 175837325058951.6250           15.83m\n",
      "       103 174661795151504.4688           15.71m\n",
      "       104 173550902832807.2500           15.60m\n",
      "       105 172453078816278.1250           15.47m\n",
      "       106 171393413827888.4062           15.36m\n",
      "       107 170424378239187.7812           15.24m\n",
      "       108 169391266858594.7812           15.13m\n",
      "       109 168550503367051.5312           15.02m\n",
      "       110 167551513326657.8125           14.92m\n",
      "       111 166664652259006.2812           14.81m\n",
      "       112 165694452680801.0000           14.71m\n",
      "       113 164734961063842.4062           14.60m\n",
      "       114 163872141538391.8125           14.50m\n",
      "       115 163038284185066.2812           14.40m\n",
      "       116 162202205687311.9375           14.31m\n",
      "       117 161459436450536.0312           14.21m\n",
      "       118 160740983988744.1875           14.12m\n",
      "       119 159812767134403.3750           14.03m\n",
      "       120 158985904542184.7188           13.94m\n",
      "       121 158122515877054.2812           13.85m\n",
      "       122 157358044899912.8438           13.76m\n",
      "       123 156406346808419.3438           13.67m\n",
      "       124 155670825457683.6250           13.58m\n",
      "       125 154902725294938.0938           13.49m\n",
      "       126 154164062591271.3750           13.40m\n",
      "       127 153472402913643.9375           13.31m\n",
      "       128 152736629043848.6875           13.23m\n",
      "       129 151991264188306.5625           13.15m\n",
      "       130 151267401125035.2188           13.06m\n",
      "       131 150462235495643.9062           12.98m\n",
      "       132 149733035344904.9062           12.90m\n",
      "       133 149079058585494.7188           12.83m\n",
      "       134 148424317804498.2812           12.75m\n",
      "       135 147672727453390.3125           12.69m\n",
      "       136 146914290515493.4375           12.61m\n",
      "       137 146217664335617.1875           12.54m\n",
      "       138 145562063861976.4062           12.47m\n",
      "       139 144982143298306.3125           12.40m\n",
      "       140 144337815209861.9062           12.34m\n",
      "       141 143666569605366.5000           12.28m\n",
      "       142 143030314637190.5938           12.21m\n",
      "       143 142321439241601.2500           12.13m\n",
      "       144 141805260597298.5312           12.06m\n",
      "       145 141232538191456.8438           12.00m\n",
      "       146 140750488583200.5625           11.93m\n",
      "       147 140195842247128.2969           11.86m\n",
      "       148 139616263975952.8281           11.80m\n",
      "       149 139063660107627.4375           11.73m\n",
      "       150 138485569015056.1094           11.67m\n",
      "       151 137964181058122.3594           11.61m\n",
      "       152 137415669545458.4375           11.55m\n",
      "       153 136825513515444.4688           11.49m\n",
      "       154 136431611099706.3594           11.43m\n",
      "       155 135884754371429.9844           11.37m\n",
      "       156 135419631540031.0156           11.32m\n",
      "       157 134856063127719.8281           11.27m\n",
      "       158 134360488568565.0312           11.21m\n",
      "       159 133870078268523.3281           11.17m\n",
      "       160 133379051489724.0625           11.11m\n",
      "       161 132873827018877.4375           11.06m\n",
      "       162 132435605923258.9219           11.00m\n",
      "       163 131874316413145.4844           10.93m\n",
      "       164 131380472466972.0469           10.87m\n",
      "       165 130870110061487.4375           10.82m\n",
      "       166 130332135405423.3125           10.76m\n",
      "       167 129918881792606.2188           10.70m\n",
      "       168 129530784328177.0625           10.65m\n",
      "       169 128987154258308.3438           10.59m\n",
      "       170 128506894938558.1875           10.53m\n",
      "       171 127969942736166.2344           10.48m\n",
      "       172 127573343043329.8594           10.42m\n",
      "       173 127064969723691.0000           10.37m\n",
      "       174 126651407924393.2500           10.31m\n",
      "       175 126212417030421.4531           10.26m\n",
      "       176 125776907824880.1406           10.21m\n",
      "       177 125477600681248.0469           10.15m\n",
      "       178 125166870365862.4688           10.10m\n",
      "       179 124736589261139.9688           10.05m\n",
      "       180 124324494336211.8125            9.99m\n",
      "       181 123791883529433.9688            9.95m\n",
      "       182 123400327427702.9531            9.90m\n",
      "       183 123018654786914.9844            9.86m\n",
      "       184 122603261587166.0156            9.81m\n",
      "       185 122193751441703.6719            9.77m\n",
      "       186 121776396157726.5312            9.77m\n",
      "       187 121403189759853.0938            9.73m\n",
      "       188 121025200868966.8125            9.71m\n",
      "       189 120669429986383.2344            9.68m\n",
      "       190 120275511759486.7500            9.64m\n",
      "       191 119935991024931.8438            9.60m\n",
      "       192 119518041788433.3281            9.56m\n",
      "       193 119129756783514.6719            9.51m\n",
      "       194 118701388739175.9531            9.47m\n",
      "       195 118358001153809.0781            9.43m\n",
      "       196 117877798926718.8438            9.39m\n",
      "       197 117577517327343.6875            9.34m\n",
      "       198 117167828004248.0625            9.30m\n",
      "       199 116807698668231.2344            9.26m\n",
      "       200 116483223181881.8125            9.21m\n",
      "       201 116193875648541.9219            9.17m\n",
      "       202 115809235158679.0625            9.13m\n",
      "       203 115461904786981.9219            9.08m\n",
      "       204 115132093964914.2812            9.04m\n",
      "       205 114828041657167.3125            9.00m\n",
      "       206 114472684515013.1094            8.96m\n",
      "       207 114120102879555.0469            8.92m\n",
      "       208 113743890856302.0469            8.88m\n",
      "       209 113357583544156.8750            8.84m\n",
      "       210 113002886950900.2031            8.80m\n",
      "       211 112684601510897.9688            8.76m\n",
      "       212 112214118079271.3750            8.72m\n",
      "       213 111988283253422.1094            8.68m\n",
      "       214 111636378729544.8750            8.64m\n",
      "       215 111323271379373.7812            8.60m\n",
      "       216 111004598958999.3281            8.56m\n",
      "       217 110671548316565.6875            8.53m\n",
      "       218 110377714002231.8281            8.49m\n",
      "       219 110084900029872.7344            8.46m\n",
      "       220 109797248277103.1406            8.42m\n",
      "       221 109558496433336.6094            8.39m\n",
      "       222 109318234047424.6250            8.35m\n",
      "       223 108984459525820.1250            8.31m\n",
      "       224 108613536778031.6719            8.28m\n",
      "       225 108317386039239.5312            8.24m\n",
      "       226 108124713389811.1406            8.21m\n",
      "       227 107916293145818.0156            8.18m\n",
      "       228 107636050723776.6875            8.15m\n",
      "       229 107268382328684.9219            8.12m\n",
      "       230 106990262454848.3594            8.09m\n",
      "       231 106636215254391.2500            8.06m\n",
      "       232 106379524659470.3281            8.03m\n",
      "       233 106116903690429.9219            8.00m\n",
      "       234 105833789437670.6406            7.97m\n",
      "       235 105554634149847.5469            7.93m\n",
      "       236 105377046218527.7031            7.90m\n",
      "       237 105144055972636.6719            7.87m\n",
      "       238 104973760616689.9688            7.85m\n",
      "       239 104689997527038.0625            7.85m\n",
      "       240 104366391638454.5312            7.82m\n",
      "       241 104053753583810.8750            7.80m\n",
      "       242 103780975062677.8906            7.81m\n",
      "       243 103545632738305.7969            7.79m\n",
      "       244 103250390320462.1719            7.79m\n",
      "       245 102996072455846.0625            7.80m\n",
      "       246 102797929135926.7969            7.78m\n",
      "       247 102658214966906.9062            7.77m\n",
      "       248 102414860581564.9375            7.75m\n",
      "       249 102175721879221.3750            7.73m\n",
      "       250 101917132858801.0781            7.71m\n",
      "       251 101711528912761.3281            7.70m\n",
      "       252 101583364935628.0312            7.68m\n",
      "       253 101389930767846.8750            7.65m\n",
      "       254 101150565151069.6719            7.62m\n",
      "       255 100972001359884.8125            7.60m\n",
      "       256 100795521571246.7656            7.58m\n",
      "       257 100500338796566.2969            7.55m\n",
      "       258 100377376875602.1250            7.53m\n",
      "       259 100162402877747.1719            7.49m\n",
      "       260 99944914933283.1406            7.47m\n",
      "       261 99684911024656.3281            7.44m\n",
      "       262 99473593723054.4531            7.42m\n",
      "       263 99290922262833.5781            7.40m\n",
      "       264 99004828247271.7656            7.39m\n",
      "       265 98730895020459.6719            7.38m\n",
      "       266 98423514228593.3750            7.37m\n",
      "       267 98235891202094.8438            7.37m\n",
      "       268 97949808003335.2812            7.37m\n",
      "       269 97757141991708.3438            7.36m\n",
      "       270 97579521078070.1250            7.36m\n",
      "       271 97368992679211.9062            7.36m\n",
      "       272 97237412432903.7500            7.34m\n",
      "       273 96976939162957.9844            7.33m\n",
      "       274 96771703117248.0781            7.32m\n",
      "       275 96631171995210.8125            7.29m\n",
      "       276 96488737531519.9219            7.27m\n",
      "       277 96379747386266.5312            7.25m\n",
      "       278 96133627359695.1250            7.24m\n",
      "       279 96010857863892.6719            7.21m\n",
      "       280 95771104443319.8594            7.19m\n",
      "       281 95585393954141.3750            7.16m\n",
      "       282 95375872456347.7656            7.14m\n",
      "       283 95074506402543.7500            7.14m\n",
      "       284 94867137752059.7344            7.12m\n",
      "       285 94707928152145.1250            7.11m\n",
      "       286 94567254222006.7969            7.09m\n",
      "       287 94379219071986.2969            7.08m\n",
      "       288 94117308396157.2344            7.07m\n",
      "       289 93991809547387.6875            7.06m\n",
      "       290 93804329761103.9219            7.07m\n",
      "       291 93576060760089.9688            7.08m\n",
      "       292 93377321737163.4219            7.05m\n",
      "       293 93152806355055.3281            7.02m\n",
      "       294 92982267496289.2500            7.00m\n",
      "       295 92784752992392.9531            6.97m\n",
      "       296 92641526344618.3438            6.94m\n",
      "       297 92429005583574.4531            6.91m\n",
      "       298 92266735848397.4219            6.89m\n",
      "       299 92078397786088.1719            6.86m\n",
      "       300 91842917470930.3281            6.85m\n",
      "       301 91766509431008.0000            6.84m\n",
      "       302 91514194208411.3281            6.83m\n",
      "       303 91340404562725.6562            6.81m\n",
      "       304 91159097109733.5156            6.80m\n",
      "       305 91032446483154.9062            6.78m\n",
      "       306 90910685182561.1406            6.77m\n",
      "       307 90786753993292.7656            6.75m\n",
      "       308 90551354915978.4219            6.73m\n",
      "       309 90307472329752.6719            6.71m\n",
      "       310 90101924955522.2344            6.69m\n",
      "       311 89898579824955.7344            6.67m\n",
      "       312 89748780855165.3125            6.65m\n",
      "       313 89520957926972.6719            6.63m\n",
      "       314 89347537279365.8594            6.61m\n",
      "       315 89123806394059.8281            6.59m\n",
      "       316 88968857610038.0469            6.57m\n",
      "       317 88767115260073.0781            6.55m\n",
      "       318 88525486326065.0000            6.53m\n",
      "       319 88348700297956.7969            6.51m\n",
      "       320 88154797000639.4688            6.50m\n",
      "       321 88052940761522.5625            6.48m\n",
      "       322 87872072560460.7188            6.46m\n",
      "       323 87637296993857.2188            6.44m\n",
      "       324 87488942827008.3906            6.42m\n",
      "       325 87290809646472.9531            6.40m\n",
      "       326 87145454276127.3438            6.38m\n",
      "       327 86942975183436.0625            6.35m\n",
      "       328 86817379650761.7031            6.32m\n",
      "       329 86631716318061.3125            6.29m\n",
      "       330 86469255708738.7500            6.29m\n",
      "       331 86236577659533.9062            6.28m\n",
      "       332 86125629609047.5000            6.26m\n",
      "       333 85929422259244.7031            6.24m\n",
      "       334 85737418114237.1094            6.22m\n",
      "       335 85656722289083.0312            6.22m\n",
      "       336 85530257354349.7500            6.22m\n",
      "       337 85383867060514.9062            6.21m\n",
      "       338 85199149968276.7812            6.21m\n",
      "       339 84958642590849.7500            6.20m\n",
      "       340 84808595469681.7812            6.20m\n",
      "       341 84641800814614.3125            6.19m\n",
      "       342 84438729014132.1562            6.18m\n",
      "       343 84312733541573.8906            6.16m\n",
      "       344 84090797859959.3594            6.16m\n",
      "       345 83897498382892.1406            6.18m\n",
      "       346 83742822743991.4219            6.17m\n",
      "       347 83669772932209.3750            6.18m\n",
      "       348 83457840575265.6094            6.17m\n",
      "       349 83300594029828.6719            6.18m\n",
      "       350 83127019022346.5781            6.17m\n",
      "       351 82896289353974.5156            6.14m\n",
      "       352 82713227354331.4375            6.12m\n",
      "       353 82547193440588.6719            6.10m\n",
      "       354 82355493921397.2500            6.08m\n",
      "       355 82183075529987.2031            6.07m\n",
      "       356 81898551489046.1562            6.07m\n",
      "       357 81664493911353.0312            6.06m\n",
      "       358 81499115671878.7969            6.04m\n",
      "       359 81247930335962.4531            6.03m\n",
      "       360 81055121671813.5312            6.02m\n",
      "       361 80892649545938.9531            6.03m\n",
      "       362 80748963416198.9219            6.03m\n",
      "       363 80618135503361.2969            6.02m\n",
      "       364 80479479048160.2656            6.02m\n",
      "       365 80273042257684.8281            6.00m\n",
      "       366 80037505658416.3594            5.98m\n",
      "       367 79799183651031.2344            5.95m\n",
      "       368 79646342531919.6562            5.93m\n",
      "       369 79448919822546.0000            5.93m\n",
      "       370 79320863920035.7812            5.92m\n",
      "       371 79204915485251.8281            5.89m\n",
      "       372 78971708981889.3281            5.86m\n",
      "       373 78806149526492.8750            5.84m\n",
      "       374 78629997889198.7031            5.81m\n",
      "       375 78547581749041.1094            5.79m\n",
      "       376 78361851907356.2344            5.77m\n",
      "       377 78146418398668.5625            5.76m\n",
      "       378 77956830755582.9844            5.74m\n",
      "       379 77849880347524.9375            5.72m\n",
      "       380 77748739197468.6094            5.70m\n",
      "       381 77614001499668.0000            5.67m\n",
      "       382 77452338125957.3125            5.64m\n",
      "       383 77297647716555.0469            5.62m\n",
      "       384 77033301100347.8750            5.61m\n",
      "       385 76933915000301.5156            5.60m\n",
      "       386 76777413757962.7031            5.58m\n",
      "       387 76665075558816.2188            5.55m\n",
      "       388 76535364669981.6875            5.53m\n",
      "       389 76415247221514.1562            5.50m\n",
      "       390 76305393481754.1094            5.47m\n",
      "       391 76070414580244.9062            5.44m\n",
      "       392 75892360362602.7812            5.42m\n",
      "       393 75743584944732.0781            5.39m\n",
      "       394 75668618935324.3594            5.37m\n",
      "       395 75521794729063.2969            5.36m\n",
      "       396 75292735678987.5625            5.35m\n",
      "       397 75108750952993.1562            5.32m\n",
      "       398 74993181683003.9688            5.30m\n",
      "       399 74810080558806.1406            5.28m\n",
      "       400 74636822785939.7500            5.27m\n",
      "       401 74368554404270.2031            5.25m\n",
      "       402 74239572493659.1875            5.23m\n",
      "       403 74128209851951.2344            5.21m\n",
      "       404 73972060296967.4688            5.19m\n",
      "       405 73793476658207.1250            5.16m\n",
      "       406 73669519171423.9688            5.14m\n",
      "       407 73486214875969.8125            5.12m\n",
      "       408 73360803631710.5312            5.10m\n",
      "       409 73185762130652.3281            5.09m\n",
      "       410 73058833265051.5781            5.08m\n",
      "       411 72857954364111.0156            5.06m\n",
      "       412 72717945924734.5938            5.04m\n",
      "       413 72587491369585.4062            5.01m\n",
      "       414 72403350471647.2500            4.98m\n",
      "       415 72218916166624.5312            4.96m\n",
      "       416 72164572988276.3594            4.93m\n",
      "       417 72022166597616.5469            4.91m\n",
      "       418 71755423216015.7031            4.88m\n",
      "       419 71659398760821.1250            4.86m\n",
      "       420 71512627741529.5156            4.83m\n",
      "       421 71327338142125.8750            4.80m\n",
      "       422 71118887182972.4844            4.78m\n",
      "       423 71073729098216.3281            4.75m\n",
      "       424 70992634090122.1719            4.72m\n",
      "       425 70778521880731.4219            4.69m\n",
      "       426 70654400795673.0469            4.67m\n",
      "       427 70526783766681.5000            4.65m\n",
      "       428 70367974036483.7344            4.63m\n",
      "       429 70271756287256.2969            4.61m\n",
      "       430 70160222033873.5469            4.58m\n",
      "       431 69991376591976.6797            4.56m\n",
      "       432 69856407449755.9141            4.54m\n",
      "       433 69695760062839.3125            4.53m\n",
      "       434 69624601457328.8203            4.51m\n",
      "       435 69523475784472.4062            4.48m\n",
      "       436 69410457522558.0000            4.46m\n",
      "       437 69298214509555.8516            4.44m\n",
      "       438 69159526020396.9844            4.41m\n",
      "       439 69000542892088.3125            4.39m\n",
      "       440 68922798074465.4453            4.36m\n",
      "       441 68765073042026.9453            4.34m\n",
      "       442 68602305383260.1953            4.31m\n",
      "       443 68427605424932.5234            4.29m\n",
      "       444 68268950994925.8906            4.26m\n",
      "       445 68073229129401.6797            4.23m\n",
      "       446 67912671984789.8516            4.20m\n",
      "       447 67752014915679.7656            4.18m\n",
      "       448 67655701812573.2344            4.15m\n",
      "       449 67447005472348.5625            4.13m\n",
      "       450 67322520399005.9062            4.11m\n",
      "       451 67207462714253.0938            4.08m\n",
      "       452 67071828111829.9844            4.06m\n",
      "       453 66930367726565.2734            4.03m\n",
      "       454 66830235669625.8438            4.00m\n",
      "       455 66694466200040.7969            3.98m\n",
      "       456 66579771870702.9375            3.95m\n",
      "       457 66435203608096.0234            3.92m\n",
      "       458 66320505450214.1719            3.89m\n",
      "       459 66249322196681.7734            3.87m\n",
      "       460 66197810849119.5547            3.84m\n",
      "       461 65963731617679.2500            3.82m\n",
      "       462 65835265893691.1172            3.79m\n",
      "       463 65722178948972.4844            3.77m\n",
      "       464 65559404465945.2188            3.74m\n",
      "       465 65411805707772.6172            3.72m\n",
      "       466 65224152988414.4453            3.70m\n",
      "       467 65063822982892.6875            3.69m\n",
      "       468 64956420003195.2188            3.67m\n",
      "       469 64826618330237.1641            3.65m\n",
      "       470 64698022159854.6328            3.63m\n",
      "       471 64552637311242.6016            3.60m\n",
      "       472 64396070970962.8984            3.58m\n",
      "       473 64239547979563.5156            3.56m\n",
      "       474 64148402990303.1641            3.54m\n",
      "       475 64113470935608.9453            3.52m\n",
      "       476 63995928397125.2109            3.49m\n",
      "       477 63872710516298.1719            3.48m\n",
      "       478 63780083369459.7031            3.46m\n",
      "       479 63679580564881.9766            3.43m\n",
      "       480 63487246758794.0781            3.40m\n",
      "       481 63348571625087.7266            3.38m\n",
      "       482 63207263615373.3594            3.36m\n",
      "       483 63126229035730.3047            3.34m\n",
      "       484 62950826797088.1328            3.31m\n",
      "       485 62797578956582.5391            3.29m\n",
      "       486 62661985235876.7188            3.27m\n",
      "       487 62490692003456.0078            3.25m\n",
      "       488 62363389871852.5703            3.22m\n",
      "       489 62293306741815.9766            3.20m\n",
      "       490 62166976371188.2031            3.17m\n",
      "       491 62088298859251.0703            3.15m\n",
      "       492 61998038156864.6875            3.12m\n",
      "       493 61874502591126.4453            3.09m\n",
      "       494 61799886718692.0625            3.07m\n",
      "       495 61671740432059.0625            3.04m\n",
      "       496 61592879867819.4688            3.01m\n",
      "       497 61492576988010.7266            2.98m\n",
      "       498 61395849550834.4141            2.95m\n",
      "       499 61304567504687.7031            2.92m\n",
      "       500 61200475771116.4141            2.90m\n",
      "       501 61099493134025.1875            2.87m\n",
      "       502 60962710576364.5938            2.85m\n",
      "       503 60884727942741.7656            2.82m\n",
      "       504 60765339084121.5625            2.79m\n",
      "       505 60659987885275.6719            2.76m\n",
      "       506 60564621795304.3984            2.73m\n",
      "       507 60419328778648.7734            2.70m\n",
      "       508 60384504266648.4375            2.67m\n",
      "       509 60261586507222.8203            2.65m\n",
      "       510 60103579816319.5703            2.62m\n",
      "       511 60019084895466.6406            2.59m\n",
      "       512 59959616974895.4609            2.56m\n",
      "       513 59876649770655.3906            2.54m\n",
      "       514 59777567442506.0625            2.51m\n",
      "       515 59552520647822.8672            2.48m\n",
      "       516 59488479666518.4297            2.45m\n",
      "       517 59432317313799.2578            2.43m\n",
      "       518 59347966524883.0078            2.40m\n",
      "       519 59243992815202.8750            2.37m\n",
      "       520 59154840354938.6250            2.34m\n",
      "       521 58996010265008.7812            2.31m\n",
      "       522 58877487017281.7812            2.28m\n",
      "       523 58799334751246.7188            2.26m\n",
      "       524 58724659472390.7266            2.23m\n",
      "       525 58654875709970.8281            2.20m\n",
      "       526 58568323475271.3281            2.17m\n",
      "       527 58463508151512.5938            2.14m\n",
      "       528 58292533333096.0703            2.11m\n",
      "       529 58171823326735.6953            2.08m\n",
      "       530 58041715509824.7969            2.05m\n",
      "       531 57938945690010.2578            2.03m\n",
      "       532 57872909563389.1953            2.01m\n",
      "       533 57789476644607.2422            1.98m\n",
      "       534 57685905134878.6797            1.95m\n",
      "       535 57560039017778.5391            1.92m\n",
      "       536 57447160161662.3594            1.89m\n",
      "       537 57397859683876.4453            1.87m\n",
      "       538 57248948051303.7891            1.84m\n",
      "       539 57165578300879.9219            1.81m\n",
      "       540 57087375167835.8984            1.78m\n",
      "       541 57010977558491.9297            1.76m\n",
      "       542 56878935098900.4688            1.73m\n",
      "       543 56797916383225.9453            1.70m\n",
      "       544 56698970443080.0859            1.67m\n",
      "       545 56575251400045.7812            1.64m\n",
      "       546 56517580464718.2188            1.61m\n",
      "       547 56460394087765.1797            1.58m\n",
      "       548 56386445622282.2031            1.55m\n",
      "       549 56306703838631.5547            1.52m\n",
      "       550 56191574688499.7812            1.49m\n",
      "       551 56120161744018.1484            1.46m\n",
      "       552 56040870673014.8672            1.43m\n",
      "       553 55982463754418.4453            1.40m\n",
      "       554 55917484486323.7422            1.37m\n",
      "       555 55845935454116.7344            1.34m\n",
      "       556 55826174704636.9531            1.31m\n",
      "       557 55764837401865.5781            1.28m\n",
      "       558 55696559352368.1719            1.25m\n",
      "       559 55528532391227.7109            1.22m\n",
      "       560 55475696425162.9766            1.19m\n",
      "       561 55416993991644.8516            1.16m\n",
      "       562 55354602812219.3750            1.13m\n",
      "       563 55329409842025.1172            1.10m\n",
      "       564 55277949135210.1016            1.07m\n",
      "       565 55212410217985.8047            1.04m\n",
      "       566 55085587353304.5703            1.01m\n",
      "       567 55040239567734.8281           58.75s\n",
      "       568 54972765636418.8828           56.99s\n",
      "       569 54916775378875.1641           55.25s\n",
      "       570 54844695739689.9844           53.62s\n",
      "       571 54779233616645.8281           51.94s\n",
      "       572 54714803907005.4766           50.18s\n",
      "       573 54632796755001.9375           48.37s\n",
      "       574 54595138498140.4922           46.65s\n",
      "       575 54471288923511.4531           44.88s\n",
      "       576 54400270648338.6797           43.13s\n",
      "       577 54355947633467.7188           41.35s\n",
      "       578 54262352773697.9688           39.59s\n",
      "       579 54224095841891.3438           37.85s\n",
      "       580 54136536706226.9062           36.08s\n",
      "       581 54097135681973.0156           34.29s\n",
      "       582 54033947827169.4688           32.52s\n",
      "       583 53965074537079.2812           30.73s\n",
      "       584 53939735405716.0547           28.92s\n",
      "       585 53884953447671.9141           27.11s\n",
      "       586 53828202361200.6562           25.33s\n",
      "       587 53753957738984.3750           23.52s\n",
      "       588 53711066480087.4375           21.73s\n",
      "       589 53643248108961.8516           19.94s\n",
      "       590 53600768355119.2109           18.15s\n",
      "       591 53528116149174.1719           16.34s\n",
      "       592 53419974940477.7266           14.52s\n",
      "       593 53345981370615.4375           12.70s\n",
      "       594 53264600220508.5547           10.89s\n",
      "       595 53174480419599.3125            9.09s\n",
      "       596 53116592024504.7578            7.28s\n",
      "       597 53078668608031.5469            5.46s\n",
      "       598 53040963917606.5391            3.64s\n",
      "       599 52985056887534.1719            1.82s\n",
      "       600 52939399379532.5234            0.00s\n",
      "Computing score 2024-12-01 18:56:32\n",
      "Model score (training set): 0.8887240550762396\n",
      "Starting to compute metrics...\n",
      "Mean Squared Error (MSE): 163270768278283.12\n",
      "Root Mean Squared Error (RMSE): 12777745.04\n",
      "Mean Absolute Error (MAE): 5499017.21\n",
      "Coefficient of Determination (R² Score): 0.66\n",
      "Fitting model on X and Y 2024-12-01 18:56:48\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 470270185941504.0625           26.22m\n",
      "         2 463921590339096.2500           22.81m\n",
      "         3 457722998192576.3750           25.92m\n",
      "         4 451619421339996.6875           27.04m\n",
      "         5 445728851722598.1875           29.90m\n",
      "         6 439957938910147.1875           30.10m\n",
      "         7 434253101691822.1875           29.50m\n",
      "         8 428619915713718.1250           28.78m\n",
      "         9 423182946570734.2500           28.83m\n",
      "        10 417777809023914.4375           28.70m\n",
      "        11 412493256638353.0625           29.22m\n",
      "        12 407377146010528.8125           29.54m\n",
      "        13 402246121879856.3125           30.25m\n",
      "        14 397193794477675.8750           30.62m\n",
      "        15 392271993530062.2500           30.70m\n",
      "        16 387486473720302.3750           31.23m\n",
      "        17 382838314462135.7500           31.81m\n",
      "        18 378282424522395.4375           31.94m\n",
      "        19 373665424928532.6875           31.94m\n",
      "        20 369157386524620.0000           31.59m\n",
      "        21 364893997713409.1875           31.52m\n",
      "        22 360621595721803.8750           30.97m\n",
      "        23 356493262344797.8750           30.50m\n",
      "        24 352476376883898.5000           29.97m\n",
      "        25 348475065941994.8125           29.61m\n",
      "        26 344440056893454.8750           28.95m\n",
      "        27 340471554091790.8750           28.38m\n",
      "        28 336545952124856.1875           28.26m\n",
      "        29 332775361805284.4375           27.86m\n",
      "        30 329045253914867.0625           27.97m\n",
      "        31 325563416225280.4375           27.80m\n",
      "        32 322054595257462.7500           27.61m\n",
      "        33 318553085394865.6250           27.50m\n",
      "        34 315161081512172.8750           27.56m\n",
      "        35 311790779460084.7500           27.31m\n",
      "        36 308556095632969.1875           26.99m\n",
      "        37 305316066134189.2500           26.64m\n",
      "        38 302263460398161.6875           26.41m\n",
      "        39 299102234304868.7500           26.42m\n",
      "        40 296032012628694.9375           26.59m\n",
      "        41 293154091472338.3125           26.70m\n",
      "        42 290135337968308.2500           26.62m\n",
      "        43 287193432946004.5625           26.32m\n",
      "        44 284443656455618.8125           26.03m\n",
      "        45 281614209267803.0000           26.04m\n",
      "        46 278851138880350.5312           26.17m\n",
      "        47 276103893130610.1250           26.34m\n",
      "        48 273353343572063.0938           26.51m\n",
      "        49 270672088259967.0312           26.53m\n",
      "        50 268021086622126.1562           26.77m\n",
      "        51 265494872640309.9688           27.22m\n",
      "        52 263064633355781.1875           27.24m\n",
      "        53 260613596415754.6562           27.06m\n",
      "        54 258290987796781.1875           26.99m\n",
      "        55 255973447678979.7188           26.74m\n",
      "        56 253605951134733.6875           26.50m\n",
      "        57 251356752066416.8125           26.29m\n",
      "        58 249152173583738.0312           26.04m\n",
      "        59 246944357618941.2500           25.79m\n",
      "        60 244767306489791.0625           25.57m\n",
      "        61 242664556155025.4062           25.35m\n",
      "        62 240672741297502.9062           25.11m\n",
      "        63 238694579265749.1562           24.92m\n",
      "        64 236658060433360.0625           24.72m\n",
      "        65 234771057734907.4062           24.52m\n",
      "        66 232827987808682.1562           24.33m\n",
      "        67 230962293425724.0000           24.14m\n",
      "        68 229104649230484.6250           24.02m\n",
      "        69 227287032904615.7500           23.89m\n",
      "        70 225497129140213.5625           23.74m\n",
      "        71 223716321616677.0000           23.69m\n",
      "        72 222000607445751.5938           23.60m\n",
      "        73 220352122420417.4062           23.52m\n",
      "        74 218678868348645.7500           23.47m\n",
      "        75 217100111062646.4062           23.44m\n",
      "        76 215438167911382.5312           23.56m\n",
      "        77 213969472763313.6562           23.58m\n",
      "        78 212432182868251.1250           23.73m\n",
      "        79 210892946170608.1875           23.79m\n",
      "        80 209323909141256.7188           23.96m\n",
      "        81 207901550670491.0312           24.06m\n",
      "        82 206485746248829.4688           24.17m\n",
      "        83 205059512921880.3125           24.21m\n",
      "        84 203672960432548.5000           24.17m\n",
      "        85 202335142689030.8750           24.15m\n",
      "        86 201041685013077.1250           24.09m\n",
      "        87 199696159879645.8438           23.91m\n",
      "        88 198377588992696.3125           23.80m\n",
      "        89 197056754331953.0938           23.71m\n",
      "        90 195888043501782.6875           23.59m\n",
      "        91 194577762836291.2812           23.42m\n",
      "        92 193425280589529.5625           23.25m\n",
      "        93 192219694954635.0625           23.10m\n",
      "        94 191066234085194.5938           22.96m\n",
      "        95 189985045016505.5625           22.83m\n",
      "        96 188858550792950.6875           22.68m\n",
      "        97 187746401738021.9375           22.53m\n",
      "        98 186701749837061.8750           22.40m\n",
      "        99 185651803026928.5938           22.33m\n",
      "       100 184489216717938.6875           22.26m\n",
      "       101 183448183461424.4375           22.17m\n",
      "       102 182379950642510.4375           22.08m\n",
      "       103 181401128206577.9688           21.98m\n",
      "       104 180383587785285.5625           21.97m\n",
      "       105 179410569809791.6562           21.88m\n",
      "       106 178465775911277.7188           21.79m\n",
      "       107 177532304978556.1562           21.78m\n",
      "       108 176556301551488.9375           21.81m\n",
      "       109 175712296055548.6562           21.76m\n",
      "       110 174837007711671.2188           21.75m\n",
      "       111 173908045793860.8438           21.68m\n",
      "       112 173001407501630.1875           21.64m\n",
      "       113 172080866915491.2188           21.59m\n",
      "       114 171248124848774.2812           21.47m\n",
      "       115 170392821079561.3125           21.36m\n",
      "       116 169571112400530.1250           21.28m\n",
      "       117 168732961951517.8750           21.23m\n",
      "       118 167925652496477.1875           21.18m\n",
      "       119 167070457356984.4375           21.07m\n",
      "       120 166251935409442.2188           20.98m\n",
      "       121 165436726905215.5000           20.91m\n",
      "       122 164683249170766.5312           20.87m\n",
      "       123 163965746594715.6562           20.79m\n",
      "       124 163201606453915.9688           20.70m\n",
      "       125 162445444363937.4062           20.62m\n",
      "       126 161706416305413.1562           20.50m\n",
      "       127 160952287448288.4688           20.43m\n",
      "       128 160306042524791.0000           20.34m\n",
      "       129 159623427279707.0312           20.26m\n",
      "       130 158970211275253.0312           20.16m\n",
      "       131 158273878512849.8750           20.08m\n",
      "       132 157641334455606.5625           20.00m\n",
      "       133 157013357970630.2812           19.90m\n",
      "       134 156385476142938.5625           19.82m\n",
      "       135 155665911284705.0625           19.76m\n",
      "       136 154988477564865.6875           19.70m\n",
      "       137 154300859383163.3438           19.61m\n",
      "       138 153716281588494.1875           19.53m\n",
      "       139 153059447695966.1250           19.44m\n",
      "       140 152426017998758.6562           19.39m\n",
      "       141 151831182999430.4688           19.31m\n",
      "       142 151283206267403.7188           19.26m\n",
      "       143 150691718018242.3750           19.18m\n",
      "       144 150201824753536.6562           19.14m\n",
      "       145 149618907854889.2812           19.08m\n",
      "       146 149133964968770.7500           19.00m\n",
      "       147 148619181936940.8125           18.91m\n",
      "       148 148173200580667.0938           18.82m\n",
      "       149 147607526239234.4688           18.77m\n",
      "       150 147085563087462.7500           18.68m\n",
      "       151 146534971969234.5938           18.60m\n",
      "       152 146012477068427.1875           18.54m\n",
      "       153 145472577135898.7188           18.45m\n",
      "       154 144961421850710.7188           18.38m\n",
      "       155 144462968236777.0000           18.33m\n",
      "       156 143989059366993.3438           18.26m\n",
      "       157 143498618163229.1250           18.20m\n",
      "       158 143120190507926.7500           18.14m\n",
      "       159 142657117636836.9688           18.05m\n",
      "       160 142200212339742.2188           17.97m\n",
      "       161 141763593319908.5938           17.91m\n",
      "       162 141271167074324.4375           17.84m\n",
      "       163 140757001685868.6250           17.78m\n",
      "       164 140317134984856.7656           17.70m\n",
      "       165 139833034041971.1406           17.63m\n",
      "       166 139401532972606.4531           17.58m\n",
      "       167 139036952332059.3594           17.53m\n",
      "       168 138655098811840.8281           17.46m\n",
      "       169 138170445205698.7500           17.39m\n",
      "       170 137755832797879.8125           17.31m\n",
      "       171 137320364589998.2656           17.22m\n",
      "       172 136956217132268.6875           17.15m\n",
      "       173 136558825604936.4844           17.07m\n",
      "       174 136172208929838.2969           17.00m\n",
      "       175 135694442902409.3281           16.92m\n",
      "       176 135244334582170.2656           16.85m\n",
      "       177 134935931325352.0000           16.77m\n",
      "       178 134610307113939.3438           16.70m\n",
      "       179 134211419009298.3438           16.62m\n",
      "       180 133838029326832.4844           16.55m\n",
      "       181 133428927472524.2344           16.49m\n",
      "       182 133059723696213.2188           16.42m\n",
      "       183 132688527531487.6250           16.35m\n",
      "       184 132411064746491.0000           16.27m\n",
      "       185 132059897846220.4375           16.21m\n",
      "       186 131685407581801.2969           16.15m\n",
      "       187 131323537893876.5312           16.09m\n",
      "       188 130897571020282.6875           16.03m\n",
      "       189 130502598154305.9531           15.96m\n",
      "       190 130084257004991.2812           15.89m\n",
      "       191 129764836583982.5156           15.82m\n",
      "       192 129402607629335.0000           15.79m\n",
      "       193 129027874437148.5156           15.75m\n",
      "       194 128673187704780.1719           15.72m\n",
      "       195 128340568756135.6250           15.69m\n",
      "       196 127944614811002.0469           15.67m\n",
      "       197 127691451969698.6562           15.64m\n",
      "       198 127405497341980.2031           15.62m\n",
      "       199 127075147078026.6562           15.60m\n",
      "       200 126826318696082.8438           15.56m\n",
      "       201 126502770706313.9375           15.52m\n",
      "       202 126156142493080.8281           15.49m\n",
      "       203 125886893719264.0781           15.45m\n",
      "       204 125578232238313.9062           15.40m\n",
      "       205 125252226875636.4844           15.36m\n",
      "       206 124909825580544.8906           15.32m\n",
      "       207 124642617474434.2500           15.28m\n",
      "       208 124327715319601.0156           15.24m\n",
      "       209 123987372929871.5781           15.20m\n",
      "       210 123726247098607.9844           15.21m\n",
      "       211 123458200661472.1719           15.18m\n",
      "       212 123180565034348.1250           15.14m\n",
      "       213 123017827748230.0938           15.08m\n",
      "       214 122716869356779.5625           15.01m\n",
      "       215 122403863694845.8906           14.94m\n",
      "       216 122150673735180.9219           14.89m\n",
      "       217 121866815734938.4531           14.82m\n",
      "       218 121688034680975.2031           14.77m\n",
      "       219 121424442276439.2188           14.74m\n",
      "       220 121173741747375.2188           14.69m\n",
      "       221 120914107362037.7969           14.64m\n",
      "       222 120607159512744.8125           14.59m\n",
      "       223 120307595169785.7031           14.55m\n",
      "       224 120017181288828.1406           14.51m\n",
      "       225 119720510308012.5938           14.47m\n",
      "       226 119539764131555.5156           14.41m\n",
      "       227 119377036334132.0000           14.35m\n",
      "       228 119109871033868.8750           14.30m\n",
      "       229 118869842282754.8438           14.25m\n",
      "       230 118576153917084.6562           14.21m\n",
      "       231 118270988787827.0625           14.17m\n",
      "       232 117934256489530.7031           14.14m\n",
      "       233 117662094178473.3125           14.10m\n",
      "       234 117369822690150.0625           14.05m\n",
      "       235 117121750295037.3438           14.01m\n",
      "       236 116904414589254.5156           13.97m\n",
      "       237 116614130210635.6094           13.94m\n",
      "       238 116320989026720.4375           13.89m\n",
      "       239 116039833661414.7500           13.84m\n",
      "       240 115814489875243.2031           13.79m\n",
      "       241 115529478480104.8438           13.74m\n",
      "       242 115257302885397.7812           13.72m\n",
      "       243 115025342192186.6719           13.66m\n",
      "       244 114757634869206.1562           13.64m\n",
      "       245 114509609160900.1875           13.61m\n",
      "       246 114317187005006.8750           13.62m\n",
      "       247 114126847679036.9844           13.58m\n",
      "       248 113965920965047.5781           13.54m\n",
      "       249 113633469330648.9375           13.51m\n",
      "       250 113394723691617.7344           13.46m\n",
      "       251 113225738876418.7656           13.43m\n",
      "       252 113039820456794.0938           13.41m\n",
      "       253 112834981916469.7656           13.38m\n",
      "       254 112601782613514.1562           13.34m\n",
      "       255 112435734691910.0469           13.29m\n",
      "       256 112199457526092.4062           13.25m\n",
      "       257 111989302127318.6406           13.21m\n",
      "       258 111851531022800.5625           13.17m\n",
      "       259 111586981337065.9219           13.12m\n",
      "       260 111453765404543.5000           13.09m\n",
      "       261 111241659452946.6719           13.05m\n",
      "       262 111038077802941.5938           13.02m\n",
      "       263 110945314942397.5312           12.98m\n",
      "       264 110726714263303.8438           12.94m\n",
      "       265 110482595204345.8594           12.91m\n",
      "       266 110224458301581.0000           12.87m\n",
      "       267 109981898757503.5000           12.84m\n",
      "       268 109727775475677.3750           12.82m\n",
      "       269 109582189268895.4844           12.79m\n",
      "       270 109283885779690.4219           12.74m\n",
      "       271 109065875557695.2031           12.70m\n",
      "       272 108889244382042.0312           12.65m\n",
      "       273 108707807350655.0781           12.61m\n",
      "       274 108513498440496.0938           12.57m\n",
      "       275 108394619692054.1406           12.53m\n",
      "       276 108209114370307.2656           12.48m\n",
      "       277 108002037215039.7500           12.45m\n",
      "       278 107758316920968.0312           12.43m\n",
      "       279 107608668924531.7656           12.39m\n",
      "       280 107392575651918.2344           12.36m\n",
      "       281 107181869945936.8438           12.30m\n",
      "       282 106988919195476.2812           12.26m\n",
      "       283 106836032340577.3438           12.21m\n",
      "       284 106656689207910.8438           12.16m\n",
      "       285 106522265607452.3125           12.13m\n",
      "       286 106282558549018.5781           12.10m\n",
      "       287 106099131865764.2812           12.05m\n",
      "       288 105853140501665.9375           12.00m\n",
      "       289 105783952941438.5938           11.95m\n",
      "       290 105631458061949.9062           11.89m\n",
      "       291 105359764867855.7031           11.84m\n",
      "       292 105177708340608.6719           11.79m\n",
      "       293 105066506987194.4375           11.75m\n",
      "       294 104875953336395.1719           11.71m\n",
      "       295 104684635841503.3906           11.69m\n",
      "       296 104465884491580.5312           11.65m\n",
      "       297 104233225523367.6406           11.61m\n",
      "       298 104079024870430.7188           11.58m\n",
      "       299 103934232987319.9375           11.56m\n",
      "       300 103705448210991.6562           11.54m\n",
      "       301 103606282418673.7344           11.51m\n",
      "       302 103383473166457.4219           11.47m\n",
      "       303 103274830757526.9062           11.44m\n",
      "       304 103131633855068.1875           11.39m\n",
      "       305 102986193414563.6094           11.37m\n",
      "       306 102900623730980.5938           11.35m\n",
      "       307 102760844656773.3281           11.31m\n",
      "       308 102588596587653.2656           11.27m\n",
      "       309 102362063391882.1875           11.23m\n",
      "       310 102238113999522.2500           11.19m\n",
      "       311 102075968330841.5781           11.16m\n",
      "       312 101891816030405.5312           11.14m\n",
      "       313 101781318543916.2500           11.11m\n",
      "       314 101640717660029.9531           11.07m\n",
      "       315 101430786437254.4688           11.04m\n",
      "       316 101293669184858.2812           11.01m\n",
      "       317 101186734625249.2188           10.96m\n",
      "       318 100983858455764.6094           10.93m\n",
      "       319 100750479974092.7969           10.89m\n",
      "       320 100579426336839.4844           10.86m\n",
      "       321 100444868561972.7344           10.83m\n",
      "       322 100229916155973.1094           10.79m\n",
      "       323 100062051615916.0156           10.75m\n",
      "       324 99916516403859.9219           10.71m\n",
      "       325 99770374389252.6562           10.70m\n",
      "       326 99586574393722.1094           10.68m\n",
      "       327 99368728416471.0312           10.65m\n",
      "       328 99271671992678.8438           10.62m\n",
      "       329 99116635558038.8594           10.58m\n"
     ]
    }
   ],
   "source": [
    "create_submission(model, trainX, trainY, testX, testY, X, Y, test_data, \"gb1.csv\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb68e8ab",
   "metadata": {},
   "source": [
    "# Multi-model running\n",
    "in one file we test different models and create multiple files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee8b316",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:29:18.843512Z",
     "iopub.status.busy": "2024-11-30T22:29:18.842802Z",
     "iopub.status.idle": "2024-11-30T22:29:18.862960Z",
     "shell.execute_reply": "2024-11-30T22:29:18.861399Z",
     "shell.execute_reply.started": "2024-11-30T22:29:18.843422Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# case 1\n",
    "model = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    subsample=1.0, \n",
    "    verbose=3\n",
    ")\n",
    "# create_submission(model, trainX, trainY, testX, testY, X, Y, test_data, \"gboost1.csv\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1702bc8-49ad-498d-b593-d54f811d9992",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:29:18.865756Z",
     "iopub.status.busy": "2024-11-30T22:29:18.865171Z",
     "iopub.status.idle": "2024-11-30T22:29:18.888451Z",
     "shell.execute_reply": "2024-11-30T22:29:18.887081Z",
     "shell.execute_reply.started": "2024-11-30T22:29:18.865696Z"
    },
    "papermill": {
     "duration": 0.053693,
     "end_time": "2024-11-07T21:46:14.588137",
     "exception": false,
     "start_time": "2024-11-07T21:46:14.534444",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# case 2\n",
    "model2 = GradientBoostingRegressor(\n",
    "    n_estimators=50,\n",
    "    max_depth=2,\n",
    "    learning_rate=0.2,\n",
    "    subsample=0.8, \n",
    "    verbose=3\n",
    ")\n",
    "# create_submission(model2, trainX, trainY, testX, testY, X, Y, test_data, \"gboost2.csv\")\n",
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed56fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:29:18.891622Z",
     "iopub.status.busy": "2024-11-30T22:29:18.891150Z",
     "iopub.status.idle": "2024-11-30T22:29:18.912461Z",
     "shell.execute_reply": "2024-11-30T22:29:18.910856Z",
     "shell.execute_reply.started": "2024-11-30T22:29:18.891579Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# case 3\n",
    "model3 = GradientBoostingRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9, \n",
    "    verbose=3\n",
    ")\n",
    "# create_submission(model3, trainX, trainY, testX, testY, X, Y, test_data, \"gboost3.csv\")\n",
    "model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be937f5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:29:18.914915Z",
     "iopub.status.busy": "2024-11-30T22:29:18.914341Z",
     "iopub.status.idle": "2024-11-30T22:29:18.939302Z",
     "shell.execute_reply": "2024-11-30T22:29:18.938049Z",
     "shell.execute_reply.started": "2024-11-30T22:29:18.914858Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# case 4\n",
    "model4 = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.85, \n",
    "    verbose=3\n",
    ")\n",
    "# create_submission(model4, trainX, trainY, testX, testY, X, Y, test_data, \"gboost4.csv\")\n",
    "model4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766db70a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:29:18.941493Z",
     "iopub.status.busy": "2024-11-30T22:29:18.941064Z",
     "iopub.status.idle": "2024-11-30T22:29:18.962814Z",
     "shell.execute_reply": "2024-11-30T22:29:18.961187Z",
     "shell.execute_reply.started": "2024-11-30T22:29:18.941454Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# case 5\n",
    "model5 = GradientBoostingRegressor(\n",
    "    n_estimators=150,\n",
    "    max_depth=2,\n",
    "    learning_rate=0.15,\n",
    "    subsample=0.7, \n",
    "    verbose=3\n",
    ")\n",
    "# create_submission(model5, trainX, trainY, testX, testY, X, Y, test_data, \"gboost5.csv\")\n",
    "model5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19831eab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:29:18.966872Z",
     "iopub.status.busy": "2024-11-30T22:29:18.966228Z",
     "iopub.status.idle": "2024-11-30T22:29:18.983055Z",
     "shell.execute_reply": "2024-11-30T22:29:18.981497Z",
     "shell.execute_reply.started": "2024-11-30T22:29:18.966813Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# case 6\n",
    "model6 = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    max_features=0.3, \n",
    "    verbose=3\n",
    ")\n",
    "# create_submission(model6, trainX, trainY, testX, testY, X, Y, test_data, \"gboost6.csv\")\n",
    "model6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aa9b33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:29:18.985601Z",
     "iopub.status.busy": "2024-11-30T22:29:18.985036Z",
     "iopub.status.idle": "2024-11-30T22:29:19.004493Z",
     "shell.execute_reply": "2024-11-30T22:29:19.002828Z",
     "shell.execute_reply.started": "2024-11-30T22:29:18.985544Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# case 7\n",
    "model7 = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.9,\n",
    "    min_samples_split=10, \n",
    "    verbose=3\n",
    ")\n",
    "# create_submission(model7, trainX, trainY, testX, testY, X, Y, test_data, \"gboost7.csv\")\n",
    "model7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aa66ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:29:19.006876Z",
     "iopub.status.busy": "2024-11-30T22:29:19.006291Z",
     "iopub.status.idle": "2024-11-30T22:29:19.022221Z",
     "shell.execute_reply": "2024-11-30T22:29:19.020928Z",
     "shell.execute_reply.started": "2024-11-30T22:29:19.006830Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# case 8\n",
    "model8 = GradientBoostingRegressor(\n",
    "    n_estimators=50,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.3,\n",
    "    subsample=0.75, \n",
    "    verbose=3\n",
    ")\n",
    "# create_submission(model8, trainX, trainY, testX, testY, X, Y, test_data, \"gboost8.csv\")\n",
    "model8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0229f77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:29:19.024374Z",
     "iopub.status.busy": "2024-11-30T22:29:19.023913Z",
     "iopub.status.idle": "2024-11-30T22:35:21.436948Z",
     "shell.execute_reply": "2024-11-30T22:35:21.435753Z",
     "shell.execute_reply.started": "2024-11-30T22:29:19.024326Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# case 9\n",
    "model9 = GradientBoostingRegressor(\n",
    "    n_estimators=120,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.08,\n",
    "    subsample=0.85,\n",
    "    max_features='sqrt', \n",
    "    verbose=3\n",
    ")\n",
    "# create_submission(model9, trainX, trainY, testX, testY, X, Y, test_data, \"gboost9.csv\")\n",
    "model9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb4571b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:35:21.438613Z",
     "iopub.status.busy": "2024-11-30T22:35:21.438215Z",
     "iopub.status.idle": "2024-11-30T22:35:21.447065Z",
     "shell.execute_reply": "2024-11-30T22:35:21.446002Z",
     "shell.execute_reply.started": "2024-11-30T22:35:21.438581Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# case 10\n",
    "model0 = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.6, \n",
    "    verbose=3\n",
    ")\n",
    "# create_submission(model0, trainX, trainY, testX, testY, X, Y, test_data, \"gboost10.csv\")\n",
    "model0"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6034234,
     "sourceId": 9837068,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6034286,
     "sourceId": 9837128,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6164648,
     "sourceId": 10013045,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6179183,
     "sourceId": 10032577,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5193.546342,
   "end_time": "2024-11-07T21:46:17.332142",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-07T20:19:43.785800",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
