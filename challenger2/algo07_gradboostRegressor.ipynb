{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd67a471-fbf2-4cee-9b17-65cc82d7886f",
   "metadata": {
    "papermill": {
     "duration": 0.013014,
     "end_time": "2024-11-07T20:19:46.538794",
     "exception": false,
     "start_time": "2024-11-07T20:19:46.525780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importing Libraries\n",
    "in this part we will install all the necessary libraries on command prompt and then import the necessary functions from those libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd54b756",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:16:32.093336Z",
     "iopub.status.busy": "2024-11-30T22:16:32.092822Z",
     "iopub.status.idle": "2024-11-30T22:16:33.443912Z",
     "shell.execute_reply": "2024-11-30T22:16:33.443064Z",
     "shell.execute_reply.started": "2024-11-30T22:16:32.093291Z"
    },
    "papermill": {
     "duration": 5.853121,
     "end_time": "2024-11-07T20:19:52.405091",
     "exception": false,
     "start_time": "2024-11-07T20:19:46.551970",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -------------------- Data Handling --------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# -------------------- Preprocessing --------------------\n",
    "from sklearn.impute import SimpleImputer, KNNImputer  # Handle missing values\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler, Normalizer  # Feature scaling\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, SelectKBest, f_classif, VarianceThreshold, f_regression  # Feature selection\n",
    "from sklearn.decomposition import PCA  # Dimensionality reduction\n",
    "from sklearn.compose import ColumnTransformer  # Preprocessing for different feature types\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder  # Feature transformation\n",
    "from sklearn.pipeline import Pipeline  # Create machine learning pipelines\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "from category_encoders import TargetEncoder, BinaryEncoder\n",
    "\n",
    "# -------------------- Model --------------------\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# -------------------- Model Evaluation --------------------\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, cross_val_score, GridSearchCV, ParameterGrid  # Train-test split, cross-validation, grid search\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, make_scorer  # Regression metrics\n",
    "\n",
    "# -------------------- Warning Handling --------------------\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c8984b",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1ca4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_datetime():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92c27a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Filter\n",
    "def correlationFilter(X, test_data_processed, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Removes highly correlated features from the dataset based on a given threshold.\n",
    "\n",
    "    Parameters:\n",
    "        X: Full dataset as a DataFrame.\n",
    "        test_data_processed: Processed test data as a DataFrame.\n",
    "        threshold: Correlation threshold above which features are considered highly correlated.\n",
    "\n",
    "    Returns:\n",
    "        Filtered datasets (X and test_data_processed) with reduced multicollinearity.\n",
    "    \"\"\"\n",
    "    print(\"Calculating correlation matrix...\")\n",
    "\n",
    "    # Compute the correlation matrix\n",
    "    corr_matrix = X.corr().abs()\n",
    "\n",
    "    # Extract the upper triangle of the correlation matrix\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    # Identify columns to drop based on the threshold\n",
    "    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > threshold)]\n",
    "\n",
    "    print(f\"Features to drop due to high correlation (> {threshold}):\", to_drop)\n",
    "\n",
    "    # Drop the highly correlated features\n",
    "    X = X.drop(columns=to_drop)\n",
    "    test_data_processed = test_data_processed.drop(columns=to_drop)\n",
    "\n",
    "    print(\"Highly correlated features removed.\")\n",
    "    print(\"New dataset shape:\", X.shape)\n",
    "\n",
    "    return X, test_data_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9601648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance Filter\n",
    "def varianceFilter(X, test_data_processed, threshold=0.001):\n",
    "    \"\"\"\n",
    "    Removes features with low variance from the dataset based on a given threshold.\n",
    "\n",
    "    Parameters:\n",
    "        X: Full dataset as a DataFrame.\n",
    "        test_data_processed: Processed test data as a DataFrame.\n",
    "        threshold: Variance threshold below which features are considered low variance.\n",
    "\n",
    "    Returns:\n",
    "        Filtered datasets (X and test_data_processed) with low variance features removed.\n",
    "    \"\"\"\n",
    "    print(\"Calculating feature variances...\")\n",
    "\n",
    "    # Compute the variance of each feature\n",
    "    feature_variances = X.var(axis=0)\n",
    "\n",
    "    # Display variance statistics\n",
    "    print(feature_variances.describe())\n",
    "\n",
    "    # Identify features with low variance\n",
    "    low_variance_columns = feature_variances[feature_variances < threshold].index.tolist()\n",
    "\n",
    "    print(f\"Features with variance below {threshold}: {low_variance_columns}\")\n",
    "\n",
    "    # Remove the low variance features\n",
    "    X = X.drop(columns=low_variance_columns)\n",
    "    test_data_processed = test_data_processed.drop(columns=low_variance_columns)\n",
    "\n",
    "    print(\"Low variance features removed.\")\n",
    "    print(\"New dataset shape:\", X.shape)\n",
    "\n",
    "    return X, test_data_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402de817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(train_data, test_data, target_column='price_doc', variance_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Apply PCA to reduce dimensionality of the dataset while preserving the specified variance threshold.\n",
    "\n",
    "    Parameters:\n",
    "        train_data (DataFrame): The training data, including features and target.\n",
    "        test_data (DataFrame): The test data, including features.\n",
    "        target_column (str): The name of the target column in the data (default is 'price_doc').\n",
    "        variance_threshold (float): The threshold for the cumulative variance to retain (default is 0.95).\n",
    "\n",
    "    Returns:\n",
    "        train_data (DataFrame): The training data with PCA-reduced features and target.\n",
    "        test_data (DataFrame): The test data with PCA-reduced features.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Starting PCA process...\")\n",
    "\n",
    "    # Separate features and target variable\n",
    "    print(\"Separating features and target variable...\")\n",
    "    train_features = train_data.drop(columns=[target_column])\n",
    "    train_target = train_data[target_column]\n",
    "\n",
    "    test_features = test_data.drop(columns=[target_column])\n",
    "\n",
    "    print(f\"Train features shape: {train_features.shape}\")\n",
    "    print(f\"Test features shape: {test_features.shape}\")\n",
    "\n",
    "    # Perform PCA to determine the optimal number of components\n",
    "    print(\"Fitting PCA to training data...\")\n",
    "    pca = PCA()\n",
    "    pca.fit(train_features)  # Fit PCA on training data\n",
    "\n",
    "    # Plot explained variance ratio to decide on optimal components\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('PCA: Explained Variance vs Number of Components')\n",
    "    plt.show()\n",
    "\n",
    "    # Select the number of components that explain the desired variance\n",
    "    print(f\"Applying PCA with {variance_threshold*100}% explained variance...\")\n",
    "    pca = PCA(n_components=variance_threshold)\n",
    "    train_features_pca = pca.fit_transform(train_features)\n",
    "    test_features_pca = pca.transform(test_features)\n",
    "\n",
    "    print(f\"Train features shape after PCA: {train_features_pca.shape}\")\n",
    "    print(f\"Test features shape after PCA: {test_features_pca.shape}\")\n",
    "\n",
    "    # Reconstruct the train_data and test_data with PCA-reduced features and target\n",
    "    print(\"Reconstructing train and test datasets with PCA-transformed features...\")\n",
    "    train_data_pca = pd.DataFrame(train_features_pca)\n",
    "    train_data_pca[target_column] = train_target.reset_index(drop=True)\n",
    "\n",
    "    test_data_pca = pd.DataFrame(test_features_pca)\n",
    "    test_data_pca[target_column] = test_data[target_column].reset_index(drop=True)\n",
    "\n",
    "    print(f\"Train data shape after PCA: {train_data_pca.shape}\")\n",
    "    print(f\"Test data shape after PCA: {test_data_pca.shape}\")\n",
    "\n",
    "    return train_data_pca, test_data_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba08960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward and Backward Selection\n",
    "def fbselection(direction, sample_model, features, X, trainX, trainY, testX, test_data_processed):\n",
    "    \"\"\"\n",
    "    Performs forward or backward feature selection.\n",
    "\n",
    "    Parameters:\n",
    "        direction: 'forward' or 'backward' for the selection method.\n",
    "        sample_model: The model to use for feature selection.\n",
    "        features: Number of features to select.\n",
    "        X: Full dataset.\n",
    "        trainX: Training feature dataset.\n",
    "        trainY: Training target dataset.\n",
    "        testX: Testing feature dataset.\n",
    "        test_data_processed: Processed test data.\n",
    "\n",
    "    Returns:\n",
    "        Updated model and adjusted datasets.\n",
    "    \"\"\"\n",
    "    print(\"Starting forward/backward selection...\")\n",
    "\n",
    "    # Define Sequential Feature Selector\n",
    "    selection = SequentialFeatureSelector(\n",
    "        sample_model,\n",
    "        direction=direction,\n",
    "        n_features_to_select=features,\n",
    "        scoring='roc_auc'\n",
    "    )\n",
    "\n",
    "    return modelSelector(sample_model, selection, X, trainX, trainY, testX, test_data_processed)\n",
    "\n",
    "\n",
    "# Model Selector for applying the transformation\n",
    "def modelSelector(sample_model, selection, X, trainX, trainY, testX, test_data_processed):\n",
    "    \"\"\"\n",
    "    Applies the given selection method to transform datasets.\n",
    "\n",
    "    Parameters:\n",
    "        sample_model: The model to use for feature selection.\n",
    "        selection: Feature selection object (e.g., SequentialFeatureSelector, SelectKBest).\n",
    "        X: Full dataset.\n",
    "        trainX: Training feature dataset.\n",
    "        trainY: Training target dataset.\n",
    "        testX: Testing feature dataset.\n",
    "        test_data_processed: Processed test data.\n",
    "\n",
    "    Returns:\n",
    "        Updated model and adjusted datasets.\n",
    "    \"\"\"\n",
    "    print(\"Extracting features using the selection method...\")\n",
    "\n",
    "    # Fit and transform training data\n",
    "    trainX = selection.fit_transform(trainX, trainY)\n",
    "\n",
    "    print(\"Features extracted, transforming other datasets...\")\n",
    "\n",
    "    # Transform other datasets using the fitted selection object\n",
    "    testX = selection.transform(testX)\n",
    "    test_data_processed = selection.transform(test_data_processed)\n",
    "    X = selection.transform(X)\n",
    "\n",
    "    print(\"All datasets transformed.\")\n",
    "    print(\"X shape -> \", X.shape)\n",
    "    print(\"trainX shape -> \", trainX.shape)\n",
    "    print(\"testX shape -> \", testX.shape)\n",
    "    print(\"test_data_processed shape -> \", test_data.shape)\n",
    "\n",
    "    return sample_model, X, trainX, trainY, testX, test_data_processed\n",
    "\n",
    "\n",
    "# K-Best Selection\n",
    "def kbest(sample_model, features, X, trainX, trainY, testX, test_data_processed):\n",
    "    \"\"\"\n",
    "    Selects the top K features based on statistical tests.\n",
    "\n",
    "    Parameters:\n",
    "        sample_model: The model to use for feature selection.\n",
    "        features: Number of top features to select.\n",
    "        X: Full dataset.\n",
    "        trainX: Training feature dataset.\n",
    "        trainY: Training target dataset.\n",
    "        testX: Testing feature dataset.\n",
    "        test_data_processed: Processed test data.\n",
    "\n",
    "    Returns:\n",
    "        Updated model and adjusted datasets.\n",
    "    \"\"\"\n",
    "    print(\"Starting K-Best feature selection...\")\n",
    "\n",
    "    # Define SelectKBest object\n",
    "    selection = SelectKBest(score_func=f_regression, k=features)\n",
    "\n",
    "    return modelSelector(sample_model, selection, X, trainX, trainY, testX, test_data_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f3b1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Function\n",
    "def featureImportance(sample_model, features, X, trainX, trainY, testX, test_data_processed):\n",
    "    \"\"\"\n",
    "    Adjusts the dataset and model based on feature importance.\n",
    "    \n",
    "    Parameters:\n",
    "        sample_model: The model used for feature importance extraction.\n",
    "        features: Number of top features to select.\n",
    "        X: The full dataset.\n",
    "        trainX: Training feature dataset.\n",
    "        trainY: Training target dataset.\n",
    "        testX: Testing feature dataset.\n",
    "        test_data_processed: Processed test data.\n",
    "\n",
    "    Returns:\n",
    "        Updated model and adjusted datasets.\n",
    "    \"\"\"\n",
    "    print(\"Fitting the model...\")\n",
    "\n",
    "    # Fit the model\n",
    "    sample_model.fit(trainX, trainY)\n",
    "\n",
    "    print(\"Extracting feature importances...\")\n",
    "\n",
    "    # Extract feature importances\n",
    "    importances = sample_model.feature_importances_\n",
    "\n",
    "    # Extract feature names\n",
    "    feature_names = trainX.columns\n",
    "\n",
    "    print(\"Feature names:\", feature_names)\n",
    "\n",
    "    # Create a DataFrame for feature importance\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Extract top features based on importance\n",
    "    top_features = feature_importance_df['Feature'].head(features).values\n",
    "\n",
    "    print(\"Top features:\", top_features)\n",
    "\n",
    "    # Filter datasets to include only top features\n",
    "    trainX = trainX[top_features]\n",
    "    testX = testX[top_features]\n",
    "    X = X[top_features]\n",
    "    test_data_processed = test_data_processed[top_features]\n",
    "\n",
    "    print(\"Top features extracted and datasets updated.\")\n",
    "\n",
    "    # Retrain the model with top features\n",
    "    print(\"Retraining the model with selected features...\")\n",
    "    sample_model.fit(trainX, trainY)\n",
    "\n",
    "    print(\"Model retrained with top features.\")\n",
    "\n",
    "    return sample_model, X, trainX, trainY, testX, test_data_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdeb617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch(param_grid, model, trainX, trainY):\n",
    "    \"\"\"\n",
    "    Performs a grid search to optimize hyperparameters for a given model.\n",
    "\n",
    "    Parameters:\n",
    "        param_grid: Dictionary containing parameter grid for optimization.\n",
    "        model: The machine learning model to optimize.\n",
    "        trainX: Training feature dataset.\n",
    "        trainY: Training target dataset.\n",
    "\n",
    "    Returns:\n",
    "        Optimized model with the best parameters found during grid search.\n",
    "    \"\"\"\n",
    "    print(\"Starting grid search...\")\n",
    "\n",
    "    # Intialize scorer\n",
    "    scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=3,\n",
    "        scoring=scorer,\n",
    "        verbose=3,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    print(\"Grid search initialized.\")\n",
    "\n",
    "    # Fit the grid search on the training data\n",
    "    grid_search.fit(trainX, trainY)\n",
    "\n",
    "    print(\"Grid search fitting completed.\")\n",
    "\n",
    "    # Retrieve the best model found during grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Best model found:\", best_model)\n",
    "\n",
    "    # Retrieve and display the best parameters\n",
    "    best_parameters = grid_search.best_params_\n",
    "    print(\"Best parameters:\", best_parameters)\n",
    "\n",
    "    # Retrieve and display the best score\n",
    "    print(\"Best cross-validated score:\", grid_search.best_score_)\n",
    "\n",
    "    # Assign the best model\n",
    "    model = best_model\n",
    "    print(\"Model assigned. Grid search completed.\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590b2444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_pred, testY):\n",
    "    \"\"\"\n",
    "    Computes and displays various regression metrics for model evaluation.\n",
    "\n",
    "    Parameters:\n",
    "        y_pred: Predicted values.\n",
    "        testY: Actual target values.\n",
    "\n",
    "    Returns:\n",
    "        Root Mean Squared Error (RMSE) of the predictions.\n",
    "    \"\"\"\n",
    "    print(\"Starting to compute metrics...\")\n",
    "\n",
    "    # Mean Squared Error (MSE)\n",
    "    mse = mean_squared_error(testY, y_pred)\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "\n",
    "    # Root Mean Squared Error (RMSE)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "\n",
    "    # Mean Absolute Error (MAE)\n",
    "    mae = mean_absolute_error(testY, y_pred)\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "\n",
    "    # Coefficient of Determination (R² Score)\n",
    "    r2_Score = r2_score(testY, y_pred)\n",
    "    print(f\"Coefficient of Determination (R² Score): {r2_Score:.2f}\")\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d81a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, trainX, trainY, testX, testY):\n",
    "    \"\"\"\n",
    "    Trains the model, evaluates it on the test set, and computes evaluation metrics.\n",
    "\n",
    "    Parameters:\n",
    "        model: The machine learning model to train and evaluate.\n",
    "        trainX: Training feature dataset.\n",
    "        trainY: Training target dataset.\n",
    "        testX: Testing feature dataset.\n",
    "        testY: Testing target dataset.\n",
    "\n",
    "    Returns:\n",
    "        Trained model and the Root Mean Squared Error (RMSE) on the test set.\n",
    "    \"\"\"\n",
    "    print(\"Training model\", get_current_datetime())\n",
    "    model.fit(trainX, trainY)\n",
    "\n",
    "    print(\"Computing score\", get_current_datetime())\n",
    "    print(\"Model score (training set):\", model.score(trainX, trainY))\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(testX)\n",
    "\n",
    "    # Compute metrics\n",
    "    rmse = metrics(y_pred, testY)\n",
    "\n",
    "    return model, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605e7cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFile(model, X, Y, test_data, file_name):\n",
    "    \"\"\"\n",
    "    Fits the model on the provided dataset, predicts on test data, and saves the predictions to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        model: The machine learning model to use for training and prediction.\n",
    "        X: Full feature dataset.\n",
    "        Y: Target dataset.\n",
    "        test_data: Test dataset (should include all features except the target column).\n",
    "        file_name: Name of the output CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(\"Fitting model on X and Y\", get_current_datetime())\n",
    "    model.fit(X, Y)\n",
    "\n",
    "    print(\"Scoring model on X and Y\", get_current_datetime())\n",
    "    score = model.score(X, Y)\n",
    "    print(\"Model training score:\", score)\n",
    "\n",
    "    print(\"Predicting on test data\", get_current_datetime())\n",
    "    test_prediction = model.predict(test_data)#.drop(columns=['price_doc']))\n",
    "    print(\"Predictions:\", test_prediction)\n",
    "\n",
    "    print(\"Preparing sample submission file\", get_current_datetime())\n",
    "    # sample_data = pd.read_csv(r\"/kaggle/input/challenge2/sample_submission.csv\")\n",
    "    sample_data = pd.read_csv(r\"D:\\Users\\DELL\\OneDrive - Institute of Business Administration\\IBA\\sem5\\machine learning\\ipynb notebooks\\challenger2\\iml-fall-2024-challenge-2\\sample_submission.csv\")\n",
    "    sample_data['price_doc'] = test_prediction\n",
    "\n",
    "    print(\"Saving submission file\", get_current_datetime())\n",
    "    # base_path = r\"/kaggle/working/\"\n",
    "    base_path = r\"D:\\Users\\DELL\\OneDrive - Institute of Business Administration\\IBA\\sem5\\machine learning\\ipynb notebooks\\challenger2\\iml-fall-2024-challenge-2\\\\\"\n",
    "    full_path = base_path + file_name\n",
    "\n",
    "    sample_data.to_csv(full_path, index=False)\n",
    "    print(f\"File saved at: {full_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091f4d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(model, trainX, trainY, testX, testY, X, Y, test_data, file_name):\n",
    "    model, rmse = run_model(model, trainX, trainY, testX, testY)\n",
    "    createFile(model, X, Y, test_data, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3912ca3f",
   "metadata": {
    "papermill": {
     "duration": 0.011237,
     "end_time": "2024-11-07T20:19:52.458899",
     "exception": false,
     "start_time": "2024-11-07T20:19:52.447662",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Loading\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data shall be loaded into variables as data sets using pandas and csv readers. they will be checked to see if they are loaded properly and will be loaded as 2 sets: train and test as per given in the kaggle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9125e82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:16:33.464089Z",
     "iopub.status.busy": "2024-11-30T22:16:33.463749Z",
     "iopub.status.idle": "2024-11-30T22:16:47.958190Z",
     "shell.execute_reply": "2024-11-30T22:16:47.957140Z",
     "shell.execute_reply.started": "2024-11-30T22:16:33.464053Z"
    },
    "papermill": {
     "duration": 4.591761,
     "end_time": "2024-11-07T20:19:57.062353",
     "exception": false,
     "start_time": "2024-11-07T20:19:52.470592",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# lets load the training data set\n",
    "# train_data = pd.read_csv(r\"/kaggle/input/challenge2/train.csv\")\n",
    "train_data = pd.read_csv(r\"D:\\Users\\DELL\\OneDrive - Institute of Business Administration\\IBA\\sem5\\machine learning\\ipynb notebooks\\challenger2\\iml-fall-2024-challenge-2\\train\\train.csv\")\n",
    "\n",
    "# lets also check it by getting the first few rows of the data, there should  be one target variable Y\n",
    "train_data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f55297",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:16:47.960533Z",
     "iopub.status.busy": "2024-11-30T22:16:47.960107Z",
     "iopub.status.idle": "2024-11-30T22:16:53.824969Z",
     "shell.execute_reply": "2024-11-30T22:16:53.823917Z",
     "shell.execute_reply.started": "2024-11-30T22:16:47.960488Z"
    },
    "papermill": {
     "duration": 1.948205,
     "end_time": "2024-11-07T20:19:59.023969",
     "exception": false,
     "start_time": "2024-11-07T20:19:57.075764",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# lets load the test data\n",
    "# test_data = pd.read_csv(r\"/kaggle/input/challenge2/test.csv\")\n",
    "test_data = pd.read_csv(r\"D:\\Users\\DELL\\OneDrive - Institute of Business Administration\\IBA\\sem5\\machine learning\\ipynb notebooks\\challenger2\\iml-fall-2024-challenge-2\\test\\test.csv\")\n",
    "\n",
    "# check if the data has been loaded by getting the first 5 rows - there should be no target variable Y as this is test data\n",
    "test_data.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db89cc8",
   "metadata": {
    "papermill": {
     "duration": 0.012664,
     "end_time": "2024-11-07T20:19:59.050712",
     "exception": false,
     "start_time": "2024-11-07T20:19:59.038048",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "before we start processing this data and using algorithms, we will fix this data first, this is called data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a83e6e",
   "metadata": {},
   "source": [
    "## split data into categorical and numerical\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "categorical will have one-hot and simple imputer of most frequent while numerical will have simple mean imputer and minmax scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306cc606",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:16:53.826867Z",
     "iopub.status.busy": "2024-11-30T22:16:53.826391Z",
     "iopub.status.idle": "2024-11-30T22:16:54.125483Z",
     "shell.execute_reply": "2024-11-30T22:16:54.124575Z",
     "shell.execute_reply.started": "2024-11-30T22:16:53.826819Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "categorical_cols = train_data.select_dtypes(include=[\"object\"]).columns\n",
    "numerical_cols = train_data.select_dtypes(exclude=[\"object\"]).drop(columns=['price_doc']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90228b33",
   "metadata": {},
   "source": [
    "## Pipelines and Column Transformers\n",
    "\n",
    "Pipelines in machine learning allow for chaining multiple preprocessing steps and modeling into a single object, ensuring that all transformations are applied consistently. ColumnTransformers enable column-specific transformations, allowing different preprocessing techniques for different features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c766ac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_transformer = Pipeline(steps=[\n",
    "#     (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "#     (\"scaler\", MinMaxScaler())\n",
    "# ])\n",
    "\n",
    "# cat_transformer = Pipeline(steps=[\n",
    "#     (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "#     (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b6dfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Column transformer for preprocessing\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         (\"num\", num_transformer, numerical_cols),\n",
    "#         (\"cat\", cat_transformer, categorical_cols)\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37955afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = preprocessor.fit_transform(train_data)\n",
    "# print(\"train completed\")\n",
    "# test_data = preprocessor.transform(test_data)\n",
    "# print(\"test data completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d3b9f4-49f7-4c84-931e-dff9c756dd75",
   "metadata": {},
   "source": [
    "## Imputers\n",
    "Imputers are used to handle missing data in a dataset by filling in missing values with estimated ones. Common strategies include using the mean, median, or most frequent value for numerical data, and the most frequent value for categorical data. Imputation helps ensure that models can be trained without the issue of missing values disrupting the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8f1a00-39b3-4207-9f2b-5ad588eded07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:16:54.126949Z",
     "iopub.status.busy": "2024-11-30T22:16:54.126615Z",
     "iopub.status.idle": "2024-11-30T22:16:55.857529Z",
     "shell.execute_reply": "2024-11-30T22:16:55.856664Z",
     "shell.execute_reply.started": "2024-11-30T22:16:54.126919Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------ Numerical Imputers ------------------\n",
    "\n",
    "# Mean Imputer (Fills missing values with the mean of each column)\n",
    "num_imputer = SimpleImputer(strategy=\"mean\")\n",
    "\n",
    "# Median Imputer (Fills missing values with the median of each column)\n",
    "# num_imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# Most Frequent Imputer (Fills missing values with the most frequent value of each column)\n",
    "# num_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "# Constant Imputer (Fills missing values with a constant value, e.g., 0)\n",
    "# num_imputer = SimpleImputer(strategy=\"constant\", fill_value=0)\n",
    "\n",
    "# KNN Imputer (Fills missing values based on nearest neighbors)\n",
    "# num_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# ------------------ Categorical Imputers ------------------\n",
    "\n",
    "# Most Frequent Imputer (Fills missing values with the most frequent value in each column)\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "# Constant Imputer (Fills missing values with a constant value, e.g., 'Unknown')\n",
    "# cat_imputer = SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")\n",
    "\n",
    "# ------------------ Apply Imputers ------------------\n",
    "\n",
    "# Impute numerical columns\n",
    "train_data[numerical_cols] = num_imputer.fit_transform(train_data[numerical_cols])\n",
    "test_data[numerical_cols] = num_imputer.transform(test_data[numerical_cols])\n",
    "\n",
    "# Impute categorical columns\n",
    "train_data[categorical_cols] = cat_imputer.fit_transform(train_data[categorical_cols])\n",
    "test_data[categorical_cols] = cat_imputer.transform(test_data[categorical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1be982f-df0e-481d-a8ce-9c1cee449a56",
   "metadata": {},
   "source": [
    "## Scalers\n",
    "Scalers are used to normalize or standardize numerical features in a dataset to ensure they are on a similar scale. This is crucial for algorithms that are sensitive to the magnitude of features, such as KNN or gradient-based models. Common scalers include MinMaxScaler, StandardScaler, and RobustScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce83d8bf-4f13-4063-be48-43b1e3fce721",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:16:55.858863Z",
     "iopub.status.busy": "2024-11-30T22:16:55.858545Z",
     "iopub.status.idle": "2024-11-30T22:16:57.439167Z",
     "shell.execute_reply": "2024-11-30T22:16:57.438069Z",
     "shell.execute_reply.started": "2024-11-30T22:16:55.858833Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------ Scalers ------------------\n",
    "\n",
    "# MinMaxScaler (Scales the features to a range [0, 1])\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# StandardScaler (Standardizes the features by removing the mean and scaling to unit variance)\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# MaxAbsScaler (Scales the features by their maximum absolute value, for data that is already centered at zero)\n",
    "# scaler = MaxAbsScaler()\n",
    "\n",
    "# RobustScaler (Scales the features using the median and interquartile range, less sensitive to outliers)\n",
    "# scaler = RobustScaler()\n",
    "\n",
    "# Normalizer (Scales the features to have unit norm, i.e., each sample is scaled to have unit norm)\n",
    "# scaler = Normalizer()\n",
    "\n",
    "# ------------------ Apply Scaler ------------------\n",
    "\n",
    "# Scale numerical columns in training and test data\n",
    "train_data[numerical_cols] = scaler.fit_transform(train_data[numerical_cols])\n",
    "test_data[numerical_cols] = scaler.transform(test_data[numerical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47600083-8cb3-455a-98fd-c65c7d1841d7",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "Encoding is the process of converting categorical variables into numerical representations so that machine learning models can process them. Techniques like One-Hot Encoding, Label Encoding, and Target Encoding are commonly used to convert categorical data into a format suitable for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e90a6a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:16:57.440887Z",
     "iopub.status.busy": "2024-11-30T22:16:57.440534Z",
     "iopub.status.idle": "2024-11-30T22:16:59.588545Z",
     "shell.execute_reply": "2024-11-30T22:16:59.587705Z",
     "shell.execute_reply.started": "2024-11-30T22:16:57.440856Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------ Encoding Methods ------------------\n",
    "\n",
    "# One-Hot Encoding (Creates binary columns for each category, default drop_first=False)\n",
    "train_data = pd.get_dummies(train_data, columns=categorical_cols, drop_first=False)\n",
    "test_data = pd.get_dummies(test_data, columns=categorical_cols, drop_first=False)\n",
    "\n",
    "# Label Encoding (Converts each category into a unique integer value)\n",
    "# label_encoder = LabelEncoder()\n",
    "# for col in categorical_cols:\n",
    "#     train_data[col] = label_encoder.fit_transform(train_data[col])\n",
    "#     test_data[col] = label_encoder.transform(test_data[col])\n",
    "\n",
    "# Ordinal Encoding (Maps categories to ordered integers, requires predefined order)\n",
    "# ordinal_encoder = OrdinalEncoder()\n",
    "# train_data[categorical_cols] = ordinal_encoder.fit_transform(train_data[categorical_cols])\n",
    "# test_data[categorical_cols] = ordinal_encoder.transform(test_data[categorical_cols])\n",
    "\n",
    "# Target Encoding (Encodes categories based on the mean of the target variable)\n",
    "# target_encoder = TargetEncoder()\n",
    "# train_data[categorical_cols] = target_encoder.fit_transform(train_data[categorical_cols], train_data['target_column'])\n",
    "# test_data[categorical_cols] = target_encoder.transform(test_data[categorical_cols])\n",
    "\n",
    "# Binary Encoding (Encodes categories as binary digits)\n",
    "# binary_encoder = BinaryEncoder()\n",
    "# train_data[categorical_cols] = binary_encoder.fit_transform(train_data[categorical_cols])\n",
    "# test_data[categorical_cols] = binary_encoder.transform(test_data[categorical_cols])\n",
    "\n",
    "# ------------------ Align Test Data with Training Data ------------------\n",
    "\n",
    "# Align test data columns with train data columns (fill missing columns with 0)\n",
    "test_data = test_data.reindex(columns=train_data.columns, fill_value=0)\n",
    "\n",
    "# ------------------ Optional: Drop 'price_doc' Column ------------------\n",
    "\n",
    "# Remove the target column if present in test_data\n",
    "# test_data = test_data.drop(columns=['price_doc'], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5e67cf",
   "metadata": {},
   "source": [
    "## correlation matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "i tried getting the correlation matrix but apparently a 2000 columns matrix is very computationally expensive as it performs pairs for all. so dont run it. it takes too long and then fails. i ran for 5 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae908d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:16:59.591699Z",
     "iopub.status.busy": "2024-11-30T22:16:59.591388Z",
     "iopub.status.idle": "2024-11-30T22:16:59.596033Z",
     "shell.execute_reply": "2024-11-30T22:16:59.595027Z",
     "shell.execute_reply.started": "2024-11-30T22:16:59.591671Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # DONT RUN\n",
    "# corr_matrix = train_data.corr()\n",
    "# print(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a46168",
   "metadata": {},
   "source": [
    "## Variance Filter\n",
    "The variance filter is used to remove features with low variance, which provide little information for predictive modeling. Features with very similar values across all observations are considered redundant and can be safely excluded from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb6c7ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:17:02.234826Z",
     "iopub.status.busy": "2024-11-30T22:17:02.234420Z",
     "iopub.status.idle": "2024-11-30T22:17:10.827879Z",
     "shell.execute_reply": "2024-11-30T22:17:10.826918Z",
     "shell.execute_reply.started": "2024-11-30T22:17:02.234792Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Save the target column separately\n",
    "# target_column = train_data['price_doc']\n",
    "\n",
    "# # Drop the target column from the features\n",
    "# train_data_copy = train_data.drop(columns=['price_doc'])\n",
    "\n",
    "# # Call the correlation filter function to filter out highly correlated features\n",
    "# train_data, test_data = varianceFiter(train_data_copy, test_data, 0.01)\n",
    "\n",
    "# # Append the target column back to the filtered data\n",
    "# train_data['price_doc'] = target_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d547f285",
   "metadata": {},
   "source": [
    "## Correlation Filter\n",
    "The correlation filter helps to remove highly correlated features. When two features are highly correlated, they convey similar information, and removing one can help reduce redundancy and improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b824fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the target column separately\n",
    "# target_column = train_data['price_doc']\n",
    "\n",
    "# # Drop the target column from the features\n",
    "# train_data_copy = train_data.drop(columns=['price_doc'])\n",
    "\n",
    "# # Call the correlation filter function to filter out highly correlated features\n",
    "# train_data, test_data = correlationFilter(train_data_copy, test_data, 0.9)\n",
    "\n",
    "# # Append the target column back to the filtered data\n",
    "# train_data['price_doc'] = target_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463d5ca5",
   "metadata": {},
   "source": [
    "## PCA (Principal Component Analysis)\n",
    "PCA is a dimensionality reduction technique that transforms the data into a new coordinate system, where the greatest variances lie along the first axes (principal components). It helps reduce the number of features while retaining most of the data's variance, improving model performance and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d7e6e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:19:58.783304Z",
     "iopub.status.busy": "2024-11-30T22:19:58.782735Z",
     "iopub.status.idle": "2024-11-30T22:19:58.818148Z",
     "shell.execute_reply": "2024-11-30T22:19:58.815910Z",
     "shell.execute_reply.started": "2024-11-30T22:19:58.783243Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_data, test_data = apply_pca(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd65ed6",
   "metadata": {
    "papermill": {
     "duration": 0.012565,
     "end_time": "2024-11-07T20:19:59.480848",
     "exception": false,
     "start_time": "2024-11-07T20:19:59.468283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Splitting - features and targets\n",
    "the data in train_data set is of x1 - x271 columns (271 variables) and one target variable (Y). we must split that data so that we can perform data preprocessing on the features variables (will be referred to as X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565fb08f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:19:58.821774Z",
     "iopub.status.busy": "2024-11-30T22:19:58.821179Z",
     "iopub.status.idle": "2024-11-30T22:19:58.996618Z",
     "shell.execute_reply": "2024-11-30T22:19:58.995737Z",
     "shell.execute_reply.started": "2024-11-30T22:19:58.821712Z"
    },
    "papermill": {
     "duration": 0.123675,
     "end_time": "2024-11-07T20:19:59.617410",
     "exception": false,
     "start_time": "2024-11-07T20:19:59.493735",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Drop the 'price_doc' column from train_data to get the features (X)\n",
    "X = train_data.drop(columns=['price_doc'])\n",
    "\n",
    "# Display X to confirm the result\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fd0e61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:19:58.998104Z",
     "iopub.status.busy": "2024-11-30T22:19:58.997788Z",
     "iopub.status.idle": "2024-11-30T22:19:59.006929Z",
     "shell.execute_reply": "2024-11-30T22:19:59.005793Z",
     "shell.execute_reply.started": "2024-11-30T22:19:58.998075Z"
    },
    "papermill": {
     "duration": 0.023643,
     "end_time": "2024-11-07T20:19:59.654979",
     "exception": false,
     "start_time": "2024-11-07T20:19:59.631336",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Extract the target variable 'price_doc' from train_data into Y\n",
    "Y = train_data['price_doc']\n",
    "\n",
    "# Display Y to confirm it contains only the target variable\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edcb49b",
   "metadata": {
    "papermill": {
     "duration": 0.016785,
     "end_time": "2024-11-07T20:20:00.974594",
     "exception": false,
     "start_time": "2024-11-07T20:20:00.957809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Splitting - train and validate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "now our test_data set is of rows with NO target variable whereas the train_data set is WITH target variable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "our rules in machine learning is that we must train half or 70% of the data and then we must check its accuracy using the remaining half or 30% of the data - we can only check accuracy IF we have the answers i.e. the target variable. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "So, what we need to do is, is split the train_data set into 2, by a 70% and 30% ratio. we train the model using the 70% and then test the model using the 30% and then use that model to predict the test_data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237c64fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:19:59.076075Z",
     "iopub.status.busy": "2024-11-30T22:19:59.075720Z",
     "iopub.status.idle": "2024-11-30T22:19:59.464683Z",
     "shell.execute_reply": "2024-11-30T22:19:59.463579Z",
     "shell.execute_reply.started": "2024-11-30T22:19:59.076044Z"
    },
    "papermill": {
     "duration": 0.273955,
     "end_time": "2024-11-07T20:20:01.264394",
     "exception": false,
     "start_time": "2024-11-07T20:20:00.990439",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# holdout method\n",
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6047d2",
   "metadata": {
    "papermill": {
     "duration": 0.013797,
     "end_time": "2024-11-07T20:20:01.398362",
     "exception": false,
     "start_time": "2024-11-07T20:20:01.384565",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## model intialization\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "here model is intialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511dc0e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:19:59.492075Z",
     "iopub.status.busy": "2024-11-30T22:19:59.491297Z",
     "iopub.status.idle": "2024-11-30T22:19:59.506105Z",
     "shell.execute_reply": "2024-11-30T22:19:59.505098Z",
     "shell.execute_reply.started": "2024-11-30T22:19:59.492029Z"
    },
    "papermill": {
     "duration": 0.021268,
     "end_time": "2024-11-07T20:20:01.433590",
     "exception": false,
     "start_time": "2024-11-07T20:20:01.412322",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(n_estimators=600, learning_rate=0.01,min_samples_leaf=5, min_samples_split=3, random_state=2,verbose=2,max_features='log2',max_depth= 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585f7723",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:19:59.507970Z",
     "iopub.status.busy": "2024-11-30T22:19:59.507470Z",
     "iopub.status.idle": "2024-11-30T22:19:59.518916Z",
     "shell.execute_reply": "2024-11-30T22:19:59.517945Z",
     "shell.execute_reply.started": "2024-11-30T22:19:59.507927Z"
    },
    "papermill": {
     "duration": 0.02329,
     "end_time": "2024-11-07T20:20:14.141486",
     "exception": false,
     "start_time": "2024-11-07T20:20:14.118196",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"X shape -> \", X.shape)\n",
    "print(\"trainX shape -> \", trainX.shape)\n",
    "print(\"testX shape -> \", testX.shape)\n",
    "print(\"test_data_processed shape -> \", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c9426d",
   "metadata": {},
   "source": [
    "# feature selection\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "here we will apply feature selection and feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e26330a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:19:59.533102Z",
     "iopub.status.busy": "2024-11-30T22:19:59.532791Z",
     "iopub.status.idle": "2024-11-30T22:19:59.544955Z",
     "shell.execute_reply": "2024-11-30T22:19:59.543814Z",
     "shell.execute_reply.started": "2024-11-30T22:19:59.533074Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07f3406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'price_doc' column exists in the test_data, and drop it if present\n",
    "# Check if 'price_doc' column exists in the test_data, and drop it if present\n",
    "if 'price_doc' in test_data.columns:\n",
    "    test_data = test_data.drop(columns=['price_doc'])\n",
    "    print(\"'price_doc' column has been dropped.\")\n",
    "else:\n",
    "    print(\"'price_doc' column was not found, nothing to drop.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b543f060",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:19:59.546596Z",
     "iopub.status.busy": "2024-11-30T22:19:59.546197Z",
     "iopub.status.idle": "2024-11-30T22:19:59.553297Z",
     "shell.execute_reply": "2024-11-30T22:19:59.552316Z",
     "shell.execute_reply.started": "2024-11-30T22:19:59.546564Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# apply feature selection here\n",
    "model, X, trainX, trainY, testX, test_data = kbest(model, 200, X, trainX, trainY, testX, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc618ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:19:59.555046Z",
     "iopub.status.busy": "2024-11-30T22:19:59.554676Z",
     "iopub.status.idle": "2024-11-30T22:19:59.566870Z",
     "shell.execute_reply": "2024-11-30T22:19:59.565832Z",
     "shell.execute_reply.started": "2024-11-30T22:19:59.555016Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6405d8",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "Grid Search is a technique used for hyperparameter tuning in machine learning models. It systematically tests different combinations of hyperparameters to find the best-performing set, based on a specified performance metric (such as accuracy or mean squared error). GridSearchCV from scikit-learn automates this process by performing cross-validation on each combination to identify the optimal model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ba6d7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:19:59.591801Z",
     "iopub.status.busy": "2024-11-30T22:19:59.591463Z",
     "iopub.status.idle": "2024-11-30T22:19:59.600628Z",
     "shell.execute_reply": "2024-11-30T22:19:59.599834Z",
     "shell.execute_reply.started": "2024-11-30T22:19:59.591772Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# define hyper parameters of grid\n",
    "# param_grid = {\n",
    "#     'max_depth': [ 1, 2, 3, 4, 5 ]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b969d8af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:19:59.602695Z",
     "iopub.status.busy": "2024-11-30T22:19:59.601951Z",
     "iopub.status.idle": "2024-11-30T22:19:59.611313Z",
     "shell.execute_reply": "2024-11-30T22:19:59.610403Z",
     "shell.execute_reply.started": "2024-11-30T22:19:59.602630Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model = gridsearch(param_grid, model, scorer, trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1178f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c4143",
   "metadata": {
    "papermill": {
     "duration": 0.014144,
     "end_time": "2024-11-07T20:20:14.233534",
     "exception": false,
     "start_time": "2024-11-07T20:20:14.219390",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## model running\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "here we run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6222c904",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission(model, trainX, trainY, testX, testY, X, Y, test_data, \"gb1.csv\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb68e8ab",
   "metadata": {},
   "source": [
    "# Multi-model running\n",
    "in one file we test different models and create multiple files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee8b316",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:29:18.843512Z",
     "iopub.status.busy": "2024-11-30T22:29:18.842802Z",
     "iopub.status.idle": "2024-11-30T22:29:18.862960Z",
     "shell.execute_reply": "2024-11-30T22:29:18.861399Z",
     "shell.execute_reply.started": "2024-11-30T22:29:18.843422Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# case 1\n",
    "model = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    subsample=1.0, \n",
    "    verbose=3\n",
    ")\n",
    "# create_submission(model, trainX, trainY, testX, testY, X, Y, test_data, \"gboost1.csv\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1702bc8-49ad-498d-b593-d54f811d9992",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:29:18.865756Z",
     "iopub.status.busy": "2024-11-30T22:29:18.865171Z",
     "iopub.status.idle": "2024-11-30T22:29:18.888451Z",
     "shell.execute_reply": "2024-11-30T22:29:18.887081Z",
     "shell.execute_reply.started": "2024-11-30T22:29:18.865696Z"
    },
    "papermill": {
     "duration": 0.053693,
     "end_time": "2024-11-07T21:46:14.588137",
     "exception": false,
     "start_time": "2024-11-07T21:46:14.534444",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# case 2\n",
    "model2 = GradientBoostingRegressor(\n",
    "    n_estimators=50,\n",
    "    max_depth=2,\n",
    "    learning_rate=0.2,\n",
    "    subsample=0.8, \n",
    "    verbose=3\n",
    ")\n",
    "# create_submission(model2, trainX, trainY, testX, testY, X, Y, test_data, \"gboost2.csv\")\n",
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed56fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:29:18.891622Z",
     "iopub.status.busy": "2024-11-30T22:29:18.891150Z",
     "iopub.status.idle": "2024-11-30T22:29:18.912461Z",
     "shell.execute_reply": "2024-11-30T22:29:18.910856Z",
     "shell.execute_reply.started": "2024-11-30T22:29:18.891579Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# case 3\n",
    "model3 = GradientBoostingRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9, \n",
    "    verbose=3\n",
    ")\n",
    "# create_submission(model3, trainX, trainY, testX, testY, X, Y, test_data, \"gboost3.csv\")\n",
    "model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be937f5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:29:18.914915Z",
     "iopub.status.busy": "2024-11-30T22:29:18.914341Z",
     "iopub.status.idle": "2024-11-30T22:29:18.939302Z",
     "shell.execute_reply": "2024-11-30T22:29:18.938049Z",
     "shell.execute_reply.started": "2024-11-30T22:29:18.914858Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# case 4\n",
    "model4 = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.85, \n",
    "    verbose=3\n",
    ")\n",
    "# create_submission(model4, trainX, trainY, testX, testY, X, Y, test_data, \"gboost4.csv\")\n",
    "model4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766db70a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:29:18.941493Z",
     "iopub.status.busy": "2024-11-30T22:29:18.941064Z",
     "iopub.status.idle": "2024-11-30T22:29:18.962814Z",
     "shell.execute_reply": "2024-11-30T22:29:18.961187Z",
     "shell.execute_reply.started": "2024-11-30T22:29:18.941454Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# case 5\n",
    "model5 = GradientBoostingRegressor(\n",
    "    n_estimators=150,\n",
    "    max_depth=2,\n",
    "    learning_rate=0.15,\n",
    "    subsample=0.7, \n",
    "    verbose=3\n",
    ")\n",
    "# create_submission(model5, trainX, trainY, testX, testY, X, Y, test_data, \"gboost5.csv\")\n",
    "model5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19831eab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:29:18.966872Z",
     "iopub.status.busy": "2024-11-30T22:29:18.966228Z",
     "iopub.status.idle": "2024-11-30T22:29:18.983055Z",
     "shell.execute_reply": "2024-11-30T22:29:18.981497Z",
     "shell.execute_reply.started": "2024-11-30T22:29:18.966813Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# case 6\n",
    "model6 = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    max_features=0.3, \n",
    "    verbose=3\n",
    ")\n",
    "# create_submission(model6, trainX, trainY, testX, testY, X, Y, test_data, \"gboost6.csv\")\n",
    "model6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aa9b33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:29:18.985601Z",
     "iopub.status.busy": "2024-11-30T22:29:18.985036Z",
     "iopub.status.idle": "2024-11-30T22:29:19.004493Z",
     "shell.execute_reply": "2024-11-30T22:29:19.002828Z",
     "shell.execute_reply.started": "2024-11-30T22:29:18.985544Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# case 7\n",
    "model7 = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.9,\n",
    "    min_samples_split=10, \n",
    "    verbose=3\n",
    ")\n",
    "# create_submission(model7, trainX, trainY, testX, testY, X, Y, test_data, \"gboost7.csv\")\n",
    "model7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aa66ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:29:19.006876Z",
     "iopub.status.busy": "2024-11-30T22:29:19.006291Z",
     "iopub.status.idle": "2024-11-30T22:29:19.022221Z",
     "shell.execute_reply": "2024-11-30T22:29:19.020928Z",
     "shell.execute_reply.started": "2024-11-30T22:29:19.006830Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# case 8\n",
    "model8 = GradientBoostingRegressor(\n",
    "    n_estimators=50,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.3,\n",
    "    subsample=0.75, \n",
    "    verbose=3\n",
    ")\n",
    "# create_submission(model8, trainX, trainY, testX, testY, X, Y, test_data, \"gboost8.csv\")\n",
    "model8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0229f77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:29:19.024374Z",
     "iopub.status.busy": "2024-11-30T22:29:19.023913Z",
     "iopub.status.idle": "2024-11-30T22:35:21.436948Z",
     "shell.execute_reply": "2024-11-30T22:35:21.435753Z",
     "shell.execute_reply.started": "2024-11-30T22:29:19.024326Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# case 9\n",
    "model9 = GradientBoostingRegressor(\n",
    "    n_estimators=120,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.08,\n",
    "    subsample=0.85,\n",
    "    max_features='sqrt', \n",
    "    verbose=3\n",
    ")\n",
    "# create_submission(model9, trainX, trainY, testX, testY, X, Y, test_data, \"gboost9.csv\")\n",
    "model9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb4571b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:35:21.438613Z",
     "iopub.status.busy": "2024-11-30T22:35:21.438215Z",
     "iopub.status.idle": "2024-11-30T22:35:21.447065Z",
     "shell.execute_reply": "2024-11-30T22:35:21.446002Z",
     "shell.execute_reply.started": "2024-11-30T22:35:21.438581Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# case 10\n",
    "model0 = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.6, \n",
    "    verbose=3\n",
    ")\n",
    "# create_submission(model0, trainX, trainY, testX, testY, X, Y, test_data, \"gboost10.csv\")\n",
    "model0"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6034234,
     "sourceId": 9837068,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6034286,
     "sourceId": 9837128,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6164648,
     "sourceId": 10013045,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6179183,
     "sourceId": 10032577,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5193.546342,
   "end_time": "2024-11-07T21:46:17.332142",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-07T20:19:43.785800",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
