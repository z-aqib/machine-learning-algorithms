{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013014,
     "end_time": "2024-11-07T20:19:46.538794",
     "exception": false,
     "start_time": "2024-11-07T20:19:46.525780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importing Libraries\n",
    "in this part we will install all the necessary libraries on command prompt and then import the necessary functions from those libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd54b756",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T20:34:42.226484Z",
     "iopub.status.busy": "2024-11-09T20:34:42.226112Z",
     "iopub.status.idle": "2024-11-09T20:34:46.885029Z",
     "shell.execute_reply": "2024-11-09T20:34:46.884194Z",
     "shell.execute_reply.started": "2024-11-09T20:34:42.226445Z"
    },
    "papermill": {
     "duration": 5.853121,
     "end_time": "2024-11-07T20:19:52.405091",
     "exception": false,
     "start_time": "2024-11-07T20:19:46.551970",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# importing all the necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "from numpy import mean\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# step 1: preprocessing\n",
    "from sklearn.impute import SimpleImputer # import some strategic imputer to fill in any missing values using mean\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler, Normalizer # scale all the values to one range to avoid any biasness (this bias is seen in mostly naive bayes and knn etc)\n",
    "\n",
    "from sklearn.impute import KNNImputer # import some strategic imputer to fill missing values using KNN (finds the nearest neighbour and fills it with that value)\n",
    "\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, SelectKBest, f_classif, VarianceThreshold\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "# step 2: data division\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, cross_val_score, GridSearchCV, ParameterGrid # to divide the code into train/test using a specific percentage or with/without replacement\n",
    "\n",
    "# step 3: model\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# step 4: displaying accuracy\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score # to display the accuracy of our tree\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, make_scorer\n",
    "\n",
    "# step 5: warning filter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18014263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Function to get current date and time as a string\n",
    "def get_current_datetime():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3912ca3f",
   "metadata": {
    "papermill": {
     "duration": 0.011237,
     "end_time": "2024-11-07T20:19:52.458899",
     "exception": false,
     "start_time": "2024-11-07T20:19:52.447662",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Loading\n",
    "data shall be loaded into variables as data sets using pandas and csv readers. they will be checked to see if they are loaded properly and will be loaded as 2 sets: train and test as per given in the kaggle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9125e82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T20:34:46.893155Z",
     "iopub.status.busy": "2024-11-09T20:34:46.892527Z",
     "iopub.status.idle": "2024-11-09T20:34:51.243865Z",
     "shell.execute_reply": "2024-11-09T20:34:51.242855Z",
     "shell.execute_reply.started": "2024-11-09T20:34:46.893109Z"
    },
    "papermill": {
     "duration": 4.591761,
     "end_time": "2024-11-07T20:19:57.062353",
     "exception": false,
     "start_time": "2024-11-07T20:19:52.470592",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# lets load the training data set\n",
    "train_data = pd.read_csv(r\"D:\\Users\\DELL\\OneDrive - Institute of Business Administration\\IBA\\sem5\\machine learning\\ipynb notebooks\\challenger2\\iml-fall-2024-challenge-2\\train\\train.csv\")\n",
    "\n",
    "# lets also check it by getting the first few rows of the data, there should be x1 - x78 and one target variable Y\n",
    "train_data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f55297",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T20:34:51.246483Z",
     "iopub.status.busy": "2024-11-09T20:34:51.246159Z",
     "iopub.status.idle": "2024-11-09T20:34:53.028096Z",
     "shell.execute_reply": "2024-11-09T20:34:53.027152Z",
     "shell.execute_reply.started": "2024-11-09T20:34:51.246449Z"
    },
    "papermill": {
     "duration": 1.948205,
     "end_time": "2024-11-07T20:19:59.023969",
     "exception": false,
     "start_time": "2024-11-07T20:19:57.075764",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# lets load the test data\n",
    "test_data = pd.read_csv(r\"D:\\Users\\DELL\\OneDrive - Institute of Business Administration\\IBA\\sem5\\machine learning\\ipynb notebooks\\challenger2\\iml-fall-2024-challenge-2\\test\\test.csv\")\n",
    "\n",
    "# check if the data has been loaded by getting the first 5 rows - there should be x1 - x78 and no target variable Y as this is test data\n",
    "test_data.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db89cc8",
   "metadata": {
    "papermill": {
     "duration": 0.012664,
     "end_time": "2024-11-07T20:19:59.050712",
     "exception": false,
     "start_time": "2024-11-07T20:19:59.038048",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Preprocessing\n",
    "before we start processing this data and using algorithms, we will fix this data first, this is called data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a83e6e",
   "metadata": {},
   "source": [
    "## split data into categorical and numerical\n",
    "categorical will have one-hot and simple imputer of most frequent while numerical will have simple mean imputer and minmax scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306cc606",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = train_data.select_dtypes(include=[\"object\"]).columns\n",
    "numerical_cols = train_data.select_dtypes(exclude=[\"object\"]).drop(columns=['price_doc']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e90a6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e20cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column transformer for preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_transformer, numerical_cols),\n",
    "        (\"cat\", cat_transformer, categorical_cols)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5e67cf",
   "metadata": {},
   "source": [
    "## correlation matrix\n",
    "i tried getting the correlation matrix but apparently a 2000 columns matrix is very computationally expensive as it performs pairs for all. so dont run it. it takes too long and then fails. i ran for 5 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae908d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DONT RUN\n",
    "# corr_matrix = train_data.corr()\n",
    "# print(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463d5ca5",
   "metadata": {},
   "source": [
    "# PCA\n",
    "principal component analysis is applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -------------------------- case  --------------------------\n",
    "# pca = PCA(n_components=33)                                 \n",
    "# X = pca.fit_transform(X)\n",
    "# test_data_processed = pca.transform(test_data_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd65ed6",
   "metadata": {
    "papermill": {
     "duration": 0.012565,
     "end_time": "2024-11-07T20:19:59.480848",
     "exception": false,
     "start_time": "2024-11-07T20:19:59.468283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Splitting - festures and targets\n",
    "the data in train_data set is of x1 - x78 columns (79 variables) and one target variable (Y). we must split that data so that we can perform data preprocessing on the features variables (will be referred to as X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565fb08f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T20:34:53.346782Z",
     "iopub.status.busy": "2024-11-09T20:34:53.346374Z",
     "iopub.status.idle": "2024-11-09T20:34:53.470850Z",
     "shell.execute_reply": "2024-11-09T20:34:53.469954Z",
     "shell.execute_reply.started": "2024-11-09T20:34:53.346734Z"
    },
    "papermill": {
     "duration": 0.123675,
     "end_time": "2024-11-07T20:19:59.617410",
     "exception": false,
     "start_time": "2024-11-07T20:19:59.493735",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# so in X, it is ALL the columns EXCEPT the last column known as 'Y' (we can confirm this using the train_data.head() we did earlier) so we must get all columns and DROP only the 'y' column\n",
    "X = train_data.drop(columns=['price_doc'])\n",
    "X # lets display X and see what it is now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fd0e61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T20:34:53.472322Z",
     "iopub.status.busy": "2024-11-09T20:34:53.472034Z",
     "iopub.status.idle": "2024-11-09T20:34:53.479744Z",
     "shell.execute_reply": "2024-11-09T20:34:53.478930Z",
     "shell.execute_reply.started": "2024-11-09T20:34:53.472292Z"
    },
    "papermill": {
     "duration": 0.023643,
     "end_time": "2024-11-07T20:19:59.654979",
     "exception": false,
     "start_time": "2024-11-07T20:19:59.631336",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# so as per our X output, we can see that number of columns in train_data is 79 and number of columns in X is 78 meaning we have successfully performed our removal of target variable\n",
    "# now to get the target variable alone, we can just get it alone,\n",
    "Y = train_data['price_doc']\n",
    "Y # lets see what it is\n",
    "# as per our Y output, we can see it is of one column and 246k rows which means we have successfully extracted the target variable column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e9cfd9",
   "metadata": {
    "papermill": {
     "duration": 0.013362,
     "end_time": "2024-11-07T20:20:00.756855",
     "exception": false,
     "start_time": "2024-11-07T20:20:00.743493",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Filters\n",
    "there are two types of filters to filter out columns/features:\n",
    "- variance filter (a column which has same values throughout the column like all are sunny)\n",
    "- correlation filter (two columns which are same like weight in kg and weight in pounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5934b338",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T20:34:54.323862Z",
     "iopub.status.busy": "2024-11-09T20:34:54.323554Z",
     "iopub.status.idle": "2024-11-09T20:34:54.329537Z",
     "shell.execute_reply": "2024-11-09T20:34:54.328588Z",
     "shell.execute_reply.started": "2024-11-09T20:34:54.323829Z"
    },
    "papermill": {
     "duration": 0.02154,
     "end_time": "2024-11-07T20:20:00.791959",
     "exception": false,
     "start_time": "2024-11-07T20:20:00.770419",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(\"X : \", X.shape)\n",
    "# print(\"test data : \", test_data_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c6b8a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T20:34:54.331337Z",
     "iopub.status.busy": "2024-11-09T20:34:54.331052Z",
     "iopub.status.idle": "2024-11-09T20:34:54.339745Z",
     "shell.execute_reply": "2024-11-09T20:34:54.338759Z",
     "shell.execute_reply.started": "2024-11-09T20:34:54.331307Z"
    },
    "papermill": {
     "duration": 0.021917,
     "end_time": "2024-11-07T20:20:00.827296",
     "exception": false,
     "start_time": "2024-11-07T20:20:00.805379",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# variance filter\n",
    "# ----------------------------- case  -----------------------------\n",
    "# variance_filter = VarianceThreshold(threshold=0.001)  # Adjust the threshold if needed\n",
    "# X = variance_filter.fit_transform(X)\n",
    "# test_data_processed = variance_filter.fit_transform(test_data_processed)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1604caf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T20:34:54.341918Z",
     "iopub.status.busy": "2024-11-09T20:34:54.341526Z",
     "iopub.status.idle": "2024-11-09T20:34:54.352761Z",
     "shell.execute_reply": "2024-11-09T20:34:54.351903Z",
     "shell.execute_reply.started": "2024-11-09T20:34:54.341864Z"
    },
    "papermill": {
     "duration": 0.026133,
     "end_time": "2024-11-07T20:20:00.868012",
     "exception": false,
     "start_time": "2024-11-07T20:20:00.841879",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# test_data_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd97f036",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T20:34:54.354559Z",
     "iopub.status.busy": "2024-11-09T20:34:54.354212Z",
     "iopub.status.idle": "2024-11-09T20:34:54.364098Z",
     "shell.execute_reply": "2024-11-09T20:34:54.362857Z",
     "shell.execute_reply.started": "2024-11-09T20:34:54.354519Z"
    },
    "papermill": {
     "duration": 0.023607,
     "end_time": "2024-11-07T20:20:00.905745",
     "exception": false,
     "start_time": "2024-11-07T20:20:00.882138",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # correlation filter\n",
    "# # ----------------------------- case  -----------------------------\n",
    "# corr_matrix = pd.DataFrame(X).corr().abs()\n",
    "# upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "# to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\n",
    "# X = pd.DataFrame(X).drop(columns=to_drop)\n",
    "# test_data_processed = pd.DataFrame(test_data_processed).drop(columns=to_drop)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d523de93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T20:34:54.365730Z",
     "iopub.status.busy": "2024-11-09T20:34:54.365330Z",
     "iopub.status.idle": "2024-11-09T20:34:54.378456Z",
     "shell.execute_reply": "2024-11-09T20:34:54.377574Z",
     "shell.execute_reply.started": "2024-11-09T20:34:54.365688Z"
    },
    "papermill": {
     "duration": 0.023492,
     "end_time": "2024-11-07T20:20:00.943356",
     "exception": false,
     "start_time": "2024-11-07T20:20:00.919864",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# test_data_processed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edcb49b",
   "metadata": {
    "papermill": {
     "duration": 0.016785,
     "end_time": "2024-11-07T20:20:00.974594",
     "exception": false,
     "start_time": "2024-11-07T20:20:00.957809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Splitting - train and validate\n",
    "now our test_data set is of rows with NO target variable whereas the train_data set is WITH target variable.\n",
    "our rules in machine learning is that we must train half or 70% of the data and then we must check its accuracy using the remaining half or 30% of the data - we can only check accuracy IF we have the answers i.e. the target variable. \n",
    "So, what we need to do is, is split the train_data set into 2, by a 70% and 30% ratio. we train the model using the 70% and then test the model using the 30% and then use that model to predict the test_data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237c64fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T20:34:54.380121Z",
     "iopub.status.busy": "2024-11-09T20:34:54.379772Z",
     "iopub.status.idle": "2024-11-09T20:34:54.615792Z",
     "shell.execute_reply": "2024-11-09T20:34:54.614961Z",
     "shell.execute_reply.started": "2024-11-09T20:34:54.380082Z"
    },
    "papermill": {
     "duration": 0.273955,
     "end_time": "2024-11-07T20:20:01.264394",
     "exception": false,
     "start_time": "2024-11-07T20:20:00.990439",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# holdout method\n",
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1485955",
   "metadata": {
    "papermill": {
     "duration": 0.013949,
     "end_time": "2024-11-07T20:20:01.292747",
     "exception": false,
     "start_time": "2024-11-07T20:20:01.278798",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# functions\n",
    "here we have defined functions like forward-backward selection, kbest selection & algorithm feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836da9a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T20:34:54.617302Z",
     "iopub.status.busy": "2024-11-09T20:34:54.616979Z",
     "iopub.status.idle": "2024-11-09T20:34:54.625434Z",
     "shell.execute_reply": "2024-11-09T20:34:54.624603Z",
     "shell.execute_reply.started": "2024-11-09T20:34:54.617269Z"
    },
    "papermill": {
     "duration": 0.023729,
     "end_time": "2024-11-07T20:20:01.330428",
     "exception": false,
     "start_time": "2024-11-07T20:20:01.306699",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# forward backward selection\n",
    "def fbselection(direction, sample_model, features, X, trainX, trainY, testX, test_data_processed):\n",
    "    print(\"starting\")\n",
    "    selection = SequentialFeatureSelector(sample_model, direction=direction, n_features_to_select=features, scoring='roc_auc')\n",
    "    return modelSelector(sample_model, selection, X, trainX, trainY, testX, test_data_processed)\n",
    "\n",
    "def modelSelector(sample_model, selection, X, trainX, trainY, testX, test_data_processed):\n",
    "    print(\"start extracting\")\n",
    "    trainX = selection.fit_transform(trainX, trainY)\n",
    "    print(\"extracted, transforming\")\n",
    "    testX = selection.transform(testX)                                  # Ensure the test set is transformed similarly\n",
    "    test_data_processed = selection.transform(test_data_processed)      # test data is also transformed\n",
    "    X = selection.transform(X)                                          # full data transforming\n",
    "    print(\"transformed\")\n",
    "    return sample_model, X, trainX, trainY, testX, test_data_processed\n",
    "\n",
    "# kbest selection\n",
    "def kbest(sample_model, features, X, trainX, trainY, testX, test_data_processed):\n",
    "    print(\"starting\")\n",
    "    selection = SelectKBest(score_func=f_regression, k=features)\n",
    "    return modelSelector(sample_model, selection, X, trainX, trainY, testX, test_data_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0bc955",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T20:34:54.626998Z",
     "iopub.status.busy": "2024-11-09T20:34:54.626670Z",
     "iopub.status.idle": "2024-11-09T20:34:54.637011Z",
     "shell.execute_reply": "2024-11-09T20:34:54.636252Z",
     "shell.execute_reply.started": "2024-11-09T20:34:54.626952Z"
    },
    "papermill": {
     "duration": 0.025235,
     "end_time": "2024-11-07T20:20:01.370030",
     "exception": false,
     "start_time": "2024-11-07T20:20:01.344795",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# feature importance function\n",
    "def featureImportance(sample_model, features, X, trainX, trainY, testX, test_data_processed):\n",
    "    print(\"fitting\")\n",
    "    \n",
    "    # fit the model\n",
    "    sample_model.fit(trainX, trainY)\n",
    "\n",
    "    print(\"extracting features\")\n",
    "\n",
    "    # extract all the feature names from data\n",
    "    importances = sample_model.feature_importances_\n",
    "    feature_names = train_data.drop(columns=['price_doc']).columns\n",
    "    print(feature_names)\n",
    "\n",
    "    # sort with respect to importance\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # extract the top ones\n",
    "    top_features = feature_importance_df['Feature'].head(features).values\n",
    "    print(top_features)\n",
    "\n",
    "    # change all data according to the top ones we have selected\n",
    "    trainX = pd.DataFrame(trainX, columns=feature_names)[top_features]\n",
    "    testX = pd.DataFrame(testX, columns=feature_names)[top_features]\n",
    "    X = pd.DataFrame(X, columns=feature_names)[top_features]\n",
    "    test_data_processed = pd.DataFrame(test_data_processed, columns=feature_names)[top_features]\n",
    "\n",
    "    print(\"features extracted\")\n",
    "    \n",
    "    # retrain the model\n",
    "    sample_model.fit(trainX, trainY)\n",
    "\n",
    "    print(\"features trained\")\n",
    "    \n",
    "    return sample_model, X, trainX, trainY, testX, test_data_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6047d2",
   "metadata": {
    "papermill": {
     "duration": 0.013797,
     "end_time": "2024-11-07T20:20:01.398362",
     "exception": false,
     "start_time": "2024-11-07T20:20:01.384565",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## model intialization\n",
    "here model is intialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5ae546",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = preprocessor.fit_transform(trainX)\n",
    "print(\"trainX completed\")\n",
    "testX = preprocessor.transform(testX)\n",
    "print(\"testX completed\")\n",
    "test_data = preprocessor.transform(test_data)\n",
    "print(\"test data completed\")\n",
    "X = preprocessor.transform(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511dc0e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T20:34:54.638616Z",
     "iopub.status.busy": "2024-11-09T20:34:54.638241Z",
     "iopub.status.idle": "2024-11-09T20:34:54.651652Z",
     "shell.execute_reply": "2024-11-09T20:34:54.650861Z",
     "shell.execute_reply.started": "2024-11-09T20:34:54.638557Z"
    },
    "papermill": {
     "duration": 0.021268,
     "end_time": "2024-11-07T20:20:01.433590",
     "exception": false,
     "start_time": "2024-11-07T20:20:01.412322",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# declare the model here\n",
    "# model = XGBRegressor(max_depth=10, learning_rate=0.01, n_estimators=1600, subsample=0.85, reg_lambda=0.2, reg_alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585f7723",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T21:05:03.767309Z",
     "iopub.status.busy": "2024-11-09T21:05:03.766992Z",
     "iopub.status.idle": "2024-11-09T21:05:03.776385Z",
     "shell.execute_reply": "2024-11-09T21:05:03.775476Z",
     "shell.execute_reply.started": "2024-11-09T21:05:03.767271Z"
    },
    "papermill": {
     "duration": 0.02329,
     "end_time": "2024-11-07T20:20:14.141486",
     "exception": false,
     "start_time": "2024-11-07T20:20:14.118196",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"X shape -> \", X.shape)\n",
    "print(\"trainX shape -> \", trainX.shape)\n",
    "print(\"testX shape -> \", testX.shape)\n",
    "print(\"test_data_processed shape -> \", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c9426d",
   "metadata": {},
   "source": [
    "# feature selection\n",
    "here we will apply feature selection and feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b543f060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "# model, X, trainX, trainY, testX, test_data = featureImportance(\n",
    "#     model, 40, X, trainX, trainY, testX, test_data\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabea5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X shape -> \", X.shape)\n",
    "print(\"trainX shape -> \", trainX.shape)\n",
    "print(\"testX shape -> \", testX.shape)\n",
    "print(\"test_data_processed shape -> \", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58edd65e",
   "metadata": {},
   "source": [
    "# grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b8b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch(param_grid, model, scorer, trainX, trainY):\n",
    "    print(\"starting grid search\")\n",
    "\n",
    "    # intialize grid search\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring=scorer, verbose=3)\n",
    "    print(\"grid search is intialized\")\n",
    "\n",
    "    # fit the model\n",
    "    grid_search.fit(trainX, trainY)\n",
    "    print(\"grid search fitting completed\")\n",
    "\n",
    "    # display the best model grid search found\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(best_model)\n",
    "\n",
    "    # display the best parameters of the best model\n",
    "    best_parameters = grid_search.best_params_\n",
    "    print(best_parameters)\n",
    "\n",
    "    # display the best score of the best model\n",
    "    print(\"Best cross-validated score:\", grid_search.best_score_)\n",
    "\n",
    "    # assign the best model our model\n",
    "    model = best_model\n",
    "    print(\"model assigned, grid search completed\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8896c271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a scoring metric (e.g., negative mean squared error)\n",
    "# scorer = make_scorer(mean_squared_error, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ba6d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define hyper parameters of grid\n",
    "# param_grid = {\n",
    "#     'estimators': [10, 100, 200, 500, 1000, 2000, 3000], \n",
    "#     'learning_rate': [0.0001, 0.001, 0.01, 0.05, 0.1, 0.5],\n",
    "#     'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13000df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from train_data\n",
    "# sample_train = train_data.sample(frac=0.5)\n",
    "# sample_X = sample_train.drop('price_doc', axis=1) \n",
    "# sample_Y = sample_train['price_doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b969d8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = gridsearch(param_grid, model, scorer, trainX, trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c4143",
   "metadata": {
    "papermill": {
     "duration": 0.014144,
     "end_time": "2024-11-07T20:20:14.233534",
     "exception": false,
     "start_time": "2024-11-07T20:20:14.219390",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## model running\n",
    "here we run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829d7189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_current_datetime():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb74356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute this predictions metrics\n",
    "def metrics(y_pred, testY):\n",
    "    print(\"starting to compute metrics\")\n",
    "    \n",
    "    # display the mean squared error of this prediction\n",
    "    mse = mean_squared_error(testY, y_pred)\n",
    "    print(\"Mean squared error: %.2f\" % mse, \"   \")\n",
    "\n",
    "    # display the root mean squared error\n",
    "    rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "    print(\"Root Mean squared error: %.2f\" % rmse, \"   \")\n",
    "\n",
    "    # display the mean absolute error of this prediction\n",
    "    mae = mean_absolute_error(testY, y_pred)\n",
    "    print(\"Mean absolute error: %.2f\" % mae, \"   \")\n",
    "\n",
    "    # display the coeffeicient of determination of this preduction\n",
    "    r2_Score = r2_score(testY, y_pred)\n",
    "    print(\"Coefficient of determination: %.2f\" % r2_Score, \"    \")\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bacd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, trainX, trainY, testX, testY):\n",
    "    print(\"training model\")\n",
    "    model.fit(trainX, trainY)\n",
    "    print(\"computing score\")\n",
    "    print(\"model score: \", model.score(trainX, trainY))\n",
    "    y_pred = model.predict(testX)\n",
    "    rmse = metrics(y_pred, testY)\n",
    "    return model, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e82977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFile(model, X, Y, test_data, file_name):\n",
    "    print(\"fitting on X Y \", get_current_datetime())\n",
    "    model.fit(X, Y)\n",
    "\n",
    "    print(\"scoring on X Y \", get_current_datetime())\n",
    "    score = model.score(X, Y)\n",
    "    print(\"model test score: \", score, \"    \")\n",
    "\n",
    "    print(\"predicting on test \", get_current_datetime())\n",
    "    test_prediction = model.predict(test_data)\n",
    "    print(test_prediction)\n",
    "\n",
    "    print(\"getting sample submission \", get_current_datetime())\n",
    "    sample_data = pd.read_csv(r\"D:\\Users\\DELL\\OneDrive - Institute of Business Administration\\IBA\\sem5\\machine learning\\ipynb notebooks\\challenger2\\iml-fall-2024-challenge-2\\sample_submission.csv\")\n",
    "    sample_data['price_doc'] = test_prediction\n",
    "\n",
    "    print(\"Saving submission \", get_current_datetime())\n",
    "    # Ensure the path ends with a backslash\n",
    "    base_path = r\"D:\\Users\\DELL\\OneDrive - Institute of Business Administration\\IBA\\sem5\\machine learning\\ipynb notebooks\\challenger2\\iml-fall-2024-challenge-2\\\\\"\n",
    "    full_path = base_path + file_name\n",
    "    sample_data.to_csv(full_path, index=False)\n",
    "    print(f\"File saved at: {full_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85abfd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(model, trainX, trainY, testX, testY, X, Y, test_data, file_name):\n",
    "    model, rmse = run_model(model, trainX, trainY, testX, testY)\n",
    "    createFile(model, X, Y, test_data, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17da1c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(\n",
    "    max_depth=18,               # Control tree depth\n",
    "    learning_rate=0.009,         # Step size\n",
    "    n_estimators=2000,          # Number of boosting rounds\n",
    "    subsample=0.85,              # Fraction of training data to grow trees\n",
    "    colsample_bytree=0.8,       # Fraction of features for each tree\n",
    "    reg_lambda=0.2,               # L2 regularization\n",
    "    reg_alpha=0.8,                # L1 regularization\n",
    "    device = 'cuda',\n",
    "    verbose = 2,\n",
    "    tree_method = 'gpu_hist',\n",
    "    predictor = 'gpu_predictor'\n",
    ")\n",
    "create_submission(model, trainX, trainY, testX, testY, X, Y, test_data, \"xgb1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ea7f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = XGBRegressor(max_depth=10, learning_rate=0.01, n_estimators=1600, subsample=0.85, reg_lambda=0.2, reg_alpha=0.8)\n",
    "# create_submission(model, trainX, trainY, testX, testY, X, Y, test_data, \"xgb1.cxv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d834bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = XGBRegressor(max_depth=10, learning_rate=0.001, n_estimators=1600, subsample=0.85, reg_lambda=0.2, reg_alpha=0.8)\n",
    "# create_submission(model, trainX, trainY, testX, testY, X, Y, test_data, \"xgb2.cxv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d41b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = XGBRegressor(max_depth=10, learning_rate=0.01, n_estimators=1700, subsample=0.85, reg_lambda=0.2, reg_alpha=0.8)\n",
    "# create_submission(model, trainX, trainY, testX, testY, X, Y, test_data, \"xgb3.cxv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c4dc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = XGBRegressor(max_depth=11, learning_rate=0.01, n_estimators=1600, subsample=0.85, reg_lambda=0.2, reg_alpha=0.8)\n",
    "# create_submission(model, trainX, trainY, testX, testY, X, Y, test_data, \"xgb4.cxv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937ae370",
   "metadata": {
    "papermill": {
     "duration": 0.019827,
     "end_time": "2024-11-07T20:55:49.286324",
     "exception": false,
     "start_time": "2024-11-07T20:55:49.266497",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## predict for test dataset\n",
    "fit the model and predict for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6eb5c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T21:14:20.527664Z",
     "iopub.status.busy": "2024-11-09T21:14:20.527257Z",
     "iopub.status.idle": "2024-11-09T21:23:34.864087Z",
     "shell.execute_reply": "2024-11-09T21:23:34.863102Z",
     "shell.execute_reply.started": "2024-11-09T21:14:20.527606Z"
    },
    "papermill": {
     "duration": 2249.874308,
     "end_time": "2024-11-07T21:33:19.180774",
     "exception": false,
     "start_time": "2024-11-07T20:55:49.306466",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18c11b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # display information regarding the regression\n",
    "# print(\"model score: \", model.score(X, Y), \"    \")\n",
    "# # print(\"model coefficient: \", model.coef_)\n",
    "# # print(\"model intercept: \", model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c93967",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T21:23:34.865465Z",
     "iopub.status.busy": "2024-11-09T21:23:34.865173Z",
     "iopub.status.idle": "2024-11-09T21:28:34.563809Z",
     "shell.execute_reply": "2024-11-09T21:28:34.562789Z",
     "shell.execute_reply.started": "2024-11-09T21:23:34.865433Z"
    },
    "papermill": {
     "duration": 774.78282,
     "end_time": "2024-11-07T21:46:13.986713",
     "exception": false,
     "start_time": "2024-11-07T21:33:19.203893",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# test_prediction = model.predict(test_data)\n",
    "\n",
    "# # test_prediction=test_prediction[:, 1]\n",
    "\n",
    "# print(test_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c8bfcf",
   "metadata": {
    "papermill": {
     "duration": 0.02242,
     "end_time": "2024-11-07T21:46:14.032429",
     "exception": false,
     "start_time": "2024-11-07T21:46:14.010009",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## write into csv\n",
    "now we write the predictions into the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0573bfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T21:28:34.565559Z",
     "iopub.status.busy": "2024-11-09T21:28:34.565203Z",
     "iopub.status.idle": "2024-11-09T21:28:34.818446Z",
     "shell.execute_reply": "2024-11-09T21:28:34.817419Z",
     "shell.execute_reply.started": "2024-11-09T21:28:34.565526Z"
    },
    "papermill": {
     "duration": 0.455346,
     "end_time": "2024-11-07T21:46:14.510398",
     "exception": false,
     "start_time": "2024-11-07T21:46:14.055052",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# sample_data = pd.read_csv(r\"D:\\Users\\DELL\\OneDrive - Institute of Business Administration\\IBA\\sem5\\machine learning\\ipynb notebooks\\challenger2\\iml-fall-2024-challenge-2\\sample_submission.csv\")\n",
    "\n",
    "# sample_data['price_doc'] = test_prediction\n",
    "# sample_data\n",
    "\n",
    "# sample_data.to_csv(r\"D:\\Users\\DELL\\OneDrive - Institute of Business Administration\\IBA\\sem5\\machine learning\\ipynb notebooks\\challenger2\\iml-fall-2024-challenge-2\\xgb1.csv\", index=False)\n",
    "# sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T21:28:34.820725Z",
     "iopub.status.busy": "2024-11-09T21:28:34.819860Z",
     "iopub.status.idle": "2024-11-09T21:28:34.838790Z",
     "shell.execute_reply": "2024-11-09T21:28:34.837841Z",
     "shell.execute_reply.started": "2024-11-09T21:28:34.820679Z"
    },
    "papermill": {
     "duration": 0.053693,
     "end_time": "2024-11-07T21:46:14.588137",
     "exception": false,
     "start_time": "2024-11-07T21:46:14.534444",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6034234,
     "sourceId": 9837068,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6034286,
     "sourceId": 9837128,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5193.546342,
   "end_time": "2024-11-07T21:46:17.332142",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-07T20:19:43.785800",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
